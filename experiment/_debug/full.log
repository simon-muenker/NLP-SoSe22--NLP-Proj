> Loaded logger: ./experiment/_debug/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0032 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__load) took: 0.0018 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.4961 sec
> f(df_encode) took: 0.5687 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> f(df_encode) took: 0.5143 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> Init BERT-Head (Base)
  Memory Usage: 0.7529 MB
  Trainable parameters: 197378
  Input Dimension: 768
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=0.5154 	loss(eval)=0.5034 	f1(train)=0.6750 	f1(eval)=0.7500 	duration(epoch)=0:00:00.019307
@010: 	loss(train)=0.4348 	loss(eval)=0.3910 	f1(train)=0.8500 	f1(eval)=0.8750 	duration(epoch)=0:00:00.018895
@015: 	loss(train)=0.2379 	loss(eval)=0.2700 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.018507
@020: 	loss(train)=0.1273 	loss(eval)=0.1146 	f1(train)=1.0000 	f1(eval)=0.9750 	duration(epoch)=0:00:00.019147
@025: 	loss(train)=0.0945 	loss(eval)=0.0694 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018912
@030: 	loss(train)=0.0590 	loss(eval)=0.0421 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018677
@035: 	loss(train)=0.0291 	loss(eval)=0.0506 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.019050
@040: 	loss(train)=0.0201 	loss(eval)=0.0145 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.082415
@045: 	loss(train)=0.0209 	loss(eval)=0.0124 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018702
@050: 	loss(train)=0.0110 	loss(eval)=0.0131 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.019331
@055: 	loss(train)=0.0069 	loss(eval)=0.0132 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018438
@060: 	loss(train)=0.0158 	loss(eval)=0.0084 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018966
@065: 	loss(train)=0.0065 	loss(eval)=0.0072 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018876
@070: 	loss(train)=0.0038 	loss(eval)=0.0040 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018304
@075: 	loss(train)=0.0064 	loss(eval)=0.0038 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018974
@080: 	loss(train)=0.0041 	loss(eval)=0.0031 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018596
@085: 	loss(train)=0.0036 	loss(eval)=0.0040 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018862
@090: 	loss(train)=0.0030 	loss(eval)=0.0033 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.019168
@095: 	loss(train)=0.0038 	loss(eval)=0.0020 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.019350
@100: 	loss(train)=0.0021 	loss(eval)=0.0018 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018595
> Load best model based on evaluation loss.
> Init BERT-Head (Base)
  Memory Usage: 0.7529 MB
  Trainable parameters: 197378
  Input Dimension: 768
  Output Dimension: 2
@100: 	loss(train)=0.0021 	loss(eval)=0.0018 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.018595

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       27	 fp:        0 	 tn:       13	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       13	 fp:        0 	 tn:       27	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
