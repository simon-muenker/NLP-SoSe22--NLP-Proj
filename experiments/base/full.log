> Loaded logger: ./experiments/base/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> Load/Init from ./data/imdb.train.csv
> f(__load) took: 0.3281 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0293 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 78.5563 sec
> Init BERT-Head (Base), trainable parameters: 1538

[--- TRAIN -> ./data/imdb.train.csv ---]
@001: 	loss(train)=0.6803 	loss(eval)=0.6678 	f1(train)=0.5955 	f1(eval)=0.6683 	duration(epoch)=0:05:47.066672
@002: 	loss(train)=0.6558 	loss(eval)=0.6299 	f1(train)=0.6580 	f1(eval)=0.6896 	duration(epoch)=0:05:49.151103
@003: 	loss(train)=0.6331 	loss(eval)=0.6254 	f1(train)=0.6794 	f1(eval)=0.7098 	duration(epoch)=0:05:49.092549
@004: 	loss(train)=0.6211 	loss(eval)=0.6042 	f1(train)=0.6876 	f1(eval)=0.6909 	duration(epoch)=0:05:48.437086
@005: 	loss(train)=0.6112 	loss(eval)=0.5920 	f1(train)=0.7025 	f1(eval)=0.7150 	duration(epoch)=0:05:48.627943
@006: 	loss(train)=0.6094 	loss(eval)=0.5927 	f1(train)=0.7019 	f1(eval)=0.7126 	duration(epoch)=0:05:48.913027
@007: 	loss(train)=0.6080 	loss(eval)=0.5924 	f1(train)=0.7032 	f1(eval)=0.7203 	duration(epoch)=0:05:48.677686
@008: 	loss(train)=0.6004 	loss(eval)=0.5934 	f1(train)=0.7082 	f1(eval)=0.7344 	duration(epoch)=0:05:48.846545
@009: 	loss(train)=0.6005 	loss(eval)=0.5949 	f1(train)=0.7072 	f1(eval)=0.7299 	duration(epoch)=0:05:48.779389
@010: 	loss(train)=0.5975 	loss(eval)=0.5901 	f1(train)=0.7072 	f1(eval)=0.7219 	duration(epoch)=0:05:48.873061
@011: 	loss(train)=0.5982 	loss(eval)=0.5997 	f1(train)=0.7047 	f1(eval)=0.7336 	duration(epoch)=0:05:48.592719
@012: 	loss(train)=0.5879 	loss(eval)=0.5877 	f1(train)=0.7132 	f1(eval)=0.7291 	duration(epoch)=0:05:48.722793
@013: 	loss(train)=0.5877 	loss(eval)=0.5845 	f1(train)=0.7112 	f1(eval)=0.7267 	duration(epoch)=0:05:48.774545
@014: 	loss(train)=0.5905 	loss(eval)=0.5920 	f1(train)=0.7124 	f1(eval)=0.7360 	duration(epoch)=0:05:48.572648
@015: 	loss(train)=0.5879 	loss(eval)=0.5895 	f1(train)=0.7132 	f1(eval)=0.7150 	duration(epoch)=0:05:48.545444
@016: 	loss(train)=0.5836 	loss(eval)=0.5887 	f1(train)=0.7166 	f1(eval)=0.7283 	duration(epoch)=0:05:48.679912
@017: 	loss(train)=0.5800 	loss(eval)=0.5836 	f1(train)=0.7155 	f1(eval)=0.7336 	duration(epoch)=0:05:48.879906
@018: 	loss(train)=0.5797 	loss(eval)=0.5804 	f1(train)=0.7149 	f1(eval)=0.7324 	duration(epoch)=0:05:48.649619
@019: 	loss(train)=0.5763 	loss(eval)=0.5859 	f1(train)=0.7166 	f1(eval)=0.7316 	duration(epoch)=0:05:48.541382
@020: 	loss(train)=0.5769 	loss(eval)=0.5750 	f1(train)=0.7162 	f1(eval)=0.7259 	duration(epoch)=0:05:48.478128
> Load best model based on evaluation loss.
> Init BERT-Head (Base), trainable parameters: 1538
@014: 	loss(train)=0.5905 	loss(eval)=0.5920 	f1(train)=0.7124 	f1(eval)=0.7360 	duration(epoch)=0:05:48.572648

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     1826	 fp:      655 	 tn:     1826	 fn:      655	 pre=0.7360	 rec=0.7360	 f1=0.7360	 acc=0.7360
negative      	 tp:      891	 fp:      345 	 tn:      935	 fn:      310	 pre=0.7209	 rec=0.7419	 f1=0.7312	 acc=0.7360
positive      	 tp:      935	 fp:      310 	 tn:      891	 fn:      345	 pre=0.7510	 rec=0.7305	 f1=0.7406	 acc=0.7360
