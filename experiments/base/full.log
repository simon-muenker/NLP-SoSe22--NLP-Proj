> Loaded logger: ./experiments/base/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2560 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 22412 
  Memory Usage: 30.5957 MB
> f(__load) took: 0.0305 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2491 
  Memory Usage: 3.3220 MB

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.3052 sec
> f(df_encode) took: 285.7095 sec
> Memory Usage (w/ Embeds): 36.1677 MB
> f(df_encode) took: 31.8618 sec
> Memory Usage (w/ Embeds): 3.9439 MB
> Init BERT-Head (Base)
  Memory Usage: 0.0059 MB
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@005: 	loss(train)=0.2869 	loss(eval)=0.3569 	f1(train)=0.8805 	f1(eval)=0.8475 	duration(epoch)=0:00:04.595978
@010: 	loss(train)=0.2747 	loss(eval)=0.3223 	f1(train)=0.8870 	f1(eval)=0.8651 	duration(epoch)=0:00:04.555869
@015: 	loss(train)=0.2692 	loss(eval)=0.3267 	f1(train)=0.8892 	f1(eval)=0.8651 	duration(epoch)=0:00:04.631507
@020: 	loss(train)=0.2653 	loss(eval)=0.2273 	f1(train)=0.8918 	f1(eval)=0.9153 	duration(epoch)=0:00:04.627090
@025: 	loss(train)=0.2617 	loss(eval)=0.2586 	f1(train)=0.8931 	f1(eval)=0.8996 	duration(epoch)=0:00:04.653151
@030: 	loss(train)=0.2596 	loss(eval)=0.2556 	f1(train)=0.8935 	f1(eval)=0.8992 	duration(epoch)=0:00:04.680283
@035: 	loss(train)=0.2573 	loss(eval)=0.2840 	f1(train)=0.8939 	f1(eval)=0.8844 	duration(epoch)=0:00:04.670370
@040: 	loss(train)=0.2567 	loss(eval)=0.2726 	f1(train)=0.8941 	f1(eval)=0.8888 	duration(epoch)=0:00:04.640532
@045: 	loss(train)=0.2550 	loss(eval)=0.2221 	f1(train)=0.8948 	f1(eval)=0.9129 	duration(epoch)=0:00:04.665320
@050: 	loss(train)=0.2553 	loss(eval)=0.3704 	f1(train)=0.8947 	f1(eval)=0.8475 	duration(epoch)=0:00:04.666393
@055: 	loss(train)=0.2531 	loss(eval)=0.2873 	f1(train)=0.8948 	f1(eval)=0.8820 	duration(epoch)=0:00:04.632361
@060: 	loss(train)=0.2523 	loss(eval)=0.3183 	f1(train)=0.8959 	f1(eval)=0.8675 	duration(epoch)=0:00:04.582795
@065: 	loss(train)=0.2507 	loss(eval)=0.2582 	f1(train)=0.8950 	f1(eval)=0.8944 	duration(epoch)=0:00:04.603716
@070: 	loss(train)=0.2505 	loss(eval)=0.2336 	f1(train)=0.8951 	f1(eval)=0.9073 	duration(epoch)=0:00:04.629378
@075: 	loss(train)=0.2485 	loss(eval)=0.3204 	f1(train)=0.8962 	f1(eval)=0.8735 	duration(epoch)=0:00:04.618058
@080: 	loss(train)=0.2492 	loss(eval)=0.2995 	f1(train)=0.8959 	f1(eval)=0.8760 	duration(epoch)=0:00:04.659531
@085: 	loss(train)=0.2483 	loss(eval)=0.3412 	f1(train)=0.8964 	f1(eval)=0.8583 	duration(epoch)=0:00:04.648739
@090: 	loss(train)=0.2473 	loss(eval)=0.3530 	f1(train)=0.8966 	f1(eval)=0.8543 	duration(epoch)=0:00:04.626572
@095: 	loss(train)=0.2475 	loss(eval)=0.3774 	f1(train)=0.8955 	f1(eval)=0.8438 	duration(epoch)=0:00:04.633494
@100: 	loss(train)=0.2460 	loss(eval)=0.2671 	f1(train)=0.8978 	f1(eval)=0.8904 	duration(epoch)=0:00:04.634666
> Load best model based on evaluation loss.
> Init BERT-Head (Base)
  Memory Usage: 0.0059 MB
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2
@020: 	loss(train)=0.2653 	loss(eval)=0.2273 	f1(train)=0.8918 	f1(eval)=0.9153 	duration(epoch)=0:00:04.627090

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2280	 fp:      211 	 tn:     2280	 fn:      211	 pre=0.9153	 rec=0.9153	 f1=0.9153	 acc=0.9153
negative      	 tp:     2280	 fp:        0 	 tn:        0	 fn:      211	 pre=1.0000	 rec=0.9153	 f1=0.9558	 acc=0.9153
positive      	 tp:        0	 fp:      211 	 tn:     2280	 fn:        0	 pre=0.0000	 rec=0.0000	 f1=0.0000	 acc=0.9153
