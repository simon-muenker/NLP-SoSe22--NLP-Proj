> Loaded logger: ./experiments/hybrid/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0515 sec
> f(__tokenize) took: 1.0470 sec
> f(__ngram) took: 0.5163 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0647 sec
> f(__tokenize) took: 1.2982 sec
> f(__ngram) took: 0.6496 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0652 sec
> f(__tokenize) took: 1.3192 sec
> f(__ngram) took: 1.6010 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.7396 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.4707 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 5.9534 sec
> Apply Space Pipeline to: ./data/imdb.train.0.100.csv
> f(apply_spacy) took: 154.5881 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.4626 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply_spacy) took: 189.1460 sec

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.4401 	loss(eval)=0.3715 	f1(train)=0.7950 	f1(eval)=0.8452 	duration(epoch)=0:02:02.964575
@002: 	loss(train)=0.3794 	loss(eval)=0.3665 	f1(train)=0.8490 	f1(eval)=0.8506 	duration(epoch)=0:02:03.811173
@003: 	loss(train)=0.3566 	loss(eval)=0.3529 	f1(train)=0.8518 	f1(eval)=0.8544 	duration(epoch)=0:02:03.538004
@004: 	loss(train)=0.3524 	loss(eval)=0.3517 	f1(train)=0.8582 	f1(eval)=0.8594 	duration(epoch)=0:02:03.228421
@005: 	loss(train)=0.3352 	loss(eval)=0.3715 	f1(train)=0.8652 	f1(eval)=0.8602 	duration(epoch)=0:02:02.770662
@006: 	loss(train)=0.3423 	loss(eval)=0.3866 	f1(train)=0.8643 	f1(eval)=0.8524 	duration(epoch)=0:02:01.886005
@007: 	loss(train)=0.3224 	loss(eval)=0.3808 	f1(train)=0.8710 	f1(eval)=0.8556 	duration(epoch)=0:02:01.845425
@008: 	loss(train)=0.3195 	loss(eval)=0.3562 	f1(train)=0.8770 	f1(eval)=0.8602 	duration(epoch)=0:02:01.308710
@009: 	loss(train)=0.3192 	loss(eval)=0.3723 	f1(train)=0.8770 	f1(eval)=0.8562 	duration(epoch)=0:02:01.736075
@010: 	loss(train)=0.3086 	loss(eval)=0.3786 	f1(train)=0.8825 	f1(eval)=0.8546 	duration(epoch)=0:02:01.833766
@011: 	loss(train)=0.3013 	loss(eval)=0.3827 	f1(train)=0.8828 	f1(eval)=0.8642 	duration(epoch)=0:02:01.037619
@012: 	loss(train)=0.2929 	loss(eval)=0.3970 	f1(train)=0.8860 	f1(eval)=0.8578 	duration(epoch)=0:02:01.693181
> Warning: Training interrupted by user!
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@011: 	loss(train)=0.3013 	loss(eval)=0.3827 	f1(train)=0.8828 	f1(eval)=0.8642 	duration(epoch)=0:02:01.037619

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4321	 fp:      679 	 tn:     4321	 fn:      679	 pre=0.8642	 rec=0.8642	 f1=0.8642	 acc=0.8642
negative      	 tp:     2123	 fp:      325 	 tn:     2198	 fn:      354	 pre=0.8672	 rec=0.8571	 f1=0.8621	 acc=0.8642
positive      	 tp:     2198	 fp:      354 	 tn:     2123	 fn:      325	 pre=0.8613	 rec=0.8712	 f1=0.8662	 acc=0.8642
