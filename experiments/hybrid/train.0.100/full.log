> Loaded logger: ./experiments/hybrid/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0496 sec
> f(__tokenize) took: 1.0473 sec
> f(__ngram) took: 0.5535 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0630 sec
> f(__tokenize) took: 1.2627 sec
> f(__ngram) took: 0.6030 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0617 sec
> f(__tokenize) took: 1.2700 sec
> f(__ngram) took: 0.7195 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.8548 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.4222 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 7.9069 sec
> Apply Space Pipeline to: ./data/imdb.train.0.100.csv
> f(apply) took: 126.1197 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 8.0189 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 155.4826 sec

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.4153 	loss(eval)=0.4076 	f1(train)=0.8103 	f1(eval)=0.8266 	duration(epoch)=0:02:00.041290
@002: 	loss(train)=0.3616 	loss(eval)=0.3874 	f1(train)=0.8393 	f1(eval)=0.8270 	duration(epoch)=0:02:01.291258
@003: 	loss(train)=0.3432 	loss(eval)=0.3898 	f1(train)=0.8510 	f1(eval)=0.8302 	duration(epoch)=0:02:00.441399
@004: 	loss(train)=0.3305 	loss(eval)=0.4370 	f1(train)=0.8537 	f1(eval)=0.8234 	duration(epoch)=0:02:00.765380
@005: 	loss(train)=0.3225 	loss(eval)=0.4157 	f1(train)=0.8640 	f1(eval)=0.8252 	duration(epoch)=0:02:00.898787
@006: 	loss(train)=0.3063 	loss(eval)=0.3956 	f1(train)=0.8705 	f1(eval)=0.8318 	duration(epoch)=0:02:01.049419
@007: 	loss(train)=0.2910 	loss(eval)=0.4149 	f1(train)=0.8755 	f1(eval)=0.8208 	duration(epoch)=0:02:00.343971
@008: 	loss(train)=0.2785 	loss(eval)=0.4320 	f1(train)=0.8830 	f1(eval)=0.8274 	duration(epoch)=0:01:59.804772
@009: 	loss(train)=0.2600 	loss(eval)=0.4374 	f1(train)=0.8918 	f1(eval)=0.8314 	duration(epoch)=0:02:00.849439
@010: 	loss(train)=0.2442 	loss(eval)=0.4371 	f1(train)=0.9015 	f1(eval)=0.8314 	duration(epoch)=0:02:01.400362
@011: 	loss(train)=0.2232 	loss(eval)=0.4601 	f1(train)=0.9093 	f1(eval)=0.8228 	duration(epoch)=0:02:00.720276
@012: 	loss(train)=0.2074 	loss(eval)=0.4998 	f1(train)=0.9197 	f1(eval)=0.8284 	duration(epoch)=0:02:01.171863
@013: 	loss(train)=0.2002 	loss(eval)=0.5342 	f1(train)=0.9207 	f1(eval)=0.8038 	duration(epoch)=0:02:00.694921
@014: 	loss(train)=0.1838 	loss(eval)=0.5687 	f1(train)=0.9265 	f1(eval)=0.8290 	duration(epoch)=0:01:59.441414
@015: 	loss(train)=0.1687 	loss(eval)=0.5622 	f1(train)=0.9343 	f1(eval)=0.8262 	duration(epoch)=0:01:59.944239
@016: 	loss(train)=0.1587 	loss(eval)=0.5640 	f1(train)=0.9345 	f1(eval)=0.8252 	duration(epoch)=0:01:59.905969
@017: 	loss(train)=0.1408 	loss(eval)=0.6213 	f1(train)=0.9427 	f1(eval)=0.8254 	duration(epoch)=0:01:59.497693
@018: 	loss(train)=0.1432 	loss(eval)=0.6597 	f1(train)=0.9468 	f1(eval)=0.8260 	duration(epoch)=0:01:59.513092
@019: 	loss(train)=0.1419 	loss(eval)=0.6221 	f1(train)=0.9450 	f1(eval)=0.8262 	duration(epoch)=0:01:59.582374
@020: 	loss(train)=0.1114 	loss(eval)=0.7688 	f1(train)=0.9563 	f1(eval)=0.8098 	duration(epoch)=0:02:00.021661
@021: 	loss(train)=0.1192 	loss(eval)=0.7013 	f1(train)=0.9563 	f1(eval)=0.8180 	duration(epoch)=0:01:59.507404
@022: 	loss(train)=0.1174 	loss(eval)=0.7521 	f1(train)=0.9573 	f1(eval)=0.8232 	duration(epoch)=0:02:00.484583
@023: 	loss(train)=0.1221 	loss(eval)=0.7067 	f1(train)=0.9575 	f1(eval)=0.8218 	duration(epoch)=0:01:59.474308
@024: 	loss(train)=0.0917 	loss(eval)=0.8300 	f1(train)=0.9677 	f1(eval)=0.8272 	duration(epoch)=0:01:59.841579
@025: 	loss(train)=0.0932 	loss(eval)=0.8505 	f1(train)=0.9617 	f1(eval)=0.8304 	duration(epoch)=0:02:00.448144
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@006: 	loss(train)=0.3063 	loss(eval)=0.3956 	f1(train)=0.8705 	f1(eval)=0.8318 	duration(epoch)=0:02:01.049419

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4159	 fp:      841 	 tn:     4159	 fn:      841	 pre=0.8318	 rec=0.8318	 f1=0.8318	 acc=0.8318
negative      	 tp:     1992	 fp:      370 	 tn:     2167	 fn:      471	 pre=0.8434	 rec=0.8088	 f1=0.8257	 acc=0.8318
positive      	 tp:     2167	 fp:      471 	 tn:     1992	 fn:      370	 pre=0.8215	 rec=0.8542	 f1=0.8375	 acc=0.8318
