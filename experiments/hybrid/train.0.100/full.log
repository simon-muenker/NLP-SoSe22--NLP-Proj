> Loaded logger: ./experiments/hybrid/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0502 sec
> f(__tokenize) took: 1.0933 sec
> f(__ngram) took: 0.4835 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0729 sec
> f(__tokenize) took: 1.2259 sec
> f(__ngram) took: 0.6648 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0674 sec
> f(__tokenize) took: 1.3363 sec
> f(__ngram) took: 1.0825 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.5848 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tagger', 'ner']
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.5233 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 5.8792 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.5002 sec

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.4472 	loss(eval)=0.3637 	f1(train)=0.7930 	f1(eval)=0.8424 	duration(epoch)=0:02:01.741621
@002: 	loss(train)=0.3749 	loss(eval)=0.3789 	f1(train)=0.8393 	f1(eval)=0.8418 	duration(epoch)=0:02:02.094588
@003: 	loss(train)=0.3584 	loss(eval)=0.3678 	f1(train)=0.8535 	f1(eval)=0.8540 	duration(epoch)=0:02:01.530269
@004: 	loss(train)=0.3466 	loss(eval)=0.3684 	f1(train)=0.8568 	f1(eval)=0.8510 	duration(epoch)=0:02:02.390221
@005: 	loss(train)=0.3408 	loss(eval)=0.3555 	f1(train)=0.8620 	f1(eval)=0.8600 	duration(epoch)=0:02:01.894852
@006: 	loss(train)=0.3250 	loss(eval)=0.3508 	f1(train)=0.8675 	f1(eval)=0.8574 	duration(epoch)=0:02:01.839140
@007: 	loss(train)=0.3195 	loss(eval)=0.3651 	f1(train)=0.8722 	f1(eval)=0.8552 	duration(epoch)=0:02:01.657143
@008: 	loss(train)=0.3187 	loss(eval)=0.3768 	f1(train)=0.8777 	f1(eval)=0.8528 	duration(epoch)=0:02:00.749228
@009: 	loss(train)=0.3136 	loss(eval)=0.3622 	f1(train)=0.8765 	f1(eval)=0.8608 	duration(epoch)=0:02:02.436714
@010: 	loss(train)=0.3088 	loss(eval)=0.3587 	f1(train)=0.8800 	f1(eval)=0.8574 	duration(epoch)=0:02:01.491986
@011: 	loss(train)=0.2985 	loss(eval)=0.3582 	f1(train)=0.8918 	f1(eval)=0.8598 	duration(epoch)=0:02:01.647201
@012: 	loss(train)=0.2933 	loss(eval)=0.3814 	f1(train)=0.8848 	f1(eval)=0.8590 	duration(epoch)=0:02:03.059545
@013: 	loss(train)=0.2852 	loss(eval)=0.3767 	f1(train)=0.8892 	f1(eval)=0.8592 	duration(epoch)=0:02:01.648400
@014: 	loss(train)=0.2751 	loss(eval)=0.3903 	f1(train)=0.8980 	f1(eval)=0.8566 	duration(epoch)=0:02:01.504510
@015: 	loss(train)=0.2721 	loss(eval)=0.3870 	f1(train)=0.9015 	f1(eval)=0.8598 	duration(epoch)=0:02:02.015021
@016: 	loss(train)=0.2627 	loss(eval)=0.4085 	f1(train)=0.8998 	f1(eval)=0.8588 	duration(epoch)=0:02:01.814354
@017: 	loss(train)=0.2584 	loss(eval)=0.4070 	f1(train)=0.9015 	f1(eval)=0.8610 	duration(epoch)=0:02:02.571062
@018: 	loss(train)=0.2522 	loss(eval)=0.4442 	f1(train)=0.9042 	f1(eval)=0.8588 	duration(epoch)=0:02:03.900658
@019: 	loss(train)=0.2435 	loss(eval)=0.4611 	f1(train)=0.9070 	f1(eval)=0.8588 	duration(epoch)=0:02:03.091787
@020: 	loss(train)=0.2426 	loss(eval)=0.4349 	f1(train)=0.9130 	f1(eval)=0.8614 	duration(epoch)=0:02:03.191120
@021: 	loss(train)=0.2232 	loss(eval)=0.4720 	f1(train)=0.9185 	f1(eval)=0.8624 	duration(epoch)=0:02:03.360893
@022: 	loss(train)=0.2135 	loss(eval)=0.5080 	f1(train)=0.9275 	f1(eval)=0.8516 	duration(epoch)=0:02:01.993033
@023: 	loss(train)=0.2103 	loss(eval)=0.5098 	f1(train)=0.9253 	f1(eval)=0.8470 	duration(epoch)=0:02:01.939899
@024: 	loss(train)=0.2043 	loss(eval)=0.4990 	f1(train)=0.9297 	f1(eval)=0.8614 	duration(epoch)=0:02:02.533415
@025: 	loss(train)=0.1992 	loss(eval)=0.5436 	f1(train)=0.9305 	f1(eval)=0.8622 	duration(epoch)=0:02:02.655974
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908
@021: 	loss(train)=0.2232 	loss(eval)=0.4720 	f1(train)=0.9185 	f1(eval)=0.8624 	duration(epoch)=0:02:03.360893

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4312	 fp:      688 	 tn:     4312	 fn:      688	 pre=0.8624	 rec=0.8624	 f1=0.8624	 acc=0.8624
negative      	 tp:     2171	 fp:      382 	 tn:     2141	 fn:      306	 pre=0.8504	 rec=0.8765	 f1=0.8632	 acc=0.8624
positive      	 tp:     2141	 fp:      306 	 tn:     2171	 fn:      382	 pre=0.8749	 rec=0.8486	 f1=0.8616	 acc=0.8624
