> Loaded logger: ./experiments/hybrid/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0511 sec
> f(__tokenize) took: 1.0353 sec
> f(__ngram) took: 0.7528 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0826 sec
> f(__tokenize) took: 1.1530 sec
> f(__ngram) took: 0.6120 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0613 sec
> f(__tokenize) took: 1.3564 sec
> f(__ngram) took: 1.0011 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.4437 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.5251 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 7.3842 sec
> Apply Space Pipeline to: ./data/imdb.train.0.100.csv
> f(apply) took: 125.5135 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 8.1011 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 152.5226 sec

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.1495 	loss(eval)=0.1376 	f1(train)=0.9503 	f1(eval)=0.9584 	duration(epoch)=0:01:59.534694
@002: 	loss(train)=0.1371 	loss(eval)=0.1241 	f1(train)=0.9553 	f1(eval)=0.9568 	duration(epoch)=0:01:59.143438
@003: 	loss(train)=0.1312 	loss(eval)=0.1383 	f1(train)=0.9607 	f1(eval)=0.9592 	duration(epoch)=0:02:00.703839
@004: 	loss(train)=0.1316 	loss(eval)=0.1251 	f1(train)=0.9607 	f1(eval)=0.9590 	duration(epoch)=0:02:00.276250
@005: 	loss(train)=0.1275 	loss(eval)=0.1194 	f1(train)=0.9623 	f1(eval)=0.9594 	duration(epoch)=0:02:00.058460
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32
@005: 	loss(train)=0.1275 	loss(eval)=0.1194 	f1(train)=0.9623 	f1(eval)=0.9594 	duration(epoch)=0:02:00.058460

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4797	 fp:      203 	 tn:     4797	 fn:      203	 pre=0.9594	 rec=0.9594	 f1=0.9594	 acc=0.9594
negative      	 tp:     2364	 fp:      104 	 tn:     2433	 fn:       99	 pre=0.9579	 rec=0.9598	 f1=0.9588	 acc=0.9594
positive      	 tp:     2433	 fp:       99 	 tn:     2364	 fn:      104	 pre=0.9609	 rec=0.9590	 f1=0.9600	 acc=0.9594
