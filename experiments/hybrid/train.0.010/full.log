> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0096 sec
> f(__tokenize) took: 0.4114 sec
> f(__ngram) took: 0.3087 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0834 sec
> f(__tokenize) took: 1.2540 sec
> f(__ngram) took: 0.5464 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0658 sec
> f(__tokenize) took: 1.2593 sec
> f(__ngram) took: 0.6552 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.5291 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tagger', 'ner']
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0451 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 0.6291 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.8291 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6557 	loss(eval)=0.5982 	f1(train)=0.6175 	f1(eval)=0.7126 	duration(epoch)=0:01:12.082529
@002: 	loss(train)=0.5466 	loss(eval)=0.5263 	f1(train)=0.7525 	f1(eval)=0.7254 	duration(epoch)=0:01:12.475306
@003: 	loss(train)=0.4537 	loss(eval)=0.4228 	f1(train)=0.7950 	f1(eval)=0.8212 	duration(epoch)=0:01:12.502053
@004: 	loss(train)=0.4122 	loss(eval)=0.4029 	f1(train)=0.8225 	f1(eval)=0.8232 	duration(epoch)=0:01:12.837790
@005: 	loss(train)=0.3833 	loss(eval)=0.3828 	f1(train)=0.8450 	f1(eval)=0.8382 	duration(epoch)=0:01:12.537329
@006: 	loss(train)=0.3424 	loss(eval)=0.3846 	f1(train)=0.8600 	f1(eval)=0.8314 	duration(epoch)=0:01:12.638096
@007: 	loss(train)=0.3300 	loss(eval)=0.3736 	f1(train)=0.8600 	f1(eval)=0.8394 	duration(epoch)=0:01:12.660958
@008: 	loss(train)=0.3415 	loss(eval)=0.3832 	f1(train)=0.8500 	f1(eval)=0.8350 	duration(epoch)=0:01:12.226014
@009: 	loss(train)=0.3042 	loss(eval)=0.4055 	f1(train)=0.8700 	f1(eval)=0.8250 	duration(epoch)=0:01:13.118664
@010: 	loss(train)=0.2690 	loss(eval)=0.3912 	f1(train)=0.8825 	f1(eval)=0.8384 	duration(epoch)=0:01:11.927181
@011: 	loss(train)=0.2769 	loss(eval)=0.3895 	f1(train)=0.8850 	f1(eval)=0.8396 	duration(epoch)=0:01:12.506791
@012: 	loss(train)=0.2590 	loss(eval)=0.3955 	f1(train)=0.8900 	f1(eval)=0.8390 	duration(epoch)=0:01:12.802211
@013: 	loss(train)=0.2473 	loss(eval)=0.3988 	f1(train)=0.9025 	f1(eval)=0.8414 	duration(epoch)=0:01:12.412328
@014: 	loss(train)=0.2431 	loss(eval)=0.4239 	f1(train)=0.8900 	f1(eval)=0.8292 	duration(epoch)=0:01:12.413865
@015: 	loss(train)=0.2379 	loss(eval)=0.4161 	f1(train)=0.9100 	f1(eval)=0.8372 	duration(epoch)=0:01:12.474023
@016: 	loss(train)=0.2236 	loss(eval)=0.4692 	f1(train)=0.8975 	f1(eval)=0.8192 	duration(epoch)=0:01:12.189507
@017: 	loss(train)=0.2204 	loss(eval)=0.4557 	f1(train)=0.9200 	f1(eval)=0.8272 	duration(epoch)=0:01:11.404514
@018: 	loss(train)=0.1935 	loss(eval)=0.4353 	f1(train)=0.9275 	f1(eval)=0.8374 	duration(epoch)=0:01:11.941704
@019: 	loss(train)=0.1801 	loss(eval)=0.4643 	f1(train)=0.9275 	f1(eval)=0.8296 	duration(epoch)=0:01:11.852662
@020: 	loss(train)=0.1854 	loss(eval)=0.4621 	f1(train)=0.9275 	f1(eval)=0.8350 	duration(epoch)=0:01:11.694956
@021: 	loss(train)=0.1518 	loss(eval)=0.4904 	f1(train)=0.9325 	f1(eval)=0.8296 	duration(epoch)=0:01:11.684062
@022: 	loss(train)=0.1539 	loss(eval)=0.4687 	f1(train)=0.9375 	f1(eval)=0.8346 	duration(epoch)=0:01:11.910906
@023: 	loss(train)=0.1330 	loss(eval)=0.4921 	f1(train)=0.9500 	f1(eval)=0.8358 	duration(epoch)=0:01:12.050287
@024: 	loss(train)=0.1145 	loss(eval)=0.5416 	f1(train)=0.9650 	f1(eval)=0.8280 	duration(epoch)=0:01:11.541893
@025: 	loss(train)=0.1161 	loss(eval)=0.6156 	f1(train)=0.9625 	f1(eval)=0.8094 	duration(epoch)=0:01:12.013803
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908
@013: 	loss(train)=0.2473 	loss(eval)=0.3988 	f1(train)=0.9025 	f1(eval)=0.8414 	duration(epoch)=0:01:12.412328

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4207	 fp:      793 	 tn:     4207	 fn:      793	 pre=0.8414	 rec=0.8414	 f1=0.8414	 acc=0.8414
negative      	 tp:     2154	 fp:      470 	 tn:     2053	 fn:      323	 pre=0.8209	 rec=0.8696	 f1=0.8445	 acc=0.8414
positive      	 tp:     2053	 fp:      323 	 tn:     2154	 fn:      470	 pre=0.8641	 rec=0.8137	 f1=0.8381	 acc=0.8414
