> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0113 sec
> f(__tokenize) took: 0.4236 sec
> f(__ngram) took: 0.3088 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0833 sec
> f(__tokenize) took: 1.3020 sec
> f(__ngram) took: 0.5429 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0690 sec
> f(__tokenize) took: 1.2652 sec
> f(__ngram) took: 0.6723 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.6240 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0426 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 6.8199 sec
> Apply Space Pipeline to: ./data/imdb.train.0.010.csv
> f(apply) took: 14.8856 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 8.0186 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 155.7744 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.1283 	loss(eval)=0.1592 	f1(train)=0.9625 	f1(eval)=0.9570 	duration(epoch)=0:01:10.617871
@002: 	loss(train)=0.0756 	loss(eval)=0.1627 	f1(train)=0.9725 	f1(eval)=0.9568 	duration(epoch)=0:01:11.210837
@003: 	loss(train)=0.0630 	loss(eval)=0.1737 	f1(train)=0.9775 	f1(eval)=0.9578 	duration(epoch)=0:01:11.735938
@004: 	loss(train)=0.0557 	loss(eval)=0.1842 	f1(train)=0.9775 	f1(eval)=0.9570 	duration(epoch)=0:01:11.661769
@005: 	loss(train)=0.0450 	loss(eval)=0.1956 	f1(train)=0.9825 	f1(eval)=0.9582 	duration(epoch)=0:01:11.896167
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32
@005: 	loss(train)=0.0450 	loss(eval)=0.1956 	f1(train)=0.9825 	f1(eval)=0.9582 	duration(epoch)=0:01:11.896167

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4791	 fp:      209 	 tn:     4791	 fn:      209	 pre=0.9582	 rec=0.9582	 f1=0.9582	 acc=0.9582
negative      	 tp:     2368	 fp:      114 	 tn:     2423	 fn:       95	 pre=0.9541	 rec=0.9614	 f1=0.9577	 acc=0.9582
positive      	 tp:     2423	 fp:       95 	 tn:     2368	 fn:      114	 pre=0.9623	 rec=0.9551	 f1=0.9587	 acc=0.9582
