> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0101 sec
> f(__tokenize) took: 0.4204 sec
> f(__ngram) took: 0.3031 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0652 sec
> f(__tokenize) took: 1.1920 sec
> f(__ngram) took: 0.5285 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0603 sec
> f(__tokenize) took: 1.2442 sec
> f(__ngram) took: 0.6602 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.5785 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0456 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 6.5116 sec
> Apply Space Pipeline to: ./data/imdb.train.0.010.csv
> f(apply) took: 14.8000 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.9186 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 156.9474 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6383 	loss(eval)=0.4911 	f1(train)=0.6150 	f1(eval)=0.7926 	duration(epoch)=0:01:11.004818
@002: 	loss(train)=0.2815 	loss(eval)=0.4539 	f1(train)=0.8850 	f1(eval)=0.7962 	duration(epoch)=0:01:11.571317
@003: 	loss(train)=0.1604 	loss(eval)=0.4890 	f1(train)=0.9425 	f1(eval)=0.7940 	duration(epoch)=0:01:11.488696
@004: 	loss(train)=0.1096 	loss(eval)=0.5932 	f1(train)=0.9650 	f1(eval)=0.7758 	duration(epoch)=0:01:11.766315
@005: 	loss(train)=0.0725 	loss(eval)=0.6686 	f1(train)=0.9775 	f1(eval)=0.7712 	duration(epoch)=0:01:10.956886
@006: 	loss(train)=0.0679 	loss(eval)=0.6490 	f1(train)=0.9725 	f1(eval)=0.7972 	duration(epoch)=0:01:11.160471
@007: 	loss(train)=0.0522 	loss(eval)=0.9960 	f1(train)=0.9850 	f1(eval)=0.7358 	duration(epoch)=0:01:11.049256
@008: 	loss(train)=0.0475 	loss(eval)=0.7845 	f1(train)=0.9850 	f1(eval)=0.7834 	duration(epoch)=0:01:11.373756
@009: 	loss(train)=0.0253 	loss(eval)=0.7918 	f1(train)=0.9900 	f1(eval)=0.7908 	duration(epoch)=0:01:11.273817
@010: 	loss(train)=0.0167 	loss(eval)=0.9236 	f1(train)=0.9950 	f1(eval)=0.7660 	duration(epoch)=0:01:11.081737
@011: 	loss(train)=0.0148 	loss(eval)=0.9390 	f1(train)=0.9975 	f1(eval)=0.7734 	duration(epoch)=0:01:11.156338
@012: 	loss(train)=0.0143 	loss(eval)=1.0288 	f1(train)=0.9975 	f1(eval)=0.7688 	duration(epoch)=0:01:11.493999
@013: 	loss(train)=0.0106 	loss(eval)=1.0252 	f1(train)=0.9975 	f1(eval)=0.7732 	duration(epoch)=0:01:11.387417
@014: 	loss(train)=0.0109 	loss(eval)=1.1282 	f1(train)=0.9950 	f1(eval)=0.7624 	duration(epoch)=0:01:11.148697
@015: 	loss(train)=0.0080 	loss(eval)=1.0756 	f1(train)=0.9975 	f1(eval)=0.7810 	duration(epoch)=0:01:11.057176
@016: 	loss(train)=0.0079 	loss(eval)=1.3445 	f1(train)=1.0000 	f1(eval)=0.7496 	duration(epoch)=0:01:11.572758
@017: 	loss(train)=0.0038 	loss(eval)=1.2097 	f1(train)=1.0000 	f1(eval)=0.7678 	duration(epoch)=0:01:11.119756
@018: 	loss(train)=0.0011 	loss(eval)=1.1894 	f1(train)=1.0000 	f1(eval)=0.7742 	duration(epoch)=0:01:11.499467
@019: 	loss(train)=0.0026 	loss(eval)=1.3623 	f1(train)=1.0000 	f1(eval)=0.7520 	duration(epoch)=0:01:11.239119
@020: 	loss(train)=0.0012 	loss(eval)=1.1616 	f1(train)=1.0000 	f1(eval)=0.7870 	duration(epoch)=0:01:10.964295
@021: 	loss(train)=0.0029 	loss(eval)=1.3417 	f1(train)=1.0000 	f1(eval)=0.7658 	duration(epoch)=0:01:11.549316
@022: 	loss(train)=0.0023 	loss(eval)=1.2674 	f1(train)=1.0000 	f1(eval)=0.7800 	duration(epoch)=0:01:11.488813
@023: 	loss(train)=0.0028 	loss(eval)=1.4016 	f1(train)=1.0000 	f1(eval)=0.7704 	duration(epoch)=0:01:11.447218
@024: 	loss(train)=0.0010 	loss(eval)=1.2843 	f1(train)=1.0000 	f1(eval)=0.7808 	duration(epoch)=0:01:11.509494
@025: 	loss(train)=0.0030 	loss(eval)=1.3162 	f1(train)=1.0000 	f1(eval)=0.7814 	duration(epoch)=0:01:11.307808
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@006: 	loss(train)=0.0679 	loss(eval)=0.6490 	f1(train)=0.9725 	f1(eval)=0.7972 	duration(epoch)=0:01:11.160471

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3986	 fp:     1014 	 tn:     3986	 fn:     1014	 pre=0.7972	 rec=0.7972	 f1=0.7972	 acc=0.7972
negative      	 tp:     1868	 fp:      419 	 tn:     2118	 fn:      595	 pre=0.8168	 rec=0.7584	 f1=0.7865	 acc=0.7972
positive      	 tp:     2118	 fp:      595 	 tn:     1868	 fn:      419	 pre=0.7807	 rec=0.8348	 f1=0.8069	 acc=0.7972
