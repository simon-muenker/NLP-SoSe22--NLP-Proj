> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0092 sec
> f(__tokenize) took: 0.4060 sec
> f(__ngram) took: 0.3028 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0837 sec
> f(__tokenize) took: 1.2057 sec
> f(__ngram) took: 0.5582 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0648 sec
> f(__tokenize) took: 1.2574 sec
> f(__ngram) took: 0.6070 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.5926 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 30

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0415 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 0.6239 sec
> Apply Space Pipeline to: ./data/imdb.train.0.010.csv
> f(apply_spacy) took: 13.5160 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.7192 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply_spacy) took: 158.3014 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6712 	loss(eval)=0.6210 	f1(train)=0.5900 	f1(eval)=0.7840 	duration(epoch)=0:01:10.576762
@002: 	loss(train)=0.5740 	loss(eval)=0.5437 	f1(train)=0.7850 	f1(eval)=0.7318 	duration(epoch)=0:01:11.627116
@003: 	loss(train)=0.4814 	loss(eval)=0.4536 	f1(train)=0.7900 	f1(eval)=0.8058 	duration(epoch)=0:01:10.984225
@004: 	loss(train)=0.4297 	loss(eval)=0.4041 	f1(train)=0.8150 	f1(eval)=0.8298 	duration(epoch)=0:01:11.346396
@005: 	loss(train)=0.3746 	loss(eval)=0.3864 	f1(train)=0.8400 	f1(eval)=0.8338 	duration(epoch)=0:01:11.913533
@006: 	loss(train)=0.3676 	loss(eval)=0.3786 	f1(train)=0.8475 	f1(eval)=0.8364 	duration(epoch)=0:01:11.786627
@007: 	loss(train)=0.3207 	loss(eval)=0.3716 	f1(train)=0.8700 	f1(eval)=0.8412 	duration(epoch)=0:01:11.341797
@008: 	loss(train)=0.3102 	loss(eval)=0.3744 	f1(train)=0.8750 	f1(eval)=0.8406 	duration(epoch)=0:01:11.926018
@009: 	loss(train)=0.3167 	loss(eval)=0.3775 	f1(train)=0.8775 	f1(eval)=0.8372 	duration(epoch)=0:01:11.706557
@010: 	loss(train)=0.2776 	loss(eval)=0.4015 	f1(train)=0.8900 	f1(eval)=0.8326 	duration(epoch)=0:01:11.751803
@011: 	loss(train)=0.2841 	loss(eval)=0.3873 	f1(train)=0.8800 	f1(eval)=0.8402 	duration(epoch)=0:01:11.810601
@012: 	loss(train)=0.2801 	loss(eval)=0.4008 	f1(train)=0.8875 	f1(eval)=0.8316 	duration(epoch)=0:01:11.941648
@013: 	loss(train)=0.2464 	loss(eval)=0.3928 	f1(train)=0.9000 	f1(eval)=0.8372 	duration(epoch)=0:01:11.973238
@014: 	loss(train)=0.2383 	loss(eval)=0.4241 	f1(train)=0.9025 	f1(eval)=0.8276 	duration(epoch)=0:01:11.257046
@015: 	loss(train)=0.2428 	loss(eval)=0.4054 	f1(train)=0.9025 	f1(eval)=0.8418 	duration(epoch)=0:01:11.899148
@016: 	loss(train)=0.2169 	loss(eval)=0.4039 	f1(train)=0.9225 	f1(eval)=0.8426 	duration(epoch)=0:01:11.447401
@017: 	loss(train)=0.2172 	loss(eval)=0.4125 	f1(train)=0.9000 	f1(eval)=0.8392 	duration(epoch)=0:01:11.059913
@018: 	loss(train)=0.1969 	loss(eval)=0.4179 	f1(train)=0.9200 	f1(eval)=0.8384 	duration(epoch)=0:01:11.943825
@019: 	loss(train)=0.1865 	loss(eval)=0.4266 	f1(train)=0.9275 	f1(eval)=0.8404 	duration(epoch)=0:01:11.266493
@020: 	loss(train)=0.2116 	loss(eval)=0.4409 	f1(train)=0.9125 	f1(eval)=0.8368 	duration(epoch)=0:01:11.684694
@021: 	loss(train)=0.1508 	loss(eval)=0.4750 	f1(train)=0.9425 	f1(eval)=0.8298 	duration(epoch)=0:01:11.846475
@022: 	loss(train)=0.1385 	loss(eval)=0.5160 	f1(train)=0.9525 	f1(eval)=0.8254 	duration(epoch)=0:01:11.434308
@023: 	loss(train)=0.1431 	loss(eval)=0.4782 	f1(train)=0.9375 	f1(eval)=0.8362 	duration(epoch)=0:01:11.431819
@024: 	loss(train)=0.1327 	loss(eval)=0.4927 	f1(train)=0.9450 	f1(eval)=0.8340 	duration(epoch)=0:01:10.696074
@025: 	loss(train)=0.1228 	loss(eval)=0.5026 	f1(train)=0.9500 	f1(eval)=0.8358 	duration(epoch)=0:01:11.739525
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 30
@016: 	loss(train)=0.2169 	loss(eval)=0.4039 	f1(train)=0.9225 	f1(eval)=0.8426 	duration(epoch)=0:01:11.447401

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4213	 fp:      787 	 tn:     4213	 fn:      787	 pre=0.8426	 rec=0.8426	 f1=0.8426	 acc=0.8426
negative      	 tp:     2049	 fp:      359 	 tn:     2164	 fn:      428	 pre=0.8509	 rec=0.8272	 f1=0.8389	 acc=0.8426
positive      	 tp:     2164	 fp:      428 	 tn:     2049	 fn:      359	 pre=0.8349	 rec=0.8577	 f1=0.8461	 acc=0.8426
