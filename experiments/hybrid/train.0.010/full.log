> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0103 sec
> f(__tokenize) took: 0.3938 sec
> f(__ngram) took: 0.3092 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0844 sec
> f(__tokenize) took: 1.3023 sec
> f(__ngram) took: 0.6149 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0819 sec
> f(__tokenize) took: 1.2468 sec
> f(__ngram) took: 0.6573 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.7807 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0531 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 5.0922 sec
> Apply Space Pipeline to: ./data/imdb.train.0.010.csv
> f(apply) took: 14.7046 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 6.4560 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 159.2700 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6915 	loss(eval)=0.6873 	f1(train)=0.5125 	f1(eval)=0.5462 	duration(epoch)=0:01:11.867431
@002: 	loss(train)=0.6546 	loss(eval)=0.6501 	f1(train)=0.7000 	f1(eval)=0.6828 	duration(epoch)=0:01:12.308167
@003: 	loss(train)=0.5534 	loss(eval)=0.5509 	f1(train)=0.8325 	f1(eval)=0.7620 	duration(epoch)=0:01:12.334341
@004: 	loss(train)=0.4133 	loss(eval)=0.4711 	f1(train)=0.8900 	f1(eval)=0.7876 	duration(epoch)=0:01:12.301926
@005: 	loss(train)=0.3171 	loss(eval)=0.4470 	f1(train)=0.9150 	f1(eval)=0.7932 	duration(epoch)=0:01:12.096246
@006: 	loss(train)=0.2679 	loss(eval)=0.4343 	f1(train)=0.9125 	f1(eval)=0.7974 	duration(epoch)=0:01:12.579660
@007: 	loss(train)=0.2252 	loss(eval)=0.5131 	f1(train)=0.9375 	f1(eval)=0.7580 	duration(epoch)=0:01:12.070841
@008: 	loss(train)=0.2010 	loss(eval)=0.4490 	f1(train)=0.9375 	f1(eval)=0.7948 	duration(epoch)=0:01:12.374644
@009: 	loss(train)=0.1711 	loss(eval)=0.4484 	f1(train)=0.9400 	f1(eval)=0.7982 	duration(epoch)=0:01:12.546284
@010: 	loss(train)=0.1534 	loss(eval)=0.5131 	f1(train)=0.9425 	f1(eval)=0.7798 	duration(epoch)=0:01:12.288056
@011: 	loss(train)=0.1404 	loss(eval)=0.4636 	f1(train)=0.9575 	f1(eval)=0.7938 	duration(epoch)=0:01:11.958297
@012: 	loss(train)=0.1156 	loss(eval)=0.4937 	f1(train)=0.9750 	f1(eval)=0.7918 	duration(epoch)=0:01:12.065832
@013: 	loss(train)=0.1005 	loss(eval)=0.4868 	f1(train)=0.9775 	f1(eval)=0.7950 	duration(epoch)=0:01:12.370608
@014: 	loss(train)=0.0936 	loss(eval)=0.4977 	f1(train)=0.9750 	f1(eval)=0.7918 	duration(epoch)=0:01:12.453911
@015: 	loss(train)=0.0868 	loss(eval)=0.5139 	f1(train)=0.9750 	f1(eval)=0.7900 	duration(epoch)=0:01:12.032001
@016: 	loss(train)=0.0806 	loss(eval)=0.5912 	f1(train)=0.9850 	f1(eval)=0.7740 	duration(epoch)=0:01:12.315336
@017: 	loss(train)=0.0668 	loss(eval)=0.5433 	f1(train)=0.9850 	f1(eval)=0.7856 	duration(epoch)=0:01:11.966429
@018: 	loss(train)=0.0563 	loss(eval)=0.5769 	f1(train)=0.9875 	f1(eval)=0.7838 	duration(epoch)=0:01:12.243930
@019: 	loss(train)=0.0496 	loss(eval)=0.6165 	f1(train)=0.9900 	f1(eval)=0.7794 	duration(epoch)=0:01:11.909348
@020: 	loss(train)=0.0458 	loss(eval)=0.5947 	f1(train)=0.9875 	f1(eval)=0.7854 	duration(epoch)=0:01:12.050733
@021: 	loss(train)=0.0440 	loss(eval)=0.6206 	f1(train)=0.9900 	f1(eval)=0.7824 	duration(epoch)=0:01:12.554848
@022: 	loss(train)=0.0465 	loss(eval)=0.5908 	f1(train)=0.9900 	f1(eval)=0.7900 	duration(epoch)=0:01:12.812580
@023: 	loss(train)=0.0420 	loss(eval)=0.6030 	f1(train)=0.9900 	f1(eval)=0.7908 	duration(epoch)=0:01:12.508853
@024: 	loss(train)=0.0347 	loss(eval)=0.6205 	f1(train)=0.9950 	f1(eval)=0.7888 	duration(epoch)=0:01:12.411784
@025: 	loss(train)=0.0358 	loss(eval)=0.6498 	f1(train)=0.9875 	f1(eval)=0.7862 	duration(epoch)=0:01:11.808832
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@009: 	loss(train)=0.1711 	loss(eval)=0.4484 	f1(train)=0.9400 	f1(eval)=0.7982 	duration(epoch)=0:01:12.546284

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3991	 fp:     1009 	 tn:     3991	 fn:     1009	 pre=0.7982	 rec=0.7982	 f1=0.7982	 acc=0.7982
negative      	 tp:     2048	 fp:      594 	 tn:     1943	 fn:      415	 pre=0.7752	 rec=0.8315	 f1=0.8024	 acc=0.7982
positive      	 tp:     1943	 fp:      415 	 tn:     2048	 fn:      594	 pre=0.8240	 rec=0.7659	 f1=0.7939	 acc=0.7982
