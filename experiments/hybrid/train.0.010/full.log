> Loaded logger: ./experiments/hybrid/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0092 sec
> f(__tokenize) took: 0.4175 sec
> f(__ngram) took: 0.3091 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0820 sec
> f(__tokenize) took: 1.1984 sec
> f(__ngram) took: 0.5632 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0645 sec
> f(__tokenize) took: 1.2338 sec
> f(__ngram) took: 0.6617 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.8306 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0431 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 6.0067 sec
> Apply Space Pipeline to: ./data/imdb.train.0.010.csv
> f(apply) took: 16.9134 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.3853 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 192.7398 sec

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6428 	loss(eval)=0.6178 	f1(train)=0.6325 	f1(eval)=0.5612 	duration(epoch)=0:01:11.365273
@002: 	loss(train)=0.5204 	loss(eval)=0.4848 	f1(train)=0.7650 	f1(eval)=0.7738 	duration(epoch)=0:01:12.581076
@003: 	loss(train)=0.4308 	loss(eval)=0.4641 	f1(train)=0.8025 	f1(eval)=0.7746 	duration(epoch)=0:01:12.303508
@004: 	loss(train)=0.3904 	loss(eval)=0.4552 	f1(train)=0.8325 	f1(eval)=0.7842 	duration(epoch)=0:01:12.921647
@005: 	loss(train)=0.3531 	loss(eval)=0.3818 	f1(train)=0.8575 	f1(eval)=0.8286 	duration(epoch)=0:01:12.583521
@006: 	loss(train)=0.3280 	loss(eval)=0.3876 	f1(train)=0.8525 	f1(eval)=0.8316 	duration(epoch)=0:01:12.664593
@007: 	loss(train)=0.3152 	loss(eval)=0.3744 	f1(train)=0.8650 	f1(eval)=0.8408 	duration(epoch)=0:01:12.947067
@008: 	loss(train)=0.3108 	loss(eval)=0.3868 	f1(train)=0.8650 	f1(eval)=0.8390 	duration(epoch)=0:01:12.731601
@009: 	loss(train)=0.2620 	loss(eval)=0.3968 	f1(train)=0.9000 	f1(eval)=0.8382 	duration(epoch)=0:01:12.690046
@010: 	loss(train)=0.2590 	loss(eval)=0.3883 	f1(train)=0.8875 	f1(eval)=0.8466 	duration(epoch)=0:01:12.639080
@011: 	loss(train)=0.2562 	loss(eval)=0.4047 	f1(train)=0.8850 	f1(eval)=0.8396 	duration(epoch)=0:01:12.039114
@012: 	loss(train)=0.2366 	loss(eval)=0.4363 	f1(train)=0.9225 	f1(eval)=0.8330 	duration(epoch)=0:01:12.346943
@013: 	loss(train)=0.2203 	loss(eval)=0.4384 	f1(train)=0.9150 	f1(eval)=0.8342 	duration(epoch)=0:01:12.363064
@014: 	loss(train)=0.2190 	loss(eval)=0.4312 	f1(train)=0.9075 	f1(eval)=0.8394 	duration(epoch)=0:01:12.778140
@015: 	loss(train)=0.1975 	loss(eval)=0.4836 	f1(train)=0.9325 	f1(eval)=0.8246 	duration(epoch)=0:01:13.096738
@016: 	loss(train)=0.1758 	loss(eval)=0.4579 	f1(train)=0.9325 	f1(eval)=0.8384 	duration(epoch)=0:01:12.517350
@017: 	loss(train)=0.1478 	loss(eval)=0.4789 	f1(train)=0.9500 	f1(eval)=0.8388 	duration(epoch)=0:01:12.989200
@018: 	loss(train)=0.1399 	loss(eval)=0.4996 	f1(train)=0.9450 	f1(eval)=0.8344 	duration(epoch)=0:01:12.694055
@019: 	loss(train)=0.1151 	loss(eval)=0.5128 	f1(train)=0.9525 	f1(eval)=0.8366 	duration(epoch)=0:01:12.521951
@020: 	loss(train)=0.1132 	loss(eval)=0.5230 	f1(train)=0.9575 	f1(eval)=0.8340 	duration(epoch)=0:01:12.956335
@021: 	loss(train)=0.0948 	loss(eval)=0.5620 	f1(train)=0.9725 	f1(eval)=0.8286 	duration(epoch)=0:01:12.713833
@022: 	loss(train)=0.0809 	loss(eval)=0.6010 	f1(train)=0.9775 	f1(eval)=0.8224 	duration(epoch)=0:01:12.354964
@023: 	loss(train)=0.0608 	loss(eval)=0.6143 	f1(train)=0.9800 	f1(eval)=0.8290 	duration(epoch)=0:01:12.840617
@024: 	loss(train)=0.0616 	loss(eval)=0.6037 	f1(train)=0.9775 	f1(eval)=0.8300 	duration(epoch)=0:01:12.609097
@025: 	loss(train)=0.0599 	loss(eval)=0.6198 	f1(train)=0.9800 	f1(eval)=0.8310 	duration(epoch)=0:01:13.056193
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@010: 	loss(train)=0.2590 	loss(eval)=0.3883 	f1(train)=0.8875 	f1(eval)=0.8466 	duration(epoch)=0:01:12.639080

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4233	 fp:      767 	 tn:     4233	 fn:      767	 pre=0.8466	 rec=0.8466	 f1=0.8466	 acc=0.8466
negative      	 tp:     2099	 fp:      389 	 tn:     2134	 fn:      378	 pre=0.8436	 rec=0.8474	 f1=0.8455	 acc=0.8466
positive      	 tp:     2134	 fp:      378 	 tn:     2099	 fn:      389	 pre=0.8495	 rec=0.8458	 f1=0.8477	 acc=0.8466
