> Loaded logger: ./experiments/hybrid/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4506 sec
> f(__tokenize) took: 6.2405 sec
> f(__ngram) took: 2.9059 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0621 sec
> f(__tokenize) took: 1.5299 sec
> f(__ngram) took: 0.9570 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0653 sec
> f(__tokenize) took: 1.5881 sec
> f(__ngram) took: 0.9676 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.8886 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 5.5354 sec
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 15.5004 sec
> Apply Space Pipeline to: ./data/imdb.train.1.000.csv
> f(apply) took: 1225.6435 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.1622 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 158.3247 sec

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.4011 	loss(eval)=0.3829 	f1(train)=0.8236 	f1(eval)=0.8408 	duration(epoch)=0:10:18.942666
@002: 	loss(train)=0.3757 	loss(eval)=0.3802 	f1(train)=0.8385 	f1(eval)=0.8372 	duration(epoch)=0:10:21.378078
@003: 	loss(train)=0.3661 	loss(eval)=0.3618 	f1(train)=0.8417 	f1(eval)=0.8458 	duration(epoch)=0:10:20.200992
@004: 	loss(train)=0.3604 	loss(eval)=0.3572 	f1(train)=0.8445 	f1(eval)=0.8478 	duration(epoch)=0:10:19.616965
@005: 	loss(train)=0.3547 	loss(eval)=0.3556 	f1(train)=0.8458 	f1(eval)=0.8486 	duration(epoch)=0:10:18.039274
@006: 	loss(train)=0.3502 	loss(eval)=0.3630 	f1(train)=0.8487 	f1(eval)=0.8470 	duration(epoch)=0:10:19.650854
@007: 	loss(train)=0.3463 	loss(eval)=0.3623 	f1(train)=0.8497 	f1(eval)=0.8490 	duration(epoch)=0:10:22.390514
@008: 	loss(train)=0.3428 	loss(eval)=0.3569 	f1(train)=0.8517 	f1(eval)=0.8496 	duration(epoch)=0:10:19.248055
@009: 	loss(train)=0.3388 	loss(eval)=0.3520 	f1(train)=0.8546 	f1(eval)=0.8494 	duration(epoch)=0:10:17.117229
@010: 	loss(train)=0.3338 	loss(eval)=0.3681 	f1(train)=0.8569 	f1(eval)=0.8490 	duration(epoch)=0:10:20.357255
@011: 	loss(train)=0.3295 	loss(eval)=0.3744 	f1(train)=0.8598 	f1(eval)=0.8414 	duration(epoch)=0:10:19.717160
@012: 	loss(train)=0.3273 	loss(eval)=0.3701 	f1(train)=0.8619 	f1(eval)=0.8522 	duration(epoch)=0:10:21.635243
@013: 	loss(train)=0.3262 	loss(eval)=0.3648 	f1(train)=0.8618 	f1(eval)=0.8514 	duration(epoch)=0:10:20.349416
@014: 	loss(train)=0.3220 	loss(eval)=0.3701 	f1(train)=0.8643 	f1(eval)=0.8516 	duration(epoch)=0:10:19.473869
@015: 	loss(train)=0.3185 	loss(eval)=0.3736 	f1(train)=0.8662 	f1(eval)=0.8534 	duration(epoch)=0:10:17.940499
@016: 	loss(train)=0.3154 	loss(eval)=0.3949 	f1(train)=0.8690 	f1(eval)=0.8526 	duration(epoch)=0:10:20.701114
@017: 	loss(train)=0.3134 	loss(eval)=0.3837 	f1(train)=0.8719 	f1(eval)=0.8502 	duration(epoch)=0:10:19.613283
@018: 	loss(train)=0.3086 	loss(eval)=0.3881 	f1(train)=0.8742 	f1(eval)=0.8516 	duration(epoch)=0:10:20.567057
@019: 	loss(train)=0.3092 	loss(eval)=0.3851 	f1(train)=0.8750 	f1(eval)=0.8506 	duration(epoch)=0:10:18.946399
@020: 	loss(train)=0.3029 	loss(eval)=0.3829 	f1(train)=0.8777 	f1(eval)=0.8490 	duration(epoch)=0:10:17.728261
@021: 	loss(train)=0.3001 	loss(eval)=0.4066 	f1(train)=0.8798 	f1(eval)=0.8536 	duration(epoch)=0:10:20.964057
@022: 	loss(train)=0.2994 	loss(eval)=0.3991 	f1(train)=0.8797 	f1(eval)=0.8462 	duration(epoch)=0:10:22.038385
@023: 	loss(train)=0.2957 	loss(eval)=0.4039 	f1(train)=0.8818 	f1(eval)=0.8498 	duration(epoch)=0:10:18.431499
@024: 	loss(train)=0.2923 	loss(eval)=0.4055 	f1(train)=0.8837 	f1(eval)=0.8450 	duration(epoch)=0:10:20.834291
@025: 	loss(train)=0.2903 	loss(eval)=0.4302 	f1(train)=0.8861 	f1(eval)=0.8410 	duration(epoch)=0:10:20.129473
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@021: 	loss(train)=0.3001 	loss(eval)=0.4066 	f1(train)=0.8798 	f1(eval)=0.8536 	duration(epoch)=0:10:20.964057

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4268	 fp:      732 	 tn:     4268	 fn:      732	 pre=0.8536	 rec=0.8536	 f1=0.8536	 acc=0.8536
negative      	 tp:     2064	 fp:      333 	 tn:     2204	 fn:      399	 pre=0.8611	 rec=0.8380	 f1=0.8494	 acc=0.8536
positive      	 tp:     2204	 fp:      399 	 tn:     2064	 fn:      333	 pre=0.8467	 rec=0.8687	 f1=0.8576	 acc=0.8536
