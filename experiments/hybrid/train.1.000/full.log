> Loaded logger: ./experiments/hybrid/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4448 sec
> f(__tokenize) took: 6.3549 sec
> f(__ngram) took: 2.9587 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0625 sec
> f(__tokenize) took: 1.6745 sec
> f(__ngram) took: 1.1300 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0640 sec
> f(__tokenize) took: 1.7591 sec
> f(__ngram) took: 1.1654 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.3771 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 5.4947 sec
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 17.5403 sec
> Apply Space Pipeline to: ./data/imdb.train.1.000.csv
> f(apply) took: 1187.7974 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 9.1804 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 150.9241 sec

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.1511 	loss(eval)=0.1518 	f1(train)=0.9580 	f1(eval)=0.9568 	duration(epoch)=0:10:09.743247
@002: 	loss(train)=0.1677 	loss(eval)=0.1351 	f1(train)=0.9588 	f1(eval)=0.9588 	duration(epoch)=0:10:05.876675
@003: 	loss(train)=0.1753 	loss(eval)=0.1573 	f1(train)=0.9599 	f1(eval)=0.9590 	duration(epoch)=0:10:11.197350
@004: 	loss(train)=0.1746 	loss(eval)=0.1404 	f1(train)=0.9601 	f1(eval)=0.9580 	duration(epoch)=0:10:10.136542
@005: 	loss(train)=0.1755 	loss(eval)=0.1546 	f1(train)=0.9607 	f1(eval)=0.9586 	duration(epoch)=0:10:07.076601
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
> Init Neural Assemble (MLP), trainable parameters: 32
@003: 	loss(train)=0.1753 	loss(eval)=0.1573 	f1(train)=0.9599 	f1(eval)=0.9590 	duration(epoch)=0:10:11.197350

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4795	 fp:      205 	 tn:     4795	 fn:      205	 pre=0.9590	 rec=0.9590	 f1=0.9590	 acc=0.9590
negative      	 tp:     2358	 fp:      100 	 tn:     2437	 fn:      105	 pre=0.9593	 rec=0.9574	 f1=0.9583	 acc=0.9590
positive      	 tp:     2437	 fp:      105 	 tn:     2358	 fn:      100	 pre=0.9587	 rec=0.9606	 f1=0.9596	 acc=0.9590
