> Loaded logger: ./experiments/hybrid/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4559 sec
> f(__tokenize) took: 6.8483 sec
> f(__ngram) took: 2.6674 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0602 sec
> f(__tokenize) took: 1.6781 sec
> f(__ngram) took: 1.1582 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0694 sec
> f(__tokenize) took: 1.7759 sec
> f(__ngram) took: 1.1873 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 8.0263 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
> Init BERT-Head (MLP), trainable parameters: 199691
> Init Neural Assemble (MLP), trainable parameters: 24

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 5.7224 sec
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 57.5680 sec
> Apply Space Pipeline to: ./data/imdb.train.1.000.csv
> f(apply_spacy) took: 1226.3864 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.2410 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply_spacy) took: 148.9744 sec

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.3447 	loss(eval)=0.3176 	f1(train)=0.8520 	f1(eval)=0.8690 	duration(epoch)=0:10:24.562343
@002: 	loss(train)=0.2980 	loss(eval)=0.2827 	f1(train)=0.8772 	f1(eval)=0.8804 	duration(epoch)=0:10:26.056813
@003: 	loss(train)=0.2872 	loss(eval)=0.2702 	f1(train)=0.8823 	f1(eval)=0.8880 	duration(epoch)=0:10:28.316291
@004: 	loss(train)=0.2793 	loss(eval)=0.2716 	f1(train)=0.8860 	f1(eval)=0.8880 	duration(epoch)=0:10:28.886298
@005: 	loss(train)=0.2741 	loss(eval)=0.2680 	f1(train)=0.8866 	f1(eval)=0.8922 	duration(epoch)=0:10:28.885932
@006: 	loss(train)=0.2704 	loss(eval)=0.2589 	f1(train)=0.8882 	f1(eval)=0.8966 	duration(epoch)=0:10:26.816176
@007: 	loss(train)=0.2665 	loss(eval)=0.2583 	f1(train)=0.8900 	f1(eval)=0.8952 	duration(epoch)=0:10:26.409312
@008: 	loss(train)=0.2635 	loss(eval)=0.2492 	f1(train)=0.8919 	f1(eval)=0.8976 	duration(epoch)=0:10:25.870684
@009: 	loss(train)=0.2600 	loss(eval)=0.2502 	f1(train)=0.8933 	f1(eval)=0.8964 	duration(epoch)=0:10:27.908145
@010: 	loss(train)=0.2569 	loss(eval)=0.2446 	f1(train)=0.8955 	f1(eval)=0.9028 	duration(epoch)=0:10:25.517370
@011: 	loss(train)=0.2530 	loss(eval)=0.2727 	f1(train)=0.8974 	f1(eval)=0.8866 	duration(epoch)=0:10:27.664227
@012: 	loss(train)=0.2523 	loss(eval)=0.2393 	f1(train)=0.8991 	f1(eval)=0.9032 	duration(epoch)=0:10:27.571861
@013: 	loss(train)=0.2486 	loss(eval)=0.2471 	f1(train)=0.8987 	f1(eval)=0.8988 	duration(epoch)=0:10:24.538766
@014: 	loss(train)=0.2466 	loss(eval)=0.2382 	f1(train)=0.8996 	f1(eval)=0.9050 	duration(epoch)=0:10:22.920110
@015: 	loss(train)=0.2445 	loss(eval)=0.2345 	f1(train)=0.9014 	f1(eval)=0.9086 	duration(epoch)=0:10:23.878255
@016: 	loss(train)=0.2417 	loss(eval)=0.2400 	f1(train)=0.9033 	f1(eval)=0.9010 	duration(epoch)=0:10:25.429723
@017: 	loss(train)=0.2412 	loss(eval)=0.2292 	f1(train)=0.9026 	f1(eval)=0.9094 	duration(epoch)=0:10:26.806419
@018: 	loss(train)=0.2391 	loss(eval)=0.2313 	f1(train)=0.9056 	f1(eval)=0.9098 	duration(epoch)=0:10:26.684517
@019: 	loss(train)=0.2371 	loss(eval)=0.2333 	f1(train)=0.9069 	f1(eval)=0.9100 	duration(epoch)=0:10:26.580492
@020: 	loss(train)=0.2337 	loss(eval)=0.2372 	f1(train)=0.9083 	f1(eval)=0.9074 	duration(epoch)=0:10:26.775639
@021: 	loss(train)=0.2337 	loss(eval)=0.2253 	f1(train)=0.9093 	f1(eval)=0.9102 	duration(epoch)=0:10:27.607063
@022: 	loss(train)=0.2311 	loss(eval)=0.2687 	f1(train)=0.9102 	f1(eval)=0.8978 	duration(epoch)=0:10:25.983436
@023: 	loss(train)=0.2302 	loss(eval)=0.2377 	f1(train)=0.9128 	f1(eval)=0.9064 	duration(epoch)=0:10:22.492804
@024: 	loss(train)=0.2281 	loss(eval)=0.2210 	f1(train)=0.9136 	f1(eval)=0.9152 	duration(epoch)=0:10:26.761786
@025: 	loss(train)=0.2249 	loss(eval)=0.2233 	f1(train)=0.9144 	f1(eval)=0.9156 	duration(epoch)=0:10:25.612365
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 199691
> Init Neural Assemble (MLP), trainable parameters: 24
@025: 	loss(train)=0.2249 	loss(eval)=0.2233 	f1(train)=0.9144 	f1(eval)=0.9156 	duration(epoch)=0:10:25.612365

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4578	 fp:      422 	 tn:     4578	 fn:      422	 pre=0.9156	 rec=0.9156	 f1=0.9156	 acc=0.9156
negative      	 tp:     2217	 fp:      162 	 tn:     2361	 fn:      260	 pre=0.9319	 rec=0.8950	 f1=0.9131	 acc=0.9156
positive      	 tp:     2361	 fp:      260 	 tn:     2217	 fn:      162	 pre=0.9008	 rec=0.9358	 f1=0.9180	 acc=0.9156
