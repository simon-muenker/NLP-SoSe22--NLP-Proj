> Loaded logger: ./experiments/hybrid/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4618 sec
> f(__tokenize) took: 6.6542 sec
> f(__ngram) took: 2.5559 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0629 sec
> f(__tokenize) took: 1.5601 sec
> f(__ngram) took: 0.9511 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0635 sec
> f(__tokenize) took: 1.6251 sec
> f(__ngram) took: 0.9949 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.6692 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 5.6272 sec
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 16.7767 sec
> Apply Space Pipeline to: ./data/imdb.train.1.000.csv
> f(apply) took: 1522.0824 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.4750 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply) took: 183.5218 sec

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.3699 	loss(eval)=0.3272 	f1(train)=0.8429 	f1(eval)=0.8668 	duration(epoch)=0:10:33.204754
@002: 	loss(train)=0.3357 	loss(eval)=0.3058 	f1(train)=0.8620 	f1(eval)=0.8782 	duration(epoch)=0:10:38.811611
@003: 	loss(train)=0.3229 	loss(eval)=0.2972 	f1(train)=0.8685 	f1(eval)=0.8748 	duration(epoch)=0:10:37.927248
@004: 	loss(train)=0.3165 	loss(eval)=0.3047 	f1(train)=0.8710 	f1(eval)=0.8836 	duration(epoch)=0:10:38.389504
@005: 	loss(train)=0.3108 	loss(eval)=0.2846 	f1(train)=0.8750 	f1(eval)=0.8870 	duration(epoch)=0:10:37.909770
@006: 	loss(train)=0.3046 	loss(eval)=0.2835 	f1(train)=0.8783 	f1(eval)=0.8910 	duration(epoch)=0:10:35.647820
@007: 	loss(train)=0.3003 	loss(eval)=0.2968 	f1(train)=0.8802 	f1(eval)=0.8930 	duration(epoch)=0:10:39.360903
@008: 	loss(train)=0.2991 	loss(eval)=0.2661 	f1(train)=0.8831 	f1(eval)=0.8966 	duration(epoch)=0:10:38.046422
@009: 	loss(train)=0.2950 	loss(eval)=0.2842 	f1(train)=0.8849 	f1(eval)=0.8948 	duration(epoch)=0:10:37.704869
@010: 	loss(train)=0.2924 	loss(eval)=0.2824 	f1(train)=0.8883 	f1(eval)=0.8902 	duration(epoch)=0:10:37.418140
@011: 	loss(train)=0.2895 	loss(eval)=0.2951 	f1(train)=0.8894 	f1(eval)=0.8922 	duration(epoch)=0:10:39.215053
@012: 	loss(train)=0.2862 	loss(eval)=0.2581 	f1(train)=0.8922 	f1(eval)=0.9038 	duration(epoch)=0:10:38.760410
@013: 	loss(train)=0.2829 	loss(eval)=0.2619 	f1(train)=0.8942 	f1(eval)=0.9018 	duration(epoch)=0:10:38.994349
@014: 	loss(train)=0.2810 	loss(eval)=0.2532 	f1(train)=0.8977 	f1(eval)=0.9060 	duration(epoch)=0:10:33.280150
@015: 	loss(train)=0.2771 	loss(eval)=0.2445 	f1(train)=0.8974 	f1(eval)=0.9086 	duration(epoch)=0:10:38.067634
@016: 	loss(train)=0.2754 	loss(eval)=0.2590 	f1(train)=0.8997 	f1(eval)=0.9066 	duration(epoch)=0:10:39.867065
@017: 	loss(train)=0.2718 	loss(eval)=0.2583 	f1(train)=0.9027 	f1(eval)=0.9132 	duration(epoch)=0:10:39.312534
@018: 	loss(train)=0.2717 	loss(eval)=0.2554 	f1(train)=0.9059 	f1(eval)=0.9138 	duration(epoch)=0:10:36.807088
@019: 	loss(train)=0.2680 	loss(eval)=0.2538 	f1(train)=0.9072 	f1(eval)=0.9062 	duration(epoch)=0:10:37.485934
@020: 	loss(train)=0.2644 	loss(eval)=0.2658 	f1(train)=0.9106 	f1(eval)=0.9110 	duration(epoch)=0:10:37.782514
@021: 	loss(train)=0.2591 	loss(eval)=0.2606 	f1(train)=0.9108 	f1(eval)=0.9174 	duration(epoch)=0:10:37.624347
@022: 	loss(train)=0.2591 	loss(eval)=0.2868 	f1(train)=0.9128 	f1(eval)=0.9128 	duration(epoch)=0:10:37.021617
@023: 	loss(train)=0.2560 	loss(eval)=0.2292 	f1(train)=0.9150 	f1(eval)=0.9226 	duration(epoch)=0:10:39.397757
@024: 	loss(train)=0.2514 	loss(eval)=0.2464 	f1(train)=0.9179 	f1(eval)=0.9194 	duration(epoch)=0:10:36.261628
@025: 	loss(train)=0.2465 	loss(eval)=0.2275 	f1(train)=0.9198 	f1(eval)=0.9282 	duration(epoch)=0:10:37.439835
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@025: 	loss(train)=0.2465 	loss(eval)=0.2275 	f1(train)=0.9198 	f1(eval)=0.9282 	duration(epoch)=0:10:37.439835

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4641	 fp:      359 	 tn:     4641	 fn:      359	 pre=0.9282	 rec=0.9282	 f1=0.9282	 acc=0.9282
negative      	 tp:     2293	 fp:      175 	 tn:     2348	 fn:      184	 pre=0.9291	 rec=0.9257	 f1=0.9274	 acc=0.9282
positive      	 tp:     2348	 fp:      184 	 tn:     2293	 fn:      175	 pre=0.9273	 rec=0.9306	 f1=0.9290	 acc=0.9282
