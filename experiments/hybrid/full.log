> Loaded logger: ./experiments/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2527 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 22412 
  Memory Usage: 30.5957 MB
> f(__tokenize) took: 3.9431 sec
> f(__ngram) took: 1.6613 sec
> f(__ngram) took: 1.5655 sec
> f(__load) took: 0.0342 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2491 
  Memory Usage: 3.3220 MB
> f(__tokenize) took: 1.0989 sec
> f(__ngram) took: 0.7303 sec
> f(__ngram) took: 0.7286 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.8122 sec
> f(df_encode) took: 286.9101 sec
> Memory Usage (w/ Embeds): 110.7668 MB
> f(df_encode) took: 32.0172 sec
> Memory Usage (w/ Embeds): 11.9990 MB

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 3.1150 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 121.6498 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 20.3950 sec
> Init Base+Features Concatenation (Hybrid)
  Memory Usage: 0.0060 MB
  Trainable parameters: 1580
  Input Dimension: 789
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@005: 	loss(train)=0.2810 	loss(eval)=0.2549 	f1(train)=0.8828 	f1(eval)=0.9020 	duration(epoch)=0:00:18.935782
@010: 	loss(train)=0.2650 	loss(eval)=0.3117 	f1(train)=0.8914 	f1(eval)=0.8747 	duration(epoch)=0:00:18.996997
@015: 	loss(train)=0.2583 	loss(eval)=0.3470 	f1(train)=0.8952 	f1(eval)=0.8583 	duration(epoch)=0:00:19.072338
@020: 	loss(train)=0.2537 	loss(eval)=0.3677 	f1(train)=0.8961 	f1(eval)=0.8499 	duration(epoch)=0:00:19.116303
@025: 	loss(train)=0.2487 	loss(eval)=0.3420 	f1(train)=0.9001 	f1(eval)=0.8647 	duration(epoch)=0:00:19.039781
@030: 	loss(train)=0.2444 	loss(eval)=0.3154 	f1(train)=0.9012 	f1(eval)=0.8752 	duration(epoch)=0:00:19.282677
@035: 	loss(train)=0.2426 	loss(eval)=0.3827 	f1(train)=0.9004 	f1(eval)=0.8446 	duration(epoch)=0:00:19.299611
@040: 	loss(train)=0.2414 	loss(eval)=0.2865 	f1(train)=0.9020 	f1(eval)=0.8868 	duration(epoch)=0:00:19.156937
@045: 	loss(train)=0.2388 	loss(eval)=0.3303 	f1(train)=0.9033 	f1(eval)=0.8667 	duration(epoch)=0:00:18.972534
@050: 	loss(train)=0.2373 	loss(eval)=0.3113 	f1(train)=0.9041 	f1(eval)=0.8760 	duration(epoch)=0:00:19.094992
@055: 	loss(train)=0.2364 	loss(eval)=0.3369 	f1(train)=0.9044 	f1(eval)=0.8687 	duration(epoch)=0:00:19.119560
@060: 	loss(train)=0.2342 	loss(eval)=0.3272 	f1(train)=0.9046 	f1(eval)=0.8731 	duration(epoch)=0:00:19.059490
@065: 	loss(train)=0.2337 	loss(eval)=0.3657 	f1(train)=0.9054 	f1(eval)=0.8539 	duration(epoch)=0:00:18.721171
@070: 	loss(train)=0.2334 	loss(eval)=0.3781 	f1(train)=0.9051 	f1(eval)=0.8475 	duration(epoch)=0:00:20.204961
@075: 	loss(train)=0.2324 	loss(eval)=0.2430 	f1(train)=0.9056 	f1(eval)=0.9073 	duration(epoch)=0:00:19.125092
@080: 	loss(train)=0.2310 	loss(eval)=0.3307 	f1(train)=0.9063 	f1(eval)=0.8715 	duration(epoch)=0:00:19.191380
@085: 	loss(train)=0.2293 	loss(eval)=0.2846 	f1(train)=0.9072 	f1(eval)=0.8868 	duration(epoch)=0:00:19.467188
@090: 	loss(train)=0.2304 	loss(eval)=0.2768 	f1(train)=0.9069 	f1(eval)=0.8924 	duration(epoch)=0:00:19.177059
@095: 	loss(train)=0.2290 	loss(eval)=0.2789 	f1(train)=0.9078 	f1(eval)=0.8884 	duration(epoch)=0:00:18.899932
@100: 	loss(train)=0.2287 	loss(eval)=0.2973 	f1(train)=0.9072 	f1(eval)=0.8796 	duration(epoch)=0:00:19.613812
> Load best model based on evaluation loss.
> Init Base+Features Concatenation (Hybrid)
  Memory Usage: 0.0060 MB
  Trainable parameters: 1580
  Input Dimension: 789
  Output Dimension: 2
@071: 	loss(train)=0.2328 	loss(eval)=0.2097 	f1(train)=0.9052 	f1(eval)=0.9165 	duration(epoch)=0:00:19.151736

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2283	 fp:      208 	 tn:     2283	 fn:      208	 pre=0.9165	 rec=0.9165	 f1=0.9165	 acc=0.9165
negative      	 tp:     2283	 fp:        0 	 tn:        0	 fn:      208	 pre=1.0000	 rec=0.9165	 f1=0.9564	 acc=0.9165
positive      	 tp:        0	 fp:      208 	 tn:     2283	 fn:        0	 pre=0.0000	 rec=0.0000	 f1=0.0000	 acc=0.9165
