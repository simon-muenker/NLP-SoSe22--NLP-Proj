> Loaded logger: ./experiments/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> Load/Init from ./data/imdb.train.csv
> f(__load) took: 0.2677 sec
> f(__tokenize) took: 4.4869 sec
> f(__ngram) took: 1.5428 sec
> f(__ngram) took: 1.7033 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0320 sec
> f(__tokenize) took: 1.0607 sec
> f(__ngram) took: 0.7030 sec
> f(__ngram) took: 0.7445 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.2104 sec

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 3.4693 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 143.7046 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 20.3043 sec
> Init BERT-Head (Base), trainable parameters: 1538
> Init Neural Weighting (Feature), trainable parameters: 66
> Init Neural Assemble (Base+Features), trainable parameters: 6

[--- TRAIN -> ./data/imdb.train.csv ---]
@001: 	loss(train)=0.5752 	loss(eval)=0.3879 	f1(train)=0.6662 	f1(eval)=0.8247 	duration(epoch)=0:06:04.393473
@002: 	loss(train)=0.4236 	loss(eval)=0.3773 	f1(train)=0.8059 	f1(eval)=0.8235 	duration(epoch)=0:06:05.429444
@003: 	loss(train)=0.3994 	loss(eval)=0.3755 	f1(train)=0.8182 	f1(eval)=0.8299 	duration(epoch)=0:06:05.954656
@004: 	loss(train)=0.3946 	loss(eval)=0.3460 	f1(train)=0.8174 	f1(eval)=0.8501 	duration(epoch)=0:06:05.479240
@005: 	loss(train)=0.3818 	loss(eval)=0.3443 	f1(train)=0.8267 	f1(eval)=0.8529 	duration(epoch)=0:06:05.653863
@006: 	loss(train)=0.3676 	loss(eval)=0.3394 	f1(train)=0.8346 	f1(eval)=0.8525 	duration(epoch)=0:06:04.996041
@007: 	loss(train)=0.3682 	loss(eval)=0.3373 	f1(train)=0.8357 	f1(eval)=0.8521 	duration(epoch)=0:06:05.719630
@008: 	loss(train)=0.3655 	loss(eval)=0.3304 	f1(train)=0.8369 	f1(eval)=0.8545 	duration(epoch)=0:06:05.229638
@009: 	loss(train)=0.3587 	loss(eval)=0.3328 	f1(train)=0.8401 	f1(eval)=0.8569 	duration(epoch)=0:06:05.622517
@010: 	loss(train)=0.3587 	loss(eval)=0.3339 	f1(train)=0.8407 	f1(eval)=0.8549 	duration(epoch)=0:06:05.203646
@011: 	loss(train)=0.3576 	loss(eval)=0.3416 	f1(train)=0.8414 	f1(eval)=0.8489 	duration(epoch)=0:06:04.839815
@012: 	loss(train)=0.3579 	loss(eval)=0.3644 	f1(train)=0.8393 	f1(eval)=0.8307 	duration(epoch)=0:06:05.619260
@013: 	loss(train)=0.3525 	loss(eval)=0.3317 	f1(train)=0.8440 	f1(eval)=0.8529 	duration(epoch)=0:06:05.373912
@014: 	loss(train)=0.3570 	loss(eval)=0.3294 	f1(train)=0.8410 	f1(eval)=0.8581 	duration(epoch)=0:06:05.889948
@015: 	loss(train)=0.3552 	loss(eval)=0.3319 	f1(train)=0.8425 	f1(eval)=0.8537 	duration(epoch)=0:06:05.246929
@016: 	loss(train)=0.3553 	loss(eval)=0.3529 	f1(train)=0.8409 	f1(eval)=0.8392 	duration(epoch)=0:06:05.715895
@017: 	loss(train)=0.3580 	loss(eval)=0.3365 	f1(train)=0.8413 	f1(eval)=0.8529 	duration(epoch)=0:06:05.239013
@018: 	loss(train)=0.3534 	loss(eval)=0.3442 	f1(train)=0.8424 	f1(eval)=0.8464 	duration(epoch)=0:06:06.204019
@019: 	loss(train)=0.3515 	loss(eval)=0.3459 	f1(train)=0.8435 	f1(eval)=0.8388 	duration(epoch)=0:06:05.393489
@020: 	loss(train)=0.3549 	loss(eval)=0.3260 	f1(train)=0.8422 	f1(eval)=0.8617 	duration(epoch)=0:06:05.677294
> Load best model based on evaluation loss.
> Init BERT-Head (Base), trainable parameters: 1538
> Init Neural Weighting (Feature), trainable parameters: 66
> Init Neural Assemble (Base+Features), trainable parameters: 6
@020: 	loss(train)=0.3549 	loss(eval)=0.3260 	f1(train)=0.8422 	f1(eval)=0.8617 	duration(epoch)=0:06:05.677294

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2138	 fp:      343 	 tn:     2138	 fn:      343	 pre=0.8617	 rec=0.8617	 f1=0.8617	 acc=0.8617
negative      	 tp:     1040	 fp:      182 	 tn:     1098	 fn:      161	 pre=0.8511	 rec=0.8659	 f1=0.8584	 acc=0.8617
positive      	 tp:     1098	 fp:      161 	 tn:     1040	 fn:      182	 pre=0.8721	 rec=0.8578	 f1=0.8649	 acc=0.8617
