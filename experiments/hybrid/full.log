> Loaded logger: ./experiments/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2243 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 20170 
  Memory Usage: 27.5464 MB
> f(__tokenize) took: 3.5821 sec
> f(__ngram) took: 1.2530 sec
> f(__ngram) took: 2.3094 sec
> f(__load) took: 0.0303 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2242 
  Memory Usage: 3.0493 MB
> f(__tokenize) took: 0.9598 sec
> f(__ngram) took: 0.6165 sec
> f(__ngram) took: 0.6484 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.3230 sec
> f(df_encode) took: 257.5549 sec
> Memory Usage (w/ Embeds): 99.7087 MB
> f(df_encode) took: 28.8724 sec
> Memory Usage (w/ Embeds): 11.0579 MB

[--- FEATURE PIPELINE ---]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0000 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 75.3023 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 12.6405 sec
> f(df_encode) took: 541.4320 sec
> f(match) took: 42.7914 sec
> f(match) took: 4.8668 sec
> Init Base+Features+MetaMatcher Concatenation (Hybrid)
  Memory Usage: 0.0119 MB
  Trainable parameters: 3110
  Input Dimension: 1554
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@005: 	loss(train)=0.3492 	loss(eval)=0.2872 	f1(train)=0.8490 	f1(eval)=0.8831 	duration(epoch)=0:00:22.217093
@010: 	loss(train)=0.3368 	loss(eval)=0.2981 	f1(train)=0.8516 	f1(eval)=0.8756 	duration(epoch)=0:00:22.056373
@015: 	loss(train)=0.3391 	loss(eval)=0.2779 	f1(train)=0.8497 	f1(eval)=0.8876 	duration(epoch)=0:00:22.108993
@020: 	loss(train)=0.3329 	loss(eval)=0.2794 	f1(train)=0.8532 	f1(eval)=0.8827 	duration(epoch)=0:00:23.394472
@025: 	loss(train)=0.3328 	loss(eval)=0.2706 	f1(train)=0.8547 	f1(eval)=0.8889 	duration(epoch)=0:00:21.997553
@030: 	loss(train)=0.3337 	loss(eval)=0.2780 	f1(train)=0.8528 	f1(eval)=0.8885 	duration(epoch)=0:00:21.770005
@035: 	loss(train)=0.3340 	loss(eval)=0.2768 	f1(train)=0.8540 	f1(eval)=0.8863 	duration(epoch)=0:00:22.337012
@040: 	loss(train)=0.3353 	loss(eval)=0.2827 	f1(train)=0.8506 	f1(eval)=0.8889 	duration(epoch)=0:00:22.093103
@045: 	loss(train)=0.3346 	loss(eval)=0.2861 	f1(train)=0.8527 	f1(eval)=0.8809 	duration(epoch)=0:00:22.028086
@050: 	loss(train)=0.3360 	loss(eval)=0.2883 	f1(train)=0.8490 	f1(eval)=0.8831 	duration(epoch)=0:00:22.018517
@055: 	loss(train)=0.3332 	loss(eval)=0.2969 	f1(train)=0.8525 	f1(eval)=0.8845 	duration(epoch)=0:00:21.967431
@060: 	loss(train)=0.3319 	loss(eval)=0.2933 	f1(train)=0.8545 	f1(eval)=0.8872 	duration(epoch)=0:00:22.758168
@065: 	loss(train)=0.3348 	loss(eval)=0.2891 	f1(train)=0.8531 	f1(eval)=0.8818 	duration(epoch)=0:00:22.274556
@070: 	loss(train)=0.3351 	loss(eval)=0.2922 	f1(train)=0.8497 	f1(eval)=0.8814 	duration(epoch)=0:00:22.300152
@075: 	loss(train)=0.3364 	loss(eval)=0.2948 	f1(train)=0.8508 	f1(eval)=0.8831 	duration(epoch)=0:00:22.246739
@080: 	loss(train)=0.3321 	loss(eval)=0.2808 	f1(train)=0.8513 	f1(eval)=0.8912 	duration(epoch)=0:00:22.247019
@085: 	loss(train)=0.3316 	loss(eval)=0.2975 	f1(train)=0.8545 	f1(eval)=0.8769 	duration(epoch)=0:00:22.253375
@090: 	loss(train)=0.3357 	loss(eval)=0.2835 	f1(train)=0.8530 	f1(eval)=0.8872 	duration(epoch)=0:00:22.194256
@095: 	loss(train)=0.3360 	loss(eval)=0.2886 	f1(train)=0.8520 	f1(eval)=0.8827 	duration(epoch)=0:00:22.074792
@100: 	loss(train)=0.3349 	loss(eval)=0.3004 	f1(train)=0.8518 	f1(eval)=0.8805 	duration(epoch)=0:00:22.144066
> Load best model based on evaluation loss.
> Init Base+Features+MetaMatcher Concatenation (Hybrid)
  Memory Usage: 0.0119 MB
  Trainable parameters: 3110
  Input Dimension: 1554
  Output Dimension: 2
@087: 	loss(train)=0.3332 	loss(eval)=0.2735 	f1(train)=0.8514 	f1(eval)=0.8916 	duration(epoch)=0:00:22.534338

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     1999	 fp:      243 	 tn:     1999	 fn:      243	 pre=0.8916	 rec=0.8916	 f1=0.8916	 acc=0.8916
negative      	 tp:      900	 fp:      131 	 tn:     1099	 fn:      112	 pre=0.8729	 rec=0.8893	 f1=0.8811	 acc=0.8916
positive      	 tp:     1099	 fp:      112 	 tn:      900	 fn:      131	 pre=0.9075	 rec=0.8935	 f1=0.9005	 acc=0.8916
