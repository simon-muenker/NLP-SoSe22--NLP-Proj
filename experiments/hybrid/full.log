> Loaded logger: ./experiments/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2896 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 25000 
  Memory Usage: 34.7926 MB
> f(__tokenize) took: 4.1926 sec
> f(__ngram) took: 1.8779 sec
> f(__ngram) took: 1.7400 sec
> f(__load) took: 0.0332 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2481 
  Memory Usage: 3.3606 MB
> f(__tokenize) took: 1.0927 sec
> f(__ngram) took: 0.7232 sec
> f(__ngram) took: 3.1553 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.6497 sec
> Encode ./data/imdb.train.csv
> f(df_encode) took: 319.8783 sec
> Encode ./data/imdb.eval.csv
> f(df_encode) took: 31.8700 sec

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 3.6901 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 111.5225 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 19.8655 sec
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1580
  Input Dimension: 789
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@001: 	loss(train)=0.3806 	loss(eval)=0.3079 	f1(train)=0.8426 	f1(eval)=0.8742 	duration(epoch)=0:00:21.575970
@002: 	loss(train)=0.3118 	loss(eval)=0.3012 	f1(train)=0.8700 	f1(eval)=0.8734 	duration(epoch)=0:00:20.655061
@003: 	loss(train)=0.2990 	loss(eval)=0.2821 	f1(train)=0.8748 	f1(eval)=0.8843 	duration(epoch)=0:00:20.774409
@004: 	loss(train)=0.2908 	loss(eval)=0.2747 	f1(train)=0.8796 	f1(eval)=0.8847 	duration(epoch)=0:00:20.699122
@005: 	loss(train)=0.2852 	loss(eval)=0.2704 	f1(train)=0.8825 	f1(eval)=0.8892 	duration(epoch)=0:00:20.773377
@006: 	loss(train)=0.2817 	loss(eval)=0.2675 	f1(train)=0.8846 	f1(eval)=0.8912 	duration(epoch)=0:00:20.424345
@007: 	loss(train)=0.2784 	loss(eval)=0.2903 	f1(train)=0.8848 	f1(eval)=0.8722 	duration(epoch)=0:00:20.754308
@008: 	loss(train)=0.2746 	loss(eval)=0.2628 	f1(train)=0.8885 	f1(eval)=0.8904 	duration(epoch)=0:00:21.049342
@009: 	loss(train)=0.2726 	loss(eval)=0.2623 	f1(train)=0.8891 	f1(eval)=0.8916 	duration(epoch)=0:00:20.738942
@010: 	loss(train)=0.2699 	loss(eval)=0.2614 	f1(train)=0.8896 	f1(eval)=0.8888 	duration(epoch)=0:00:20.742673
@011: 	loss(train)=0.2678 	loss(eval)=0.2667 	f1(train)=0.8914 	f1(eval)=0.8884 	duration(epoch)=0:00:20.407926
@012: 	loss(train)=0.2669 	loss(eval)=0.2645 	f1(train)=0.8914 	f1(eval)=0.8908 	duration(epoch)=0:00:20.470180
@013: 	loss(train)=0.2649 	loss(eval)=0.2600 	f1(train)=0.8926 	f1(eval)=0.8924 	duration(epoch)=0:00:20.510086
@014: 	loss(train)=0.2627 	loss(eval)=0.2582 	f1(train)=0.8934 	f1(eval)=0.8928 	duration(epoch)=0:00:20.440515
@015: 	loss(train)=0.2622 	loss(eval)=0.2592 	f1(train)=0.8927 	f1(eval)=0.8932 	duration(epoch)=0:00:21.010397
@016: 	loss(train)=0.2609 	loss(eval)=0.2564 	f1(train)=0.8944 	f1(eval)=0.8944 	duration(epoch)=0:00:21.332905
@017: 	loss(train)=0.2594 	loss(eval)=0.2559 	f1(train)=0.8942 	f1(eval)=0.8948 	duration(epoch)=0:00:21.310772
@018: 	loss(train)=0.2599 	loss(eval)=0.2555 	f1(train)=0.8945 	f1(eval)=0.8932 	duration(epoch)=0:00:21.130560
@019: 	loss(train)=0.2569 	loss(eval)=0.2539 	f1(train)=0.8962 	f1(eval)=0.8952 	duration(epoch)=0:00:21.176724
@020: 	loss(train)=0.2567 	loss(eval)=0.2570 	f1(train)=0.8961 	f1(eval)=0.8932 	duration(epoch)=0:00:21.015004
@021: 	loss(train)=0.2567 	loss(eval)=0.2543 	f1(train)=0.8957 	f1(eval)=0.8952 	duration(epoch)=0:00:20.981758
@022: 	loss(train)=0.2550 	loss(eval)=0.2681 	f1(train)=0.8970 	f1(eval)=0.8884 	duration(epoch)=0:00:20.983878
@023: 	loss(train)=0.2539 	loss(eval)=0.2598 	f1(train)=0.8967 	f1(eval)=0.8952 	duration(epoch)=0:00:21.168081
@024: 	loss(train)=0.2529 	loss(eval)=0.2543 	f1(train)=0.8974 	f1(eval)=0.8952 	duration(epoch)=0:00:20.810311
@025: 	loss(train)=0.2531 	loss(eval)=0.2533 	f1(train)=0.8993 	f1(eval)=0.8956 	duration(epoch)=0:00:21.151962
> Load best model based on evaluation loss.
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1580
  Input Dimension: 789
  Output Dimension: 2
@025: 	loss(train)=0.2531 	loss(eval)=0.2533 	f1(train)=0.8993 	f1(eval)=0.8956 	duration(epoch)=0:00:21.151962

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2222	 fp:      259 	 tn:     2222	 fn:      259	 pre=0.8956	 rec=0.8956	 f1=0.8956	 acc=0.8956
negative      	 tp:     1069	 fp:      127 	 tn:     1153	 fn:      132	 pre=0.8938	 rec=0.8901	 f1=0.8919	 acc=0.8956
positive      	 tp:     1153	 fp:      132 	 tn:     1069	 fn:      127	 pre=0.8973	 rec=0.9008	 f1=0.8990	 acc=0.8956
