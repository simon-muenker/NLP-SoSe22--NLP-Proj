> Loaded logger: ./experiments/hybrid/train.0.001/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.001.csv
> f(__load) took: 0.0026 sec
> f(__tokenize) took: 0.3707 sec
> f(__ngram) took: 0.3263 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0838 sec
> f(__tokenize) took: 1.2635 sec
> f(__ngram) took: 0.5389 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0644 sec
> f(__tokenize) took: 1.2202 sec
> f(__ngram) took: 0.6333 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.5338 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tagger', 'ner']
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.001.csv
> f(fit) took: 0.0154 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.001.csv
> f(predict) took: 0.0676 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.4996 sec

[--- TRAIN -> ./data/imdb.train.0.001.csv ---]
@001: 	loss(train)=0.6802 	loss(eval)=0.7050 	f1(train)=0.5750 	f1(eval)=0.4954 	duration(epoch)=0:01:06.400426
@002: 	loss(train)=0.6528 	loss(eval)=0.7149 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:07.376988
@003: 	loss(train)=0.6147 	loss(eval)=0.6882 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:06.999124
@004: 	loss(train)=0.6122 	loss(eval)=0.6547 	f1(train)=0.5750 	f1(eval)=0.6336 	duration(epoch)=0:01:08.200113
@005: 	loss(train)=0.5664 	loss(eval)=0.6456 	f1(train)=0.7250 	f1(eval)=0.6288 	duration(epoch)=0:01:07.832832
@006: 	loss(train)=0.5527 	loss(eval)=0.6637 	f1(train)=0.7000 	f1(eval)=0.5418 	duration(epoch)=0:01:08.300912
@007: 	loss(train)=0.5028 	loss(eval)=0.6248 	f1(train)=0.7750 	f1(eval)=0.6576 	duration(epoch)=0:01:07.901161
@008: 	loss(train)=0.4620 	loss(eval)=0.6188 	f1(train)=0.8500 	f1(eval)=0.6558 	duration(epoch)=0:01:07.186389
@009: 	loss(train)=0.4402 	loss(eval)=0.6000 	f1(train)=0.9250 	f1(eval)=0.6882 	duration(epoch)=0:01:08.422117
@010: 	loss(train)=0.3970 	loss(eval)=0.5782 	f1(train)=0.9000 	f1(eval)=0.7178 	duration(epoch)=0:01:07.124475
@011: 	loss(train)=0.3600 	loss(eval)=0.5746 	f1(train)=0.9500 	f1(eval)=0.7114 	duration(epoch)=0:01:07.347227
@012: 	loss(train)=0.3404 	loss(eval)=0.5649 	f1(train)=0.9000 	f1(eval)=0.7160 	duration(epoch)=0:01:08.010469
@013: 	loss(train)=0.3012 	loss(eval)=0.5674 	f1(train)=0.9500 	f1(eval)=0.7110 	duration(epoch)=0:01:07.701502
@014: 	loss(train)=0.2834 	loss(eval)=0.5505 	f1(train)=0.9500 	f1(eval)=0.7296 	duration(epoch)=0:01:07.532824
@015: 	loss(train)=0.2531 	loss(eval)=0.5738 	f1(train)=0.9750 	f1(eval)=0.7070 	duration(epoch)=0:01:07.778457
@016: 	loss(train)=0.2447 	loss(eval)=0.5521 	f1(train)=0.9500 	f1(eval)=0.7240 	duration(epoch)=0:01:07.323083
@017: 	loss(train)=0.2144 	loss(eval)=0.5554 	f1(train)=0.9500 	f1(eval)=0.7220 	duration(epoch)=0:01:07.660389
@018: 	loss(train)=0.1885 	loss(eval)=0.5561 	f1(train)=0.9500 	f1(eval)=0.7256 	duration(epoch)=0:01:07.969799
@019: 	loss(train)=0.1815 	loss(eval)=0.5604 	f1(train)=0.9500 	f1(eval)=0.7256 	duration(epoch)=0:01:08.304334
@020: 	loss(train)=0.1610 	loss(eval)=0.5614 	f1(train)=1.0000 	f1(eval)=0.7238 	duration(epoch)=0:01:07.874703
@021: 	loss(train)=0.1487 	loss(eval)=0.5697 	f1(train)=1.0000 	f1(eval)=0.7224 	duration(epoch)=0:01:07.901921
@022: 	loss(train)=0.1317 	loss(eval)=0.5772 	f1(train)=1.0000 	f1(eval)=0.7232 	duration(epoch)=0:01:07.670801
@023: 	loss(train)=0.1233 	loss(eval)=0.5789 	f1(train)=1.0000 	f1(eval)=0.7270 	duration(epoch)=0:01:07.971162
@024: 	loss(train)=0.1196 	loss(eval)=0.5811 	f1(train)=0.9500 	f1(eval)=0.7262 	duration(epoch)=0:01:07.682272
@025: 	loss(train)=0.0949 	loss(eval)=0.5937 	f1(train)=1.0000 	f1(eval)=0.7228 	duration(epoch)=0:01:07.863949
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 197908
@014: 	loss(train)=0.2834 	loss(eval)=0.5505 	f1(train)=0.9500 	f1(eval)=0.7296 	duration(epoch)=0:01:07.532824

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3648	 fp:     1352 	 tn:     3648	 fn:     1352	 pre=0.7296	 rec=0.7296	 f1=0.7296	 acc=0.7296
negative      	 tp:     1747	 fp:      622 	 tn:     1901	 fn:      730	 pre=0.7374	 rec=0.7053	 f1=0.7210	 acc=0.7296
positive      	 tp:     1901	 fp:      730 	 tn:     1747	 fn:      622	 pre=0.7225	 rec=0.7535	 f1=0.7377	 acc=0.7296
