> Loaded logger: ./experiments/hybrid/train.0.001/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.001.csv
> f(__load) took: 0.0024 sec
> f(__tokenize) took: 0.3300 sec
> f(__ngram) took: 0.2807 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0833 sec
> f(__tokenize) took: 1.2408 sec
> f(__ngram) took: 0.6110 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0642 sec
> f(__tokenize) took: 1.2465 sec
> f(__ngram) took: 0.5951 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.4600 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 30

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.001.csv
> f(fit) took: 0.0139 sec
> Predict with Freq. Classifier on ./data/imdb.train.0.001.csv
> f(predict) took: 0.0661 sec
> Apply Space Pipeline to: ./data/imdb.train.0.001.csv
> f(apply_spacy) took: 1.3272 sec
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 7.4826 sec
> Apply Space Pipeline to: ./data/imdb.eval.csv
> f(apply_spacy) took: 156.3550 sec

[--- TRAIN -> ./data/imdb.train.0.001.csv ---]
@001: 	loss(train)=0.6822 	loss(eval)=0.6888 	f1(train)=0.6000 	f1(eval)=0.4996 	duration(epoch)=0:01:05.134375
@002: 	loss(train)=0.6445 	loss(eval)=0.7007 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:05.685806
@003: 	loss(train)=0.6147 	loss(eval)=0.6882 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:06.300278
@004: 	loss(train)=0.6050 	loss(eval)=0.6847 	f1(train)=0.6000 	f1(eval)=0.4984 	duration(epoch)=0:01:06.292654
@005: 	loss(train)=0.5755 	loss(eval)=0.6724 	f1(train)=0.6000 	f1(eval)=0.5192 	duration(epoch)=0:01:06.250634
@006: 	loss(train)=0.5503 	loss(eval)=0.6655 	f1(train)=0.7000 	f1(eval)=0.5442 	duration(epoch)=0:01:07.192964
@007: 	loss(train)=0.5289 	loss(eval)=0.6413 	f1(train)=0.7500 	f1(eval)=0.6306 	duration(epoch)=0:01:06.044813
@008: 	loss(train)=0.4830 	loss(eval)=0.6333 	f1(train)=0.9000 	f1(eval)=0.6410 	duration(epoch)=0:01:06.779799
@009: 	loss(train)=0.4639 	loss(eval)=0.6172 	f1(train)=0.9000 	f1(eval)=0.6738 	duration(epoch)=0:01:06.649830
@010: 	loss(train)=0.4368 	loss(eval)=0.6096 	f1(train)=0.8750 	f1(eval)=0.6732 	duration(epoch)=0:01:06.622051
@011: 	loss(train)=0.4169 	loss(eval)=0.5974 	f1(train)=0.9000 	f1(eval)=0.6888 	duration(epoch)=0:01:07.341109
@012: 	loss(train)=0.3881 	loss(eval)=0.5751 	f1(train)=0.9000 	f1(eval)=0.7140 	duration(epoch)=0:01:06.572095
@013: 	loss(train)=0.3677 	loss(eval)=0.5963 	f1(train)=0.9250 	f1(eval)=0.6780 	duration(epoch)=0:01:06.806435
@014: 	loss(train)=0.3440 	loss(eval)=0.5609 	f1(train)=0.9000 	f1(eval)=0.7262 	duration(epoch)=0:01:07.009046
@015: 	loss(train)=0.2985 	loss(eval)=0.5570 	f1(train)=0.9500 	f1(eval)=0.7210 	duration(epoch)=0:01:07.118984
@016: 	loss(train)=0.2801 	loss(eval)=0.5668 	f1(train)=0.9500 	f1(eval)=0.7118 	duration(epoch)=0:01:07.021044
@017: 	loss(train)=0.2284 	loss(eval)=0.5515 	f1(train)=1.0000 	f1(eval)=0.7234 	duration(epoch)=0:01:06.170099
@018: 	loss(train)=0.2281 	loss(eval)=0.5591 	f1(train)=0.9500 	f1(eval)=0.7176 	duration(epoch)=0:01:07.267840
@019: 	loss(train)=0.2082 	loss(eval)=0.5558 	f1(train)=0.9750 	f1(eval)=0.7234 	duration(epoch)=0:01:06.639030
@020: 	loss(train)=0.1977 	loss(eval)=0.5591 	f1(train)=0.9500 	f1(eval)=0.7240 	duration(epoch)=0:01:06.937578
@021: 	loss(train)=0.1581 	loss(eval)=0.5630 	f1(train)=0.9750 	f1(eval)=0.7234 	duration(epoch)=0:01:06.579288
@022: 	loss(train)=0.1628 	loss(eval)=0.5661 	f1(train)=0.9750 	f1(eval)=0.7248 	duration(epoch)=0:01:06.348664
@023: 	loss(train)=0.1531 	loss(eval)=0.5656 	f1(train)=0.9750 	f1(eval)=0.7266 	duration(epoch)=0:01:07.302178
@024: 	loss(train)=0.1361 	loss(eval)=0.5731 	f1(train)=0.9750 	f1(eval)=0.7268 	duration(epoch)=0:01:06.183469
@025: 	loss(train)=0.0971 	loss(eval)=0.5795 	f1(train)=1.0000 	f1(eval)=0.7258 	duration(epoch)=0:01:06.210557
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197892
> Init Neural Assemble (MLP), trainable parameters: 30
@024: 	loss(train)=0.1361 	loss(eval)=0.5731 	f1(train)=0.9750 	f1(eval)=0.7268 	duration(epoch)=0:01:06.183469

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3634	 fp:     1366 	 tn:     3634	 fn:     1366	 pre=0.7268	 rec=0.7268	 f1=0.7268	 acc=0.7268
negative      	 tp:     1795	 fp:      684 	 tn:     1839	 fn:      682	 pre=0.7241	 rec=0.7247	 f1=0.7244	 acc=0.7268
positive      	 tp:     1839	 fp:      682 	 tn:     1795	 fn:      684	 pre=0.7295	 rec=0.7289	 f1=0.7292	 acc=0.7268
