> Loaded logger: ./experiments/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2288 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 20170 
  Memory Usage: 27.5464 MB
> f(__tokenize) took: 3.5414 sec
> f(__ngram) took: 1.2852 sec
> f(__ngram) took: 2.4189 sec
> f(__load) took: 0.0330 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2242 
  Memory Usage: 3.0493 MB
> f(__tokenize) took: 0.9829 sec
> f(__ngram) took: 0.6203 sec
> f(__ngram) took: 0.6410 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 2.9920 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 83.0988 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 13.1720 sec
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0002 MB
  Trainable parameters: 44
  Input Dimension: 21
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@005: 	loss(train)=0.5775 	loss(eval)=0.5432 	f1(train)=0.7089 	f1(eval)=0.7877 	duration(epoch)=0:00:15.751035
@010: 	loss(train)=0.5466 	loss(eval)=0.5061 	f1(train)=0.7269 	f1(eval)=0.7881 	duration(epoch)=0:00:15.304170
@015: 	loss(train)=0.5410 	loss(eval)=0.4833 	f1(train)=0.7235 	f1(eval)=0.8162 	duration(epoch)=0:00:15.354497
@020: 	loss(train)=0.5306 	loss(eval)=0.4676 	f1(train)=0.7307 	f1(eval)=0.8234 	duration(epoch)=0:00:15.562576
@025: 	loss(train)=0.5286 	loss(eval)=0.4613 	f1(train)=0.7348 	f1(eval)=0.8260 	duration(epoch)=0:00:15.726821
@030: 	loss(train)=0.5292 	loss(eval)=0.4561 	f1(train)=0.7336 	f1(eval)=0.8278 	duration(epoch)=0:00:15.296393
@035: 	loss(train)=0.5243 	loss(eval)=0.4567 	f1(train)=0.7366 	f1(eval)=0.8278 	duration(epoch)=0:00:15.589251
@040: 	loss(train)=0.5197 	loss(eval)=0.4593 	f1(train)=0.7392 	f1(eval)=0.8287 	duration(epoch)=0:00:15.743818
@045: 	loss(train)=0.5191 	loss(eval)=0.4485 	f1(train)=0.7417 	f1(eval)=0.8283 	duration(epoch)=0:00:15.302287
@050: 	loss(train)=0.5177 	loss(eval)=0.4495 	f1(train)=0.7400 	f1(eval)=0.8274 	duration(epoch)=0:00:15.754813
@055: 	loss(train)=0.5170 	loss(eval)=0.4587 	f1(train)=0.7398 	f1(eval)=0.8327 	duration(epoch)=0:00:15.764895
@060: 	loss(train)=0.5163 	loss(eval)=0.4483 	f1(train)=0.7425 	f1(eval)=0.8332 	duration(epoch)=0:00:15.565709
@065: 	loss(train)=0.5161 	loss(eval)=0.4476 	f1(train)=0.7420 	f1(eval)=0.8327 	duration(epoch)=0:00:15.911686
@070: 	loss(train)=0.5119 	loss(eval)=0.4451 	f1(train)=0.7472 	f1(eval)=0.8345 	duration(epoch)=0:00:15.775075
@075: 	loss(train)=0.5111 	loss(eval)=0.4483 	f1(train)=0.7471 	f1(eval)=0.8376 	duration(epoch)=0:00:15.465476
@080: 	loss(train)=0.5097 	loss(eval)=0.4429 	f1(train)=0.7482 	f1(eval)=0.8318 	duration(epoch)=0:00:15.895681
@085: 	loss(train)=0.5057 	loss(eval)=0.4421 	f1(train)=0.7545 	f1(eval)=0.8399 	duration(epoch)=0:00:15.706177
@090: 	loss(train)=0.5132 	loss(eval)=0.4403 	f1(train)=0.7425 	f1(eval)=0.8350 	duration(epoch)=0:00:15.774608
@095: 	loss(train)=0.5102 	loss(eval)=0.4373 	f1(train)=0.7481 	f1(eval)=0.8417 	duration(epoch)=0:00:15.295797
@100: 	loss(train)=0.5043 	loss(eval)=0.4469 	f1(train)=0.7485 	f1(eval)=0.8296 	duration(epoch)=0:00:15.739945
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0002 MB
  Trainable parameters: 44
  Input Dimension: 21
  Output Dimension: 2
@095: 	loss(train)=0.5102 	loss(eval)=0.4373 	f1(train)=0.7481 	f1(eval)=0.8417 	duration(epoch)=0:00:15.295797

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     1887	 fp:      355 	 tn:     1887	 fn:      355	 pre=0.8417	 rec=0.8417	 f1=0.8417	 acc=0.8417
negative      	 tp:      774	 fp:      117 	 tn:     1113	 fn:      238	 pre=0.8687	 rec=0.7648	 f1=0.8135	 acc=0.8417
positive      	 tp:     1113	 fp:      238 	 tn:      774	 fn:      117	 pre=0.8238	 rec=0.9049	 f1=0.8625	 acc=0.8417
