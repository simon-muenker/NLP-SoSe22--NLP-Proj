> Loaded logger: ./experiments/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2671 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 22412 
  Memory Usage: 30.5957 MB
> f(__tokenize) took: 3.7201 sec
> f(__ngram) took: 1.6941 sec
> f(__ngram) took: 1.5918 sec
> f(__load) took: 0.0350 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2491 
  Memory Usage: 3.3220 MB
> f(__tokenize) took: 1.0365 sec
> f(__ngram) took: 0.7210 sec
> f(__ngram) took: 0.7305 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 3.1205 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 113.9553 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 15.4549 sec
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0019 MB
  Trainable parameters: 506
  Input Dimension: 21
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@005: 	loss(train)=0.3739 	loss(eval)=0.4927 	f1(train)=0.8349 	f1(eval)=0.7523 	duration(epoch)=0:00:18.241772
@010: 	loss(train)=0.3545 	loss(eval)=0.3882 	f1(train)=0.8429 	f1(eval)=0.8226 	duration(epoch)=0:00:17.964871
@015: 	loss(train)=0.3378 	loss(eval)=0.4051 	f1(train)=0.8536 	f1(eval)=0.8129 	duration(epoch)=0:00:18.588637
@020: 	loss(train)=0.3208 	loss(eval)=0.3435 	f1(train)=0.8626 	f1(eval)=0.8422 	duration(epoch)=0:00:18.496868
@025: 	loss(train)=0.3076 	loss(eval)=0.4118 	f1(train)=0.8694 	f1(eval)=0.8017 	duration(epoch)=0:00:18.237680
@030: 	loss(train)=0.3013 	loss(eval)=0.4169 	f1(train)=0.8731 	f1(eval)=0.8009 	duration(epoch)=0:00:18.402239
@035: 	loss(train)=0.2973 	loss(eval)=0.3450 	f1(train)=0.8750 	f1(eval)=0.8414 	duration(epoch)=0:00:17.911766
@040: 	loss(train)=0.2969 	loss(eval)=0.4378 	f1(train)=0.8736 	f1(eval)=0.7916 	duration(epoch)=0:00:18.260189
@045: 	loss(train)=0.2958 	loss(eval)=0.3675 	f1(train)=0.8760 	f1(eval)=0.8290 	duration(epoch)=0:00:17.751498
@050: 	loss(train)=0.2951 	loss(eval)=0.3184 	f1(train)=0.8756 	f1(eval)=0.8603 	duration(epoch)=0:00:18.098791
@055: 	loss(train)=0.2961 	loss(eval)=0.3721 	f1(train)=0.8748 	f1(eval)=0.8286 	duration(epoch)=0:00:18.061422
@060: 	loss(train)=0.2958 	loss(eval)=0.3032 	f1(train)=0.8759 	f1(eval)=0.8695 	duration(epoch)=0:00:18.073863
@065: 	loss(train)=0.2952 	loss(eval)=0.3992 	f1(train)=0.8755 	f1(eval)=0.8202 	duration(epoch)=0:00:17.953261
@070: 	loss(train)=0.2947 	loss(eval)=0.4391 	f1(train)=0.8753 	f1(eval)=0.7933 	duration(epoch)=0:00:17.638918
@075: 	loss(train)=0.2955 	loss(eval)=0.4476 	f1(train)=0.8739 	f1(eval)=0.7916 	duration(epoch)=0:00:18.198546
@080: 	loss(train)=0.2950 	loss(eval)=0.3686 	f1(train)=0.8736 	f1(eval)=0.8322 	duration(epoch)=0:00:18.004970
@085: 	loss(train)=0.2963 	loss(eval)=0.4664 	f1(train)=0.8744 	f1(eval)=0.7792 	duration(epoch)=0:00:18.290036
@090: 	loss(train)=0.2951 	loss(eval)=0.3427 	f1(train)=0.8755 	f1(eval)=0.8483 	duration(epoch)=0:00:18.019806
@095: 	loss(train)=0.2953 	loss(eval)=0.3479 	f1(train)=0.8745 	f1(eval)=0.8450 	duration(epoch)=0:00:18.207766
@100: 	loss(train)=0.2962 	loss(eval)=0.4323 	f1(train)=0.8753 	f1(eval)=0.7957 	duration(epoch)=0:00:18.270220
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0019 MB
  Trainable parameters: 506
  Input Dimension: 21
  Output Dimension: 2
@064: 	loss(train)=0.2958 	loss(eval)=0.2868 	f1(train)=0.8752 	f1(eval)=0.8812 	duration(epoch)=0:00:18.056312

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2195	 fp:      296 	 tn:     2195	 fn:      296	 pre=0.8812	 rec=0.8812	 f1=0.8812	 acc=0.8812
negative      	 tp:     2195	 fp:        0 	 tn:        0	 fn:      296	 pre=1.0000	 rec=0.8812	 f1=0.9368	 acc=0.8812
positive      	 tp:        0	 fp:      296 	 tn:     2195	 fn:        0	 pre=0.0000	 rec=0.0000	 f1=0.0000	 acc=0.8812
