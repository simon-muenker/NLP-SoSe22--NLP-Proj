> Loaded logger: ./experiments/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.2812 sec
> Load/Init from ./data/imdb.train.csv
  Number of Samples: 25000 
  Memory Usage: 34.7926 MB
> f(__tokenize) took: 4.6713 sec
> f(__ngram) took: 1.6679 sec
> f(__ngram) took: 1.8920 sec
> f(__load) took: 0.0340 sec
> Load/Init from ./data/imdb.eval.csv
  Number of Samples: 2481 
  Memory Usage: 3.3606 MB
> f(__tokenize) took: 1.2340 sec
> f(__ngram) took: 0.8388 sec
> f(__ngram) took: 0.8899 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 768), ('2', 5120)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb.train.csv on (only N-Gram Group Counter)
> f(fit) took: 3.6224 sec
> Apply Feature Pipeline on ./data/imdb.train.csv
> f(apply) took: 101.7587 sec
> Apply Feature Pipeline on ./data/imdb.eval.csv
> f(apply) took: 15.7962 sec
> Init nGrams+NeLa Weighting (Features)
  Trainable parameters: 44
  Input Dimension: 21
  Output Dimension: 2

[--- TRAIN -> ./data/imdb.train.csv ---]
@001: 	loss(train)=0.5610 	loss(eval)=0.4838 	f1(train)=0.7405 	f1(eval)=0.7920 	duration(epoch)=0:00:19.632758
@002: 	loss(train)=0.4952 	loss(eval)=0.4476 	f1(train)=0.7623 	f1(eval)=0.8089 	duration(epoch)=0:00:19.490443
@003: 	loss(train)=0.4790 	loss(eval)=0.4360 	f1(train)=0.7690 	f1(eval)=0.8045 	duration(epoch)=0:00:19.514730
@004: 	loss(train)=0.4686 	loss(eval)=0.4220 	f1(train)=0.7709 	f1(eval)=0.8134 	duration(epoch)=0:00:19.393718
@005: 	loss(train)=0.4646 	loss(eval)=0.4183 	f1(train)=0.7760 	f1(eval)=0.8134 	duration(epoch)=0:00:19.390768
@006: 	loss(train)=0.4582 	loss(eval)=0.4106 	f1(train)=0.7820 	f1(eval)=0.8214 	duration(epoch)=0:00:19.440575
@007: 	loss(train)=0.4544 	loss(eval)=0.4075 	f1(train)=0.7874 	f1(eval)=0.8198 	duration(epoch)=0:00:19.317723
@008: 	loss(train)=0.4512 	loss(eval)=0.4071 	f1(train)=0.7884 	f1(eval)=0.8227 	duration(epoch)=0:00:19.340822
@009: 	loss(train)=0.4480 	loss(eval)=0.4050 	f1(train)=0.7884 	f1(eval)=0.8271 	duration(epoch)=0:00:19.285057
@010: 	loss(train)=0.4431 	loss(eval)=0.4026 	f1(train)=0.7918 	f1(eval)=0.8222 	duration(epoch)=0:00:19.285037
@011: 	loss(train)=0.4405 	loss(eval)=0.4054 	f1(train)=0.7921 	f1(eval)=0.8182 	duration(epoch)=0:00:19.336575
@012: 	loss(train)=0.4388 	loss(eval)=0.3990 	f1(train)=0.7947 	f1(eval)=0.8222 	duration(epoch)=0:00:19.268681
@013: 	loss(train)=0.4332 	loss(eval)=0.3979 	f1(train)=0.7978 	f1(eval)=0.8222 	duration(epoch)=0:00:19.325817
@014: 	loss(train)=0.4293 	loss(eval)=0.3939 	f1(train)=0.7998 	f1(eval)=0.8319 	duration(epoch)=0:00:19.356205
@015: 	loss(train)=0.4273 	loss(eval)=0.3922 	f1(train)=0.8013 	f1(eval)=0.8347 	duration(epoch)=0:00:19.350248
@016: 	loss(train)=0.4270 	loss(eval)=0.3919 	f1(train)=0.7993 	f1(eval)=0.8323 	duration(epoch)=0:00:19.363199
@017: 	loss(train)=0.4229 	loss(eval)=0.3932 	f1(train)=0.8063 	f1(eval)=0.8231 	duration(epoch)=0:00:19.453137
@018: 	loss(train)=0.4207 	loss(eval)=0.3906 	f1(train)=0.8050 	f1(eval)=0.8347 	duration(epoch)=0:00:19.446368
@019: 	loss(train)=0.4184 	loss(eval)=0.3894 	f1(train)=0.8076 	f1(eval)=0.8287 	duration(epoch)=0:00:19.550407
@020: 	loss(train)=0.4187 	loss(eval)=0.3866 	f1(train)=0.8044 	f1(eval)=0.8315 	duration(epoch)=0:00:19.528569
@021: 	loss(train)=0.4158 	loss(eval)=0.3872 	f1(train)=0.8097 	f1(eval)=0.8263 	duration(epoch)=0:00:19.434488
@022: 	loss(train)=0.4146 	loss(eval)=0.3850 	f1(train)=0.8067 	f1(eval)=0.8299 	duration(epoch)=0:00:19.459235
@023: 	loss(train)=0.4135 	loss(eval)=0.3846 	f1(train)=0.8100 	f1(eval)=0.8299 	duration(epoch)=0:00:19.383315
@024: 	loss(train)=0.4092 	loss(eval)=0.3850 	f1(train)=0.8123 	f1(eval)=0.8287 	duration(epoch)=0:00:19.339833
@025: 	loss(train)=0.4118 	loss(eval)=0.3817 	f1(train)=0.8115 	f1(eval)=0.8364 	duration(epoch)=0:00:19.325546
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Trainable parameters: 44
  Input Dimension: 21
  Output Dimension: 2
@025: 	loss(train)=0.4118 	loss(eval)=0.3817 	f1(train)=0.8115 	f1(eval)=0.8364 	duration(epoch)=0:00:19.325546

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     2075	 fp:      406 	 tn:     2075	 fn:      406	 pre=0.8364	 rec=0.8364	 f1=0.8364	 acc=0.8364
negative      	 tp:      940	 fp:      145 	 tn:     1135	 fn:      261	 pre=0.8664	 rec=0.7827	 f1=0.8224	 acc=0.8364
positive      	 tp:     1135	 fp:      261 	 tn:      940	 fn:      145	 pre=0.8130	 rec=0.8867	 f1=0.8483	 acc=0.8364
