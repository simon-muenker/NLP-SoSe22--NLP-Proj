> Loaded logger: ./experiments/transformer/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0097 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0605 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0612 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.8038 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6953 	loss(eval)=0.6705 	f1(train)=0.5325 	f1(eval)=0.5864 	duration(epoch)=0:01:06.948977
@002: 	loss(train)=0.6000 	loss(eval)=0.6488 	f1(train)=0.6775 	f1(eval)=0.6220 	duration(epoch)=0:01:07.847506
@003: 	loss(train)=0.5238 	loss(eval)=0.6648 	f1(train)=0.7500 	f1(eval)=0.6162 	duration(epoch)=0:01:08.180040
@004: 	loss(train)=0.4799 	loss(eval)=0.7110 	f1(train)=0.7575 	f1(eval)=0.6076 	duration(epoch)=0:01:08.248446
@005: 	loss(train)=0.4516 	loss(eval)=0.6981 	f1(train)=0.7950 	f1(eval)=0.6166 	duration(epoch)=0:01:08.234867
@006: 	loss(train)=0.4148 	loss(eval)=0.8109 	f1(train)=0.7950 	f1(eval)=0.5882 	duration(epoch)=0:01:08.181073
@007: 	loss(train)=0.4051 	loss(eval)=0.6654 	f1(train)=0.8100 	f1(eval)=0.6430 	duration(epoch)=0:01:08.184924
@008: 	loss(train)=0.3064 	loss(eval)=0.7451 	f1(train)=0.8925 	f1(eval)=0.6250 	duration(epoch)=0:01:08.303531
@009: 	loss(train)=0.3408 	loss(eval)=0.7690 	f1(train)=0.8450 	f1(eval)=0.6238 	duration(epoch)=0:01:08.179092
@010: 	loss(train)=0.2920 	loss(eval)=0.7405 	f1(train)=0.8900 	f1(eval)=0.6364 	duration(epoch)=0:01:08.221286
@011: 	loss(train)=0.2550 	loss(eval)=0.7343 	f1(train)=0.9125 	f1(eval)=0.6460 	duration(epoch)=0:01:08.253651
@012: 	loss(train)=0.2666 	loss(eval)=0.7458 	f1(train)=0.8850 	f1(eval)=0.6422 	duration(epoch)=0:01:08.246106
@013: 	loss(train)=0.2119 	loss(eval)=0.7491 	f1(train)=0.9000 	f1(eval)=0.6466 	duration(epoch)=0:01:08.287498
@014: 	loss(train)=0.1789 	loss(eval)=0.7814 	f1(train)=0.9575 	f1(eval)=0.6438 	duration(epoch)=0:01:08.272239
@015: 	loss(train)=0.2276 	loss(eval)=0.8232 	f1(train)=0.9075 	f1(eval)=0.6366 	duration(epoch)=0:01:08.269614
@016: 	loss(train)=0.2032 	loss(eval)=0.8090 	f1(train)=0.9225 	f1(eval)=0.6382 	duration(epoch)=0:01:08.162228
@017: 	loss(train)=0.1593 	loss(eval)=0.8233 	f1(train)=0.9500 	f1(eval)=0.6426 	duration(epoch)=0:01:08.181384
@018: 	loss(train)=0.1584 	loss(eval)=0.8309 	f1(train)=0.9450 	f1(eval)=0.6426 	duration(epoch)=0:01:08.290312
@019: 	loss(train)=0.1365 	loss(eval)=0.8672 	f1(train)=0.9550 	f1(eval)=0.6376 	duration(epoch)=0:01:08.266648
@020: 	loss(train)=0.1315 	loss(eval)=0.9260 	f1(train)=0.9600 	f1(eval)=0.6378 	duration(epoch)=0:01:08.180871
@021: 	loss(train)=0.1264 	loss(eval)=0.9288 	f1(train)=0.9550 	f1(eval)=0.6388 	duration(epoch)=0:01:08.222177
@022: 	loss(train)=0.0964 	loss(eval)=0.9265 	f1(train)=0.9675 	f1(eval)=0.6414 	duration(epoch)=0:01:08.283921
@023: 	loss(train)=0.1143 	loss(eval)=0.8807 	f1(train)=0.9625 	f1(eval)=0.6402 	duration(epoch)=0:01:08.132863
@024: 	loss(train)=0.1045 	loss(eval)=0.9460 	f1(train)=0.9750 	f1(eval)=0.6298 	duration(epoch)=0:01:08.172545
@025: 	loss(train)=0.1233 	loss(eval)=1.0366 	f1(train)=0.9500 	f1(eval)=0.6240 	duration(epoch)=0:01:08.248517
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@013: 	loss(train)=0.2119 	loss(eval)=0.7491 	f1(train)=0.9000 	f1(eval)=0.6466 	duration(epoch)=0:01:08.287498

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3233	 fp:     1767 	 tn:     3233	 fn:     1767	 pre=0.6466	 rec=0.6466	 f1=0.6466	 acc=0.6466
negative      	 tp:     1610	 fp:      914 	 tn:     1623	 fn:      853	 pre=0.6379	 rec=0.6537	 f1=0.6457	 acc=0.6466
positive      	 tp:     1623	 fp:      853 	 tn:     1610	 fn:      914	 pre=0.6555	 rec=0.6397	 f1=0.6475	 acc=0.6466
