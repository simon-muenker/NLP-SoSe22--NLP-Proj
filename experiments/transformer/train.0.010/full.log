> Loaded logger: ./experiments/transformer/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0103 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0585 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0599 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.8088 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.6727 	loss(eval)=0.6418 	f1(train)=0.6000 	f1(eval)=0.6594 	duration(epoch)=0:01:07.602700
@002: 	loss(train)=0.6160 	loss(eval)=0.5921 	f1(train)=0.7425 	f1(eval)=0.7926 	duration(epoch)=0:01:08.503280
@003: 	loss(train)=0.5696 	loss(eval)=0.5484 	f1(train)=0.7925 	f1(eval)=0.7942 	duration(epoch)=0:01:08.862783
@004: 	loss(train)=0.5235 	loss(eval)=0.5113 	f1(train)=0.8100 	f1(eval)=0.8070 	duration(epoch)=0:01:08.817307
@005: 	loss(train)=0.4765 	loss(eval)=0.4849 	f1(train)=0.8075 	f1(eval)=0.8052 	duration(epoch)=0:01:08.877359
@006: 	loss(train)=0.4465 	loss(eval)=0.4612 	f1(train)=0.8250 	f1(eval)=0.8028 	duration(epoch)=0:01:08.812686
@007: 	loss(train)=0.4441 	loss(eval)=0.4477 	f1(train)=0.8400 	f1(eval)=0.8064 	duration(epoch)=0:01:08.837640
@008: 	loss(train)=0.4024 	loss(eval)=0.4197 	f1(train)=0.8575 	f1(eval)=0.8268 	duration(epoch)=0:01:08.918432
@009: 	loss(train)=0.3861 	loss(eval)=0.4171 	f1(train)=0.8675 	f1(eval)=0.8208 	duration(epoch)=0:01:08.911890
@010: 	loss(train)=0.3669 	loss(eval)=0.4039 	f1(train)=0.8500 	f1(eval)=0.8282 	duration(epoch)=0:01:08.718649
@011: 	loss(train)=0.3493 	loss(eval)=0.3950 	f1(train)=0.8725 	f1(eval)=0.8356 	duration(epoch)=0:01:08.953919
@012: 	loss(train)=0.3322 	loss(eval)=0.3890 	f1(train)=0.8775 	f1(eval)=0.8366 	duration(epoch)=0:01:08.823077
@013: 	loss(train)=0.3238 	loss(eval)=0.3846 	f1(train)=0.8700 	f1(eval)=0.8396 	duration(epoch)=0:01:08.939652
@014: 	loss(train)=0.3190 	loss(eval)=0.4039 	f1(train)=0.8725 	f1(eval)=0.8162 	duration(epoch)=0:01:08.891291
@015: 	loss(train)=0.3192 	loss(eval)=0.3777 	f1(train)=0.8675 	f1(eval)=0.8404 	duration(epoch)=0:01:08.914452
@016: 	loss(train)=0.2936 	loss(eval)=0.3793 	f1(train)=0.8925 	f1(eval)=0.8384 	duration(epoch)=0:01:08.744781
@017: 	loss(train)=0.2878 	loss(eval)=0.3790 	f1(train)=0.8775 	f1(eval)=0.8346 	duration(epoch)=0:01:09.008074
@018: 	loss(train)=0.2772 	loss(eval)=0.3711 	f1(train)=0.9050 	f1(eval)=0.8394 	duration(epoch)=0:01:08.823529
@019: 	loss(train)=0.2664 	loss(eval)=0.3714 	f1(train)=0.8950 	f1(eval)=0.8398 	duration(epoch)=0:01:08.917840
@020: 	loss(train)=0.2641 	loss(eval)=0.3704 	f1(train)=0.9050 	f1(eval)=0.8432 	duration(epoch)=0:01:08.992484
@021: 	loss(train)=0.2535 	loss(eval)=0.3750 	f1(train)=0.9075 	f1(eval)=0.8384 	duration(epoch)=0:01:09.026113
@022: 	loss(train)=0.2401 	loss(eval)=0.3696 	f1(train)=0.9100 	f1(eval)=0.8408 	duration(epoch)=0:01:08.899748
@023: 	loss(train)=0.2458 	loss(eval)=0.3677 	f1(train)=0.9075 	f1(eval)=0.8432 	duration(epoch)=0:01:08.743472
@024: 	loss(train)=0.2217 	loss(eval)=0.3711 	f1(train)=0.9200 	f1(eval)=0.8434 	duration(epoch)=0:01:08.879153
@025: 	loss(train)=0.2214 	loss(eval)=0.3716 	f1(train)=0.9175 	f1(eval)=0.8440 	duration(epoch)=0:01:08.944576
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@025: 	loss(train)=0.2214 	loss(eval)=0.3716 	f1(train)=0.9175 	f1(eval)=0.8440 	duration(epoch)=0:01:08.944576

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4220	 fp:      780 	 tn:     4220	 fn:      780	 pre=0.8440	 rec=0.8440	 f1=0.8440	 acc=0.8440
negative      	 tp:     2141	 fp:      444 	 tn:     2079	 fn:      336	 pre=0.8282	 rec=0.8644	 f1=0.8459	 acc=0.8440
positive      	 tp:     2079	 fp:      336 	 tn:     2141	 fn:      444	 pre=0.8609	 rec=0.8240	 f1=0.8420	 acc=0.8440
