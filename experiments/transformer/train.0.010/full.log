> Loaded logger: ./experiments/transformer/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0087 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0554 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0565 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.6013 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.010.csv ---]
@001: 	loss(train)=0.1800 	loss(eval)=0.2287 	f1(train)=0.9150 	f1(eval)=0.9566 	duration(epoch)=0:01:07.586554
@002: 	loss(train)=0.0850 	loss(eval)=0.1589 	f1(train)=0.9725 	f1(eval)=0.9570 	duration(epoch)=0:01:08.093228
@003: 	loss(train)=0.0686 	loss(eval)=0.1459 	f1(train)=0.9825 	f1(eval)=0.9574 	duration(epoch)=0:01:08.322759
@004: 	loss(train)=0.0531 	loss(eval)=0.1352 	f1(train)=0.9875 	f1(eval)=0.9562 	duration(epoch)=0:01:08.520461
@005: 	loss(train)=0.0425 	loss(eval)=0.1481 	f1(train)=0.9900 	f1(eval)=0.9558 	duration(epoch)=0:01:08.556984
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@003: 	loss(train)=0.0686 	loss(eval)=0.1459 	f1(train)=0.9825 	f1(eval)=0.9574 	duration(epoch)=0:01:08.322759

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4787	 fp:      213 	 tn:     4787	 fn:      213	 pre=0.9574	 rec=0.9574	 f1=0.9574	 acc=0.9574
negative      	 tp:     2374	 fp:      124 	 tn:     2413	 fn:       89	 pre=0.9504	 rec=0.9639	 f1=0.9571	 acc=0.9574
positive      	 tp:     2413	 fp:       89 	 tn:     2374	 fn:      124	 pre=0.9644	 rec=0.9511	 f1=0.9577	 acc=0.9574
