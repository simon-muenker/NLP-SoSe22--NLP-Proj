> Loaded logger: ./experiments/transformer/train.0.001/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.001.csv
> f(__load) took: 0.0024 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0622 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0585 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.5983 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.001.csv ---]
@001: 	loss(train)=0.6913 	loss(eval)=0.6907 	f1(train)=0.5000 	f1(eval)=0.5068 	duration(epoch)=0:01:02.809480
@002: 	loss(train)=0.6109 	loss(eval)=0.7011 	f1(train)=0.6500 	f1(eval)=0.4968 	duration(epoch)=0:01:03.480029
@003: 	loss(train)=0.6463 	loss(eval)=0.7127 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:04.032099
@004: 	loss(train)=0.5633 	loss(eval)=0.7130 	f1(train)=0.6000 	f1(eval)=0.4954 	duration(epoch)=0:01:04.171905
@005: 	loss(train)=0.6785 	loss(eval)=0.7114 	f1(train)=0.6000 	f1(eval)=0.4962 	duration(epoch)=0:01:04.329295
@006: 	loss(train)=0.6638 	loss(eval)=0.6974 	f1(train)=0.6250 	f1(eval)=0.4980 	duration(epoch)=0:01:04.224881
@007: 	loss(train)=0.6102 	loss(eval)=0.6784 	f1(train)=0.6000 	f1(eval)=0.5162 	duration(epoch)=0:01:04.317405
@008: 	loss(train)=0.5678 	loss(eval)=0.6626 	f1(train)=0.6500 	f1(eval)=0.5642 	duration(epoch)=0:01:04.178883
@009: 	loss(train)=0.5912 	loss(eval)=0.6518 	f1(train)=0.7500 	f1(eval)=0.6088 	duration(epoch)=0:01:04.226803
@010: 	loss(train)=0.5359 	loss(eval)=0.6425 	f1(train)=0.7750 	f1(eval)=0.6632 	duration(epoch)=0:01:04.229456
@011: 	loss(train)=0.5375 	loss(eval)=0.6355 	f1(train)=0.8500 	f1(eval)=0.6790 	duration(epoch)=0:01:04.328985
@012: 	loss(train)=0.5277 	loss(eval)=0.6319 	f1(train)=0.8500 	f1(eval)=0.6798 	duration(epoch)=0:01:04.158251
@013: 	loss(train)=0.5202 	loss(eval)=0.6305 	f1(train)=0.8000 	f1(eval)=0.6724 	duration(epoch)=0:01:04.250263
@014: 	loss(train)=0.5066 	loss(eval)=0.6255 	f1(train)=0.8250 	f1(eval)=0.6780 	duration(epoch)=0:01:04.295480
@015: 	loss(train)=0.4699 	loss(eval)=0.6191 	f1(train)=0.8750 	f1(eval)=0.6892 	duration(epoch)=0:01:04.213184
@016: 	loss(train)=0.4496 	loss(eval)=0.6137 	f1(train)=0.9000 	f1(eval)=0.7010 	duration(epoch)=0:01:04.185548
@017: 	loss(train)=0.4649 	loss(eval)=0.6106 	f1(train)=0.8500 	f1(eval)=0.6942 	duration(epoch)=0:01:04.284032
@018: 	loss(train)=0.4727 	loss(eval)=0.6094 	f1(train)=0.8750 	f1(eval)=0.6840 	duration(epoch)=0:01:04.232721
@019: 	loss(train)=0.4639 	loss(eval)=0.6051 	f1(train)=0.8500 	f1(eval)=0.6934 	duration(epoch)=0:01:04.300392
@020: 	loss(train)=0.4279 	loss(eval)=0.5991 	f1(train)=0.8750 	f1(eval)=0.7024 	duration(epoch)=0:01:04.256486
@021: 	loss(train)=0.4219 	loss(eval)=0.5950 	f1(train)=0.9000 	f1(eval)=0.7042 	duration(epoch)=0:01:04.269959
@022: 	loss(train)=0.3814 	loss(eval)=0.5892 	f1(train)=0.9000 	f1(eval)=0.7092 	duration(epoch)=0:01:04.379105
@023: 	loss(train)=0.3682 	loss(eval)=0.5842 	f1(train)=0.9250 	f1(eval)=0.7132 	duration(epoch)=0:01:04.227759
@024: 	loss(train)=0.3352 	loss(eval)=0.5826 	f1(train)=0.9250 	f1(eval)=0.7126 	duration(epoch)=0:01:04.267199
@025: 	loss(train)=0.3881 	loss(eval)=0.5817 	f1(train)=0.9500 	f1(eval)=0.7102 	duration(epoch)=0:01:04.209203
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@023: 	loss(train)=0.3682 	loss(eval)=0.5842 	f1(train)=0.9250 	f1(eval)=0.7132 	duration(epoch)=0:01:04.227759

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3566	 fp:     1434 	 tn:     3566	 fn:     1434	 pre=0.7132	 rec=0.7132	 f1=0.7132	 acc=0.7132
negative      	 tp:     1995	 fp:      952 	 tn:     1571	 fn:      482	 pre=0.6770	 rec=0.8054	 f1=0.7356	 acc=0.7132
positive      	 tp:     1571	 fp:      482 	 tn:     1995	 fn:      952	 pre=0.7652	 rec=0.6227	 f1=0.6866	 acc=0.7132
