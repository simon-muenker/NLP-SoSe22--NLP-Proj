> Loaded logger: ./experiments/transformer/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0498 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0594 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0588 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.3593 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.1541 	loss(eval)=0.1169 	f1(train)=0.9485 	f1(eval)=0.9586 	duration(epoch)=0:01:53.886121
@002: 	loss(train)=0.1246 	loss(eval)=0.1170 	f1(train)=0.9560 	f1(eval)=0.9586 	duration(epoch)=0:01:54.301691
@003: 	loss(train)=0.1185 	loss(eval)=0.1173 	f1(train)=0.9587 	f1(eval)=0.9600 	duration(epoch)=0:01:54.108135
@004: 	loss(train)=0.1167 	loss(eval)=0.1310 	f1(train)=0.9613 	f1(eval)=0.9598 	duration(epoch)=0:01:54.374300
@005: 	loss(train)=0.1116 	loss(eval)=0.1184 	f1(train)=0.9607 	f1(eval)=0.9608 	duration(epoch)=0:01:54.053898
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@005: 	loss(train)=0.1116 	loss(eval)=0.1184 	f1(train)=0.9607 	f1(eval)=0.9608 	duration(epoch)=0:01:54.053898

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4804	 fp:      196 	 tn:     4804	 fn:      196	 pre=0.9608	 rec=0.9608	 f1=0.9608	 acc=0.9608
negative      	 tp:     2345	 fp:       78 	 tn:     2459	 fn:      118	 pre=0.9678	 rec=0.9521	 f1=0.9599	 acc=0.9608
positive      	 tp:     2459	 fp:      118 	 tn:     2345	 fn:       78	 pre=0.9542	 rec=0.9693	 f1=0.9617	 acc=0.9608
