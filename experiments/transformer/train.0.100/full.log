> Loaded logger: ./experiments/transformer/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0493 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0596 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0571 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.7272 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.0.100.csv ---]
@001: 	loss(train)=0.6305 	loss(eval)=0.5909 	f1(train)=0.6320 	f1(eval)=0.6688 	duration(epoch)=0:01:53.615018
@002: 	loss(train)=0.5767 	loss(eval)=0.6026 	f1(train)=0.6903 	f1(eval)=0.6654 	duration(epoch)=0:01:53.989640
@003: 	loss(train)=0.5392 	loss(eval)=0.5468 	f1(train)=0.7175 	f1(eval)=0.7126 	duration(epoch)=0:01:53.938302
@004: 	loss(train)=0.5273 	loss(eval)=0.5432 	f1(train)=0.7370 	f1(eval)=0.7140 	duration(epoch)=0:01:54.102235
@005: 	loss(train)=0.5009 	loss(eval)=0.5419 	f1(train)=0.7452 	f1(eval)=0.7196 	duration(epoch)=0:01:54.140930
@006: 	loss(train)=0.4853 	loss(eval)=0.5455 	f1(train)=0.7572 	f1(eval)=0.7160 	duration(epoch)=0:01:54.042997
@007: 	loss(train)=0.4598 	loss(eval)=0.5540 	f1(train)=0.7778 	f1(eval)=0.7116 	duration(epoch)=0:01:54.226212
@008: 	loss(train)=0.4460 	loss(eval)=0.5524 	f1(train)=0.7853 	f1(eval)=0.7210 	duration(epoch)=0:01:54.129826
@009: 	loss(train)=0.4284 	loss(eval)=0.5672 	f1(train)=0.7995 	f1(eval)=0.7162 	duration(epoch)=0:01:54.150983
@010: 	loss(train)=0.4064 	loss(eval)=0.5457 	f1(train)=0.8137 	f1(eval)=0.7270 	duration(epoch)=0:01:54.197624
@011: 	loss(train)=0.3885 	loss(eval)=0.5551 	f1(train)=0.8287 	f1(eval)=0.7274 	duration(epoch)=0:01:54.011897
@012: 	loss(train)=0.3650 	loss(eval)=0.5749 	f1(train)=0.8350 	f1(eval)=0.7146 	duration(epoch)=0:01:53.879761
@013: 	loss(train)=0.3496 	loss(eval)=0.5811 	f1(train)=0.8435 	f1(eval)=0.7308 	duration(epoch)=0:01:54.077167
@014: 	loss(train)=0.3423 	loss(eval)=0.5912 	f1(train)=0.8472 	f1(eval)=0.7188 	duration(epoch)=0:01:54.054477
@015: 	loss(train)=0.3165 	loss(eval)=0.6000 	f1(train)=0.8622 	f1(eval)=0.7176 	duration(epoch)=0:01:53.883179
@016: 	loss(train)=0.3114 	loss(eval)=0.6307 	f1(train)=0.8638 	f1(eval)=0.7028 	duration(epoch)=0:01:53.948815
@017: 	loss(train)=0.2901 	loss(eval)=0.6289 	f1(train)=0.8695 	f1(eval)=0.7080 	duration(epoch)=0:01:53.957585
@018: 	loss(train)=0.2755 	loss(eval)=0.6485 	f1(train)=0.8825 	f1(eval)=0.7096 	duration(epoch)=0:01:53.820525
@019: 	loss(train)=0.2716 	loss(eval)=0.6298 	f1(train)=0.8793 	f1(eval)=0.7202 	duration(epoch)=0:01:53.895352
@020: 	loss(train)=0.2467 	loss(eval)=0.6846 	f1(train)=0.9015 	f1(eval)=0.7104 	duration(epoch)=0:01:53.903607
@021: 	loss(train)=0.2451 	loss(eval)=0.6657 	f1(train)=0.8935 	f1(eval)=0.7176 	duration(epoch)=0:01:53.912440
@022: 	loss(train)=0.2334 	loss(eval)=0.6834 	f1(train)=0.9020 	f1(eval)=0.7178 	duration(epoch)=0:01:54.039899
@023: 	loss(train)=0.2406 	loss(eval)=0.7101 	f1(train)=0.8988 	f1(eval)=0.7088 	duration(epoch)=0:01:53.836164
@024: 	loss(train)=0.2226 	loss(eval)=0.7021 	f1(train)=0.9070 	f1(eval)=0.7056 	duration(epoch)=0:01:53.877577
@025: 	loss(train)=0.2071 	loss(eval)=0.7081 	f1(train)=0.9150 	f1(eval)=0.7242 	duration(epoch)=0:01:53.873272
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@013: 	loss(train)=0.3496 	loss(eval)=0.5811 	f1(train)=0.8435 	f1(eval)=0.7308 	duration(epoch)=0:01:54.077167

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3654	 fp:     1346 	 tn:     3654	 fn:     1346	 pre=0.7308	 rec=0.7308	 f1=0.7308	 acc=0.7308
negative      	 tp:     1795	 fp:      678 	 tn:     1859	 fn:      668	 pre=0.7258	 rec=0.7288	 f1=0.7273	 acc=0.7308
positive      	 tp:     1859	 fp:      668 	 tn:     1795	 fn:      678	 pre=0.7357	 rec=0.7328	 f1=0.7342	 acc=0.7308
