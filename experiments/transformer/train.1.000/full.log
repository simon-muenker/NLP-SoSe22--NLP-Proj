> Loaded logger: ./experiments/transformer/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4888 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0612 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0606 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.4949 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.1266 	loss(eval)=0.1129 	f1(train)=0.9579 	f1(eval)=0.9596 	duration(epoch)=0:09:30.360622
@002: 	loss(train)=0.1217 	loss(eval)=0.1229 	f1(train)=0.9595 	f1(eval)=0.9584 	duration(epoch)=0:09:30.850570
@003: 	loss(train)=0.1201 	loss(eval)=0.1155 	f1(train)=0.9597 	f1(eval)=0.9588 	duration(epoch)=0:09:30.795623
@004: 	loss(train)=0.1196 	loss(eval)=0.1147 	f1(train)=0.9595 	f1(eval)=0.9588 	duration(epoch)=0:09:30.574251
@005: 	loss(train)=0.1182 	loss(eval)=0.1155 	f1(train)=0.9605 	f1(eval)=0.9596 	duration(epoch)=0:09:31.061705
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@005: 	loss(train)=0.1182 	loss(eval)=0.1155 	f1(train)=0.9605 	f1(eval)=0.9596 	duration(epoch)=0:09:31.061705

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4798	 fp:      202 	 tn:     4798	 fn:      202	 pre=0.9596	 rec=0.9596	 f1=0.9596	 acc=0.9596
negative      	 tp:     2364	 fp:      103 	 tn:     2434	 fn:       99	 pre=0.9582	 rec=0.9598	 f1=0.9590	 acc=0.9596
positive      	 tp:     2434	 fp:       99 	 tn:     2364	 fn:      103	 pre=0.9609	 rec=0.9594	 f1=0.9602	 acc=0.9596
