> Loaded logger: ./experiments/transformer/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4531 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0576 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0581 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.9361 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.3632 	loss(eval)=0.3141 	f1(train)=0.8428 	f1(eval)=0.8650 	duration(epoch)=0:09:36.078083
@002: 	loss(train)=0.3211 	loss(eval)=0.3004 	f1(train)=0.8635 	f1(eval)=0.8746 	duration(epoch)=0:09:37.345183
@003: 	loss(train)=0.3101 	loss(eval)=0.2942 	f1(train)=0.8672 	f1(eval)=0.8814 	duration(epoch)=0:09:37.792992
@004: 	loss(train)=0.3029 	loss(eval)=0.2881 	f1(train)=0.8717 	f1(eval)=0.8806 	duration(epoch)=0:09:38.258272
@005: 	loss(train)=0.2964 	loss(eval)=0.2853 	f1(train)=0.8753 	f1(eval)=0.8802 	duration(epoch)=0:09:37.578154
@006: 	loss(train)=0.2907 	loss(eval)=0.2839 	f1(train)=0.8758 	f1(eval)=0.8792 	duration(epoch)=0:09:37.665313
@007: 	loss(train)=0.2861 	loss(eval)=0.2738 	f1(train)=0.8789 	f1(eval)=0.8870 	duration(epoch)=0:09:36.394281
@008: 	loss(train)=0.2808 	loss(eval)=0.2701 	f1(train)=0.8815 	f1(eval)=0.8870 	duration(epoch)=0:09:37.565392
@009: 	loss(train)=0.2762 	loss(eval)=0.2634 	f1(train)=0.8856 	f1(eval)=0.8934 	duration(epoch)=0:09:37.682006
@010: 	loss(train)=0.2710 	loss(eval)=0.2634 	f1(train)=0.8862 	f1(eval)=0.8958 	duration(epoch)=0:09:37.375028
@011: 	loss(train)=0.2675 	loss(eval)=0.2599 	f1(train)=0.8877 	f1(eval)=0.8976 	duration(epoch)=0:09:37.575731
@012: 	loss(train)=0.2631 	loss(eval)=0.2564 	f1(train)=0.8905 	f1(eval)=0.8966 	duration(epoch)=0:09:37.482718
@013: 	loss(train)=0.2595 	loss(eval)=0.2507 	f1(train)=0.8923 	f1(eval)=0.8990 	duration(epoch)=0:09:37.591661
@014: 	loss(train)=0.2540 	loss(eval)=0.2481 	f1(train)=0.8951 	f1(eval)=0.8974 	duration(epoch)=0:09:37.391948
@015: 	loss(train)=0.2488 	loss(eval)=0.2440 	f1(train)=0.8976 	f1(eval)=0.8998 	duration(epoch)=0:09:37.655128
@016: 	loss(train)=0.2441 	loss(eval)=0.2405 	f1(train)=0.9002 	f1(eval)=0.9000 	duration(epoch)=0:09:37.570958
@017: 	loss(train)=0.2393 	loss(eval)=0.2461 	f1(train)=0.9011 	f1(eval)=0.8992 	duration(epoch)=0:09:37.811916
@018: 	loss(train)=0.2377 	loss(eval)=0.2354 	f1(train)=0.9030 	f1(eval)=0.9046 	duration(epoch)=0:09:38.121727
@019: 	loss(train)=0.2309 	loss(eval)=0.2233 	f1(train)=0.9051 	f1(eval)=0.9098 	duration(epoch)=0:09:38.038594
@020: 	loss(train)=0.2259 	loss(eval)=0.2381 	f1(train)=0.9080 	f1(eval)=0.9050 	duration(epoch)=0:09:37.818487
@021: 	loss(train)=0.2216 	loss(eval)=0.2277 	f1(train)=0.9093 	f1(eval)=0.9092 	duration(epoch)=0:09:37.976652
@022: 	loss(train)=0.2175 	loss(eval)=0.2269 	f1(train)=0.9118 	f1(eval)=0.9064 	duration(epoch)=0:09:37.628719
@023: 	loss(train)=0.2106 	loss(eval)=0.2143 	f1(train)=0.9154 	f1(eval)=0.9144 	duration(epoch)=0:09:37.569209
@024: 	loss(train)=0.2082 	loss(eval)=0.2221 	f1(train)=0.9156 	f1(eval)=0.9106 	duration(epoch)=0:09:37.462964
@025: 	loss(train)=0.2027 	loss(eval)=0.2024 	f1(train)=0.9183 	f1(eval)=0.9208 	duration(epoch)=0:09:37.666471
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@025: 	loss(train)=0.2027 	loss(eval)=0.2024 	f1(train)=0.9183 	f1(eval)=0.9208 	duration(epoch)=0:09:37.666471

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4604	 fp:      396 	 tn:     4604	 fn:      396	 pre=0.9208	 rec=0.9208	 f1=0.9208	 acc=0.9208
negative      	 tp:     2291	 fp:      210 	 tn:     2313	 fn:      186	 pre=0.9160	 rec=0.9249	 f1=0.9204	 acc=0.9208
positive      	 tp:     2313	 fp:      186 	 tn:     2291	 fn:      210	 pre=0.9256	 rec=0.9168	 f1=0.9211	 acc=0.9208
