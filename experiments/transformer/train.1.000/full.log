> Loaded logger: ./experiments/transformer/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4832 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0590 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0594 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 8.8520 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.1268 	loss(eval)=0.1138 	f1(train)=0.9573 	f1(eval)=0.9588 	duration(epoch)=0:09:32.248906
@002: 	loss(train)=0.1228 	loss(eval)=0.1184 	f1(train)=0.9591 	f1(eval)=0.9580 	duration(epoch)=0:09:32.544058
@003: 	loss(train)=0.1207 	loss(eval)=0.1152 	f1(train)=0.9595 	f1(eval)=0.9592 	duration(epoch)=0:09:33.521144
@004: 	loss(train)=0.1208 	loss(eval)=0.1155 	f1(train)=0.9591 	f1(eval)=0.9592 	duration(epoch)=0:09:33.530964
@005: 	loss(train)=0.1190 	loss(eval)=0.1154 	f1(train)=0.9599 	f1(eval)=0.9592 	duration(epoch)=0:09:32.735362
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@005: 	loss(train)=0.1190 	loss(eval)=0.1154 	f1(train)=0.9599 	f1(eval)=0.9592 	duration(epoch)=0:09:32.735362

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     4796	 fp:      204 	 tn:     4796	 fn:      204	 pre=0.9592	 rec=0.9592	 f1=0.9592	 acc=0.9592
negative      	 tp:     2355	 fp:       96 	 tn:     2441	 fn:      108	 pre=0.9608	 rec=0.9562	 f1=0.9585	 acc=0.9592
positive      	 tp:     2441	 fp:      108 	 tn:     2355	 fn:       96	 pre=0.9576	 rec=0.9622	 f1=0.9599	 acc=0.9592
