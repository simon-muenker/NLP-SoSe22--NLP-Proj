> Loaded logger: ./experiments/transformer/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4865 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0598 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0586 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 10.5754 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb.train.1.000.csv ---]
@001: 	loss(train)=0.5693 	loss(eval)=0.5378 	f1(train)=0.6962 	f1(eval)=0.7312 	duration(epoch)=0:09:31.570335
@002: 	loss(train)=0.5357 	loss(eval)=0.5113 	f1(train)=0.7233 	f1(eval)=0.7524 	duration(epoch)=0:09:31.419377
@003: 	loss(train)=0.5234 	loss(eval)=0.5166 	f1(train)=0.7311 	f1(eval)=0.7394 	duration(epoch)=0:09:30.972083
@004: 	loss(train)=0.5143 	loss(eval)=0.5100 	f1(train)=0.7380 	f1(eval)=0.7448 	duration(epoch)=0:09:30.521315
@005: 	loss(train)=0.5029 	loss(eval)=0.4974 	f1(train)=0.7462 	f1(eval)=0.7552 	duration(epoch)=0:09:30.730081
@006: 	loss(train)=0.4939 	loss(eval)=0.4981 	f1(train)=0.7510 	f1(eval)=0.7518 	duration(epoch)=0:09:30.721625
@007: 	loss(train)=0.4880 	loss(eval)=0.4905 	f1(train)=0.7563 	f1(eval)=0.7574 	duration(epoch)=0:09:31.225033
@008: 	loss(train)=0.4788 	loss(eval)=0.5020 	f1(train)=0.7643 	f1(eval)=0.7524 	duration(epoch)=0:09:31.127795
@009: 	loss(train)=0.4718 	loss(eval)=0.4940 	f1(train)=0.7671 	f1(eval)=0.7526 	duration(epoch)=0:09:30.348299
@010: 	loss(train)=0.4650 	loss(eval)=0.4944 	f1(train)=0.7717 	f1(eval)=0.7528 	duration(epoch)=0:09:30.259977
@011: 	loss(train)=0.4543 	loss(eval)=0.4999 	f1(train)=0.7803 	f1(eval)=0.7528 	duration(epoch)=0:09:30.388445
@012: 	loss(train)=0.4484 	loss(eval)=0.5077 	f1(train)=0.7820 	f1(eval)=0.7592 	duration(epoch)=0:09:30.559993
@013: 	loss(train)=0.4408 	loss(eval)=0.5095 	f1(train)=0.7863 	f1(eval)=0.7506 	duration(epoch)=0:09:31.240995
@014: 	loss(train)=0.4357 	loss(eval)=0.5110 	f1(train)=0.7874 	f1(eval)=0.7516 	duration(epoch)=0:09:30.899396
@015: 	loss(train)=0.4319 	loss(eval)=0.5064 	f1(train)=0.7939 	f1(eval)=0.7504 	duration(epoch)=0:09:31.016788
@016: 	loss(train)=0.4231 	loss(eval)=0.5114 	f1(train)=0.7974 	f1(eval)=0.7508 	duration(epoch)=0:09:30.598803
@017: 	loss(train)=0.4174 	loss(eval)=0.5182 	f1(train)=0.8001 	f1(eval)=0.7522 	duration(epoch)=0:09:31.233553
@018: 	loss(train)=0.4136 	loss(eval)=0.5214 	f1(train)=0.8040 	f1(eval)=0.7510 	duration(epoch)=0:09:31.418329
@019: 	loss(train)=0.4007 	loss(eval)=0.5181 	f1(train)=0.8118 	f1(eval)=0.7598 	duration(epoch)=0:09:30.361822
@020: 	loss(train)=0.4023 	loss(eval)=0.5196 	f1(train)=0.8099 	f1(eval)=0.7540 	duration(epoch)=0:09:31.235797
@021: 	loss(train)=0.3960 	loss(eval)=0.5316 	f1(train)=0.8134 	f1(eval)=0.7466 	duration(epoch)=0:09:31.177091
@022: 	loss(train)=0.3954 	loss(eval)=0.5390 	f1(train)=0.8163 	f1(eval)=0.7416 	duration(epoch)=0:09:31.493178
@023: 	loss(train)=0.3847 	loss(eval)=0.5368 	f1(train)=0.8209 	f1(eval)=0.7446 	duration(epoch)=0:09:31.193685
@024: 	loss(train)=0.3844 	loss(eval)=0.5296 	f1(train)=0.8190 	f1(eval)=0.7564 	duration(epoch)=0:09:30.925707
@025: 	loss(train)=0.3817 	loss(eval)=0.5322 	f1(train)=0.8223 	f1(eval)=0.7514 	duration(epoch)=0:09:30.983477
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@019: 	loss(train)=0.4007 	loss(eval)=0.5181 	f1(train)=0.8118 	f1(eval)=0.7598 	duration(epoch)=0:09:30.361822

[--- EVAL -> ./data/imdb.eval.csv ---]
AVG           	 tp:     3799	 fp:     1201 	 tn:     3799	 fn:     1201	 pre=0.7598	 rec=0.7598	 f1=0.7598	 acc=0.7598
negative      	 tp:     1893	 fp:      631 	 tn:     1906	 fn:      570	 pre=0.7500	 rec=0.7686	 f1=0.7592	 acc=0.7598
positive      	 tp:     1906	 fp:      570 	 tn:     1893	 fn:      631	 pre=0.7698	 rec=0.7513	 f1=0.7604	 acc=0.7598
