> Loaded logger: ./experiments/_debug/base/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0029 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__load) took: 0.0017 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.4552 sec
> f(df_encode) took: 0.5455 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> f(df_encode) took: 0.5321 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> Init BERT-Head (Base)
  Memory Usage: 2.2588 MB
  Trainable parameters: 592130
  Input Dimension: 768
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=0.5297 	loss(eval)=0.5059 	f1(train)=0.7000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.021332
@010: 	loss(train)=0.4220 	loss(eval)=0.2916 	f1(train)=0.8000 	f1(eval)=0.8750 	duration(epoch)=0:00:00.020933
@015: 	loss(train)=0.1889 	loss(eval)=0.1346 	f1(train)=0.9500 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020706
@020: 	loss(train)=0.0518 	loss(eval)=0.0602 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020845
@025: 	loss(train)=0.0329 	loss(eval)=0.0390 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020705
@030: 	loss(train)=0.0272 	loss(eval)=0.0117 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020474
@035: 	loss(train)=0.0084 	loss(eval)=0.0061 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020662
@040: 	loss(train)=0.0065 	loss(eval)=0.0042 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.021054
@045: 	loss(train)=0.0057 	loss(eval)=0.0043 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020328
@050: 	loss(train)=0.0032 	loss(eval)=0.0023 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020548
@055: 	loss(train)=0.0029 	loss(eval)=0.0019 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020416
@060: 	loss(train)=0.0018 	loss(eval)=0.0016 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020671
@065: 	loss(train)=0.0023 	loss(eval)=0.0016 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020557
@070: 	loss(train)=0.0024 	loss(eval)=0.0017 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020372
@075: 	loss(train)=0.0016 	loss(eval)=0.0014 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020436
@080: 	loss(train)=0.0012 	loss(eval)=0.0014 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020460
@085: 	loss(train)=0.0010 	loss(eval)=0.0010 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020722
@090: 	loss(train)=0.0015 	loss(eval)=0.0008 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020458
@095: 	loss(train)=0.0010 	loss(eval)=0.0008 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020441
@100: 	loss(train)=0.0013 	loss(eval)=0.0007 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020369
> Load best model based on evaluation loss.
> Init BERT-Head (Base)
  Memory Usage: 2.2588 MB
  Trainable parameters: 592130
  Input Dimension: 768
  Output Dimension: 2
@100: 	loss(train)=0.0013 	loss(eval)=0.0007 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.020369

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       27	 fp:        0 	 tn:       13	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       13	 fp:        0 	 tn:       27	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
