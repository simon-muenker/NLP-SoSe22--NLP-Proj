> Loaded logger: ./experiments/_debug/base/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0030 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__load) took: 0.0017 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.3812 sec
> Encode ./data/imdb._debug.csv
> f(df_encode) took: 0.5420 sec
> Encode ./data/imdb._debug.csv
> f(df_encode) took: 0.5101 sec
> Init BERT-Head (Base)
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6961 	loss(eval)=0.6262 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.040826
@002: 	loss(train)=0.6239 	loss(eval)=0.5832 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.018857
@003: 	loss(train)=0.5718 	loss(eval)=0.5755 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.018485
@004: 	loss(train)=0.6000 	loss(eval)=0.5530 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.017671
@005: 	loss(train)=0.6748 	loss(eval)=0.5226 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.018240
@006: 	loss(train)=0.5523 	loss(eval)=0.5067 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.017832
@007: 	loss(train)=0.5074 	loss(eval)=0.5411 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.017585
@008: 	loss(train)=0.4998 	loss(eval)=0.5201 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.018132
@009: 	loss(train)=0.5196 	loss(eval)=0.5012 	f1(train)=0.6750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.017758
@010: 	loss(train)=0.5361 	loss(eval)=0.4732 	f1(train)=0.7000 	f1(eval)=0.7000 	duration(epoch)=0:00:00.018095
@011: 	loss(train)=0.5200 	loss(eval)=0.5893 	f1(train)=0.7000 	f1(eval)=0.7000 	duration(epoch)=0:00:00.018017
@012: 	loss(train)=0.5791 	loss(eval)=0.4585 	f1(train)=0.7000 	f1(eval)=0.7500 	duration(epoch)=0:00:00.017925
@013: 	loss(train)=0.5343 	loss(eval)=0.5049 	f1(train)=0.7500 	f1(eval)=0.7500 	duration(epoch)=0:00:00.017957
@014: 	loss(train)=0.4964 	loss(eval)=0.4935 	f1(train)=0.7750 	f1(eval)=0.8250 	duration(epoch)=0:00:00.017721
@015: 	loss(train)=0.4610 	loss(eval)=0.4976 	f1(train)=0.8250 	f1(eval)=0.8750 	duration(epoch)=0:00:00.018068
@016: 	loss(train)=0.4522 	loss(eval)=0.4470 	f1(train)=0.8750 	f1(eval)=0.9000 	duration(epoch)=0:00:00.017943
@017: 	loss(train)=0.4864 	loss(eval)=0.4934 	f1(train)=0.9000 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017539
@018: 	loss(train)=0.4719 	loss(eval)=0.4603 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017989
@019: 	loss(train)=0.5006 	loss(eval)=0.4543 	f1(train)=0.8750 	f1(eval)=0.9000 	duration(epoch)=0:00:00.017417
@020: 	loss(train)=0.4457 	loss(eval)=0.4245 	f1(train)=0.9000 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017745
@021: 	loss(train)=0.3994 	loss(eval)=0.4843 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017685
@022: 	loss(train)=0.4151 	loss(eval)=0.4452 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017180
@023: 	loss(train)=0.4584 	loss(eval)=0.4152 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017726
@024: 	loss(train)=0.3980 	loss(eval)=0.3778 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.019216
@025: 	loss(train)=0.4058 	loss(eval)=0.4038 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:00.017784
> Load best model based on evaluation loss.
> Init BERT-Head (Base)
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2
@019: 	loss(train)=0.5006 	loss(eval)=0.4543 	f1(train)=0.8750 	f1(eval)=0.9000 	duration(epoch)=0:00:00.017417

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       36	 fp:        4 	 tn:       36	 fn:        4	 pre=0.9000	 rec=0.9000	 f1=0.9000	 acc=0.9000
negative      	 tp:       27	 fp:        4 	 tn:        9	 fn:        0	 pre=0.8710	 rec=1.0000	 f1=0.9310	 acc=0.9000
positive      	 tp:        9	 fp:        0 	 tn:       27	 fn:        4	 pre=1.0000	 rec=0.6923	 f1=0.8182	 acc=0.9000
