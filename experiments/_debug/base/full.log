> Loaded logger: ./experiments/_debug/base/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0028 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__load) took: 0.0017 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.3752 sec
> f(df_encode) took: 0.5494 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> f(df_encode) took: 0.5272 sec
> Memory Usage (w/ Embeds): 0.0904 MB
> Init BERT-Head (Base)
  Memory Usage: 0.0059 MB
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=0.7429 	loss(eval)=0.5449 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.017398
@010: 	loss(train)=0.5696 	loss(eval)=0.5232 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.017243
@015: 	loss(train)=0.5305 	loss(eval)=0.5335 	f1(train)=0.7000 	f1(eval)=0.7250 	duration(epoch)=0:00:00.016758
@020: 	loss(train)=0.5100 	loss(eval)=0.4712 	f1(train)=0.8500 	f1(eval)=0.8500 	duration(epoch)=0:00:00.017085
@025: 	loss(train)=0.4164 	loss(eval)=0.4345 	f1(train)=0.8750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.016954
@030: 	loss(train)=0.4539 	loss(eval)=0.5036 	f1(train)=0.8000 	f1(eval)=0.8500 	duration(epoch)=0:00:00.016788
@035: 	loss(train)=0.3621 	loss(eval)=0.3256 	f1(train)=0.9250 	f1(eval)=0.9000 	duration(epoch)=0:00:00.017179
@040: 	loss(train)=0.3591 	loss(eval)=0.2843 	f1(train)=0.8750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.016866
@045: 	loss(train)=0.3036 	loss(eval)=0.2869 	f1(train)=0.9500 	f1(eval)=0.9000 	duration(epoch)=0:00:00.016880
@050: 	loss(train)=0.3231 	loss(eval)=0.2733 	f1(train)=0.9750 	f1(eval)=0.9250 	duration(epoch)=0:00:00.016907
@055: 	loss(train)=0.2869 	loss(eval)=0.2575 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:00.016971
@060: 	loss(train)=0.2592 	loss(eval)=0.2224 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016741
@065: 	loss(train)=0.2096 	loss(eval)=0.2409 	f1(train)=0.9750 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016911
@070: 	loss(train)=0.1867 	loss(eval)=0.2116 	f1(train)=0.9250 	f1(eval)=0.9750 	duration(epoch)=0:00:00.016823
@075: 	loss(train)=0.2157 	loss(eval)=0.1840 	f1(train)=0.9500 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016650
@080: 	loss(train)=0.1979 	loss(eval)=0.1785 	f1(train)=0.9500 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016934
@085: 	loss(train)=0.2371 	loss(eval)=0.1757 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016733
@090: 	loss(train)=0.1992 	loss(eval)=0.1921 	f1(train)=0.9750 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016798
@095: 	loss(train)=0.1943 	loss(eval)=0.1563 	f1(train)=0.9750 	f1(eval)=1.0000 	duration(epoch)=0:00:00.017045
@100: 	loss(train)=0.1607 	loss(eval)=0.1375 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016928
> Load best model based on evaluation loss.
> Init BERT-Head (Base)
  Memory Usage: 0.0059 MB
  Trainable parameters: 1538
  Input Dimension: 768
  Output Dimension: 2
@100: 	loss(train)=0.1607 	loss(eval)=0.1375 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.016928

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       27	 fp:        0 	 tn:       13	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       13	 fp:        0 	 tn:       27	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
