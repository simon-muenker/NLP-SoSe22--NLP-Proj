> Loaded logger: ./experiments/_debug/transformer/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0023 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0015 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0013 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.6872 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6905 	loss(eval)=0.6445 	f1(train)=0.5250 	f1(eval)=0.6250 	duration(epoch)=0:00:01.048294
@002: 	loss(train)=0.6096 	loss(eval)=0.5870 	f1(train)=0.6500 	f1(eval)=0.6000 	duration(epoch)=0:00:01.009293
@003: 	loss(train)=0.6452 	loss(eval)=0.6336 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:00.976192
@004: 	loss(train)=0.5646 	loss(eval)=0.5976 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:01.008076
@005: 	loss(train)=0.6727 	loss(eval)=0.6673 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:00.957584
@006: 	loss(train)=0.6594 	loss(eval)=0.6505 	f1(train)=0.6250 	f1(eval)=0.6000 	duration(epoch)=0:00:00.954550
@007: 	loss(train)=0.5989 	loss(eval)=0.5287 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.971555
@008: 	loss(train)=0.5552 	loss(eval)=0.5426 	f1(train)=0.7000 	f1(eval)=0.7500 	duration(epoch)=0:00:01.033903
@009: 	loss(train)=0.5808 	loss(eval)=0.5203 	f1(train)=0.7750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.944202
@010: 	loss(train)=0.5269 	loss(eval)=0.5745 	f1(train)=0.7750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.979376
@011: 	loss(train)=0.5348 	loss(eval)=0.5041 	f1(train)=0.8250 	f1(eval)=0.8500 	duration(epoch)=0:00:01.017758
@012: 	loss(train)=0.5111 	loss(eval)=0.4804 	f1(train)=0.8750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.976283
@013: 	loss(train)=0.5211 	loss(eval)=0.4578 	f1(train)=0.8750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.979346
@014: 	loss(train)=0.4962 	loss(eval)=0.4640 	f1(train)=0.8250 	f1(eval)=0.8500 	duration(epoch)=0:00:00.992204
@015: 	loss(train)=0.4653 	loss(eval)=0.4736 	f1(train)=0.8750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.947463
@016: 	loss(train)=0.4399 	loss(eval)=0.4545 	f1(train)=0.9000 	f1(eval)=0.8750 	duration(epoch)=0:00:01.029098
@017: 	loss(train)=0.4405 	loss(eval)=0.4135 	f1(train)=0.8500 	f1(eval)=0.8750 	duration(epoch)=0:00:01.033982
@018: 	loss(train)=0.4737 	loss(eval)=0.3991 	f1(train)=0.8750 	f1(eval)=0.8750 	duration(epoch)=0:00:01.013986
@019: 	loss(train)=0.4403 	loss(eval)=0.4101 	f1(train)=0.8750 	f1(eval)=0.9000 	duration(epoch)=0:00:01.033156
@020: 	loss(train)=0.4062 	loss(eval)=0.4123 	f1(train)=0.9250 	f1(eval)=0.9250 	duration(epoch)=0:00:00.994667
@021: 	loss(train)=0.4055 	loss(eval)=0.3929 	f1(train)=0.9000 	f1(eval)=0.9500 	duration(epoch)=0:00:01.023862
@022: 	loss(train)=0.3613 	loss(eval)=0.3401 	f1(train)=0.9250 	f1(eval)=0.9250 	duration(epoch)=0:00:00.985887
@023: 	loss(train)=0.3618 	loss(eval)=0.3592 	f1(train)=0.9250 	f1(eval)=0.9250 	duration(epoch)=0:00:01.029272
@024: 	loss(train)=0.3170 	loss(eval)=0.3457 	f1(train)=0.9250 	f1(eval)=0.9250 	duration(epoch)=0:00:00.993170
@025: 	loss(train)=0.3690 	loss(eval)=0.3598 	f1(train)=0.9250 	f1(eval)=0.9500 	duration(epoch)=0:00:00.931879
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@025: 	loss(train)=0.3690 	loss(eval)=0.3598 	f1(train)=0.9250 	f1(eval)=0.9500 	duration(epoch)=0:00:00.931879

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       38	 fp:        2 	 tn:       38	 fn:        2	 pre=0.9500	 rec=0.9500	 f1=0.9500	 acc=0.9500
negative      	 tp:       24	 fp:        2 	 tn:       14	 fn:        0	 pre=0.9231	 rec=1.0000	 f1=0.9600	 acc=0.9500
positive      	 tp:       14	 fp:        0 	 tn:       24	 fn:        2	 pre=1.0000	 rec=0.8750	 f1=0.9333	 acc=0.9500
