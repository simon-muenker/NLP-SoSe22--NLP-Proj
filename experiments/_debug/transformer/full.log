> Loaded logger: ./experiments/_debug/transformer/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0032 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0017 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0016 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'fabriceyhc/bert-base-uncased-imdb'
> f(__init__) took: 23.2767 sec
> Init BERT-Head (MLP), trainable parameters: 197378

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6879 	loss(eval)=0.4685 	f1(train)=0.2500 	f1(eval)=0.9750 	duration(epoch)=0:00:01.075667
@002: 	loss(train)=0.3832 	loss(eval)=0.2604 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.021735
@003: 	loss(train)=0.2127 	loss(eval)=0.1461 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.016996
@004: 	loss(train)=0.1270 	loss(eval)=0.1034 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.003887
@005: 	loss(train)=0.1501 	loss(eval)=0.0668 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.013226
@006: 	loss(train)=0.0731 	loss(eval)=0.0533 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.015614
@007: 	loss(train)=0.0458 	loss(eval)=0.0420 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.023603
@008: 	loss(train)=0.0450 	loss(eval)=0.0391 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.025714
@009: 	loss(train)=0.0343 	loss(eval)=0.0360 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.007254
@010: 	loss(train)=0.0363 	loss(eval)=0.0317 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.018377
@011: 	loss(train)=0.0290 	loss(eval)=0.0953 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.027416
@012: 	loss(train)=0.0337 	loss(eval)=0.0288 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.009793
@013: 	loss(train)=0.0325 	loss(eval)=0.0297 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.026435
@014: 	loss(train)=0.0305 	loss(eval)=0.0237 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.015857
@015: 	loss(train)=0.0263 	loss(eval)=0.0216 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.009226
@016: 	loss(train)=0.0213 	loss(eval)=0.0198 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.012800
@017: 	loss(train)=0.0699 	loss(eval)=0.0620 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.017421
@018: 	loss(train)=0.0222 	loss(eval)=0.0183 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.028497
@019: 	loss(train)=0.0183 	loss(eval)=0.0164 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.023817
@020: 	loss(train)=0.0153 	loss(eval)=0.0442 	f1(train)=0.9750 	f1(eval)=1.0000 	duration(epoch)=0:00:01.018260
@021: 	loss(train)=0.0149 	loss(eval)=0.0130 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.008273
@022: 	loss(train)=0.0334 	loss(eval)=0.0144 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.017406
@023: 	loss(train)=0.0134 	loss(eval)=0.0342 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.008492
@024: 	loss(train)=0.0119 	loss(eval)=0.0122 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.019352
@025: 	loss(train)=0.0134 	loss(eval)=0.0103 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.019456
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 197378
@025: 	loss(train)=0.0134 	loss(eval)=0.0103 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.019456

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       27	 fp:        0 	 tn:       13	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       13	 fp:        0 	 tn:       27	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
