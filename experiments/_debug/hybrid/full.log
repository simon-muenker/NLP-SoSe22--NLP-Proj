> Loaded logger: ./experiments/_debug/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0025 sec
> f(__tokenize) took: 0.3633 sec
> f(__ngram) took: 0.2924 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0047 sec
> f(__tokenize) took: 0.2954 sec
> f(__ngram) took: 0.2908 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0041 sec
> f(__tokenize) took: 0.3106 sec
> f(__ngram) took: 0.3011 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 7.7186 sec
> Init Freq. Classifier, n-grams: ['1', '2']
> Init Spacy Pipeline: 'en_core_web_sm', with: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'spacytextblob']
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb._debug.csv
> f(fit) took: 0.0163 sec
> Predict with Freq. Classifier on ./data/imdb._debug.csv
> f(predict) took: 5.3564 sec
> Apply Space Pipeline to: ./data/imdb._debug.csv
> f(apply) took: 1.7727 sec
> Predict with Freq. Classifier on ./data/imdb._debug.csv
> f(predict) took: 5.5945 sec
> Apply Space Pipeline to: ./data/imdb._debug.csv
> f(apply) took: 1.3689 sec

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6657 	loss(eval)=0.6384 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:01.091967
@002: 	loss(train)=0.6383 	loss(eval)=0.6107 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:01.074381
@003: 	loss(train)=0.5965 	loss(eval)=0.5848 	f1(train)=0.6000 	f1(eval)=0.6000 	duration(epoch)=0:00:01.052932
@004: 	loss(train)=0.5808 	loss(eval)=0.5506 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:01.026181
@005: 	loss(train)=0.5445 	loss(eval)=0.5136 	f1(train)=0.7250 	f1(eval)=0.9000 	duration(epoch)=0:00:01.005236
@006: 	loss(train)=0.5048 	loss(eval)=0.4742 	f1(train)=0.7750 	f1(eval)=0.8750 	duration(epoch)=0:00:01.025894
@007: 	loss(train)=0.4687 	loss(eval)=0.4414 	f1(train)=0.8500 	f1(eval)=0.8250 	duration(epoch)=0:00:01.053494
@008: 	loss(train)=0.4229 	loss(eval)=0.3980 	f1(train)=0.9000 	f1(eval)=0.9000 	duration(epoch)=0:00:01.048827
@009: 	loss(train)=0.3778 	loss(eval)=0.3494 	f1(train)=0.9500 	f1(eval)=0.9500 	duration(epoch)=0:00:00.991754
@010: 	loss(train)=0.3352 	loss(eval)=0.3109 	f1(train)=0.9000 	f1(eval)=0.9500 	duration(epoch)=0:00:00.989663
@011: 	loss(train)=0.2988 	loss(eval)=0.2727 	f1(train)=0.9250 	f1(eval)=0.9500 	duration(epoch)=0:00:01.011163
@012: 	loss(train)=0.2626 	loss(eval)=0.2379 	f1(train)=0.9500 	f1(eval)=0.9750 	duration(epoch)=0:00:01.039504
@013: 	loss(train)=0.2269 	loss(eval)=0.2073 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.033084
@014: 	loss(train)=0.2072 	loss(eval)=0.1822 	f1(train)=0.9500 	f1(eval)=0.9750 	duration(epoch)=0:00:01.012669
@015: 	loss(train)=0.2093 	loss(eval)=0.1642 	f1(train)=0.9500 	f1(eval)=0.9750 	duration(epoch)=0:00:01.015136
@016: 	loss(train)=0.1505 	loss(eval)=0.1429 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.021511
@017: 	loss(train)=0.1553 	loss(eval)=0.1300 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.005327
@018: 	loss(train)=0.1378 	loss(eval)=0.1211 	f1(train)=0.9750 	f1(eval)=0.9750 	duration(epoch)=0:00:01.056330
@019: 	loss(train)=0.1048 	loss(eval)=0.0940 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.032514
@020: 	loss(train)=0.1017 	loss(eval)=0.0853 	f1(train)=0.9750 	f1(eval)=1.0000 	duration(epoch)=0:00:01.003532
@021: 	loss(train)=0.0864 	loss(eval)=0.0787 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.063348
@022: 	loss(train)=0.0682 	loss(eval)=0.0690 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.037641
@023: 	loss(train)=0.0729 	loss(eval)=0.0552 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.041901
@024: 	loss(train)=0.0558 	loss(eval)=0.0509 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.106586
@025: 	loss(train)=0.0494 	loss(eval)=0.0458 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.064110
> Load best model based on evaluation loss.
> Init BERT-Head (MLP), trainable parameters: 200205
> Init Neural Assemble (MLP), trainable parameters: 28
@025: 	loss(train)=0.0494 	loss(eval)=0.0458 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:01.064110

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       24	 fp:        0 	 tn:       16	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       16	 fp:        0 	 tn:       24	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
