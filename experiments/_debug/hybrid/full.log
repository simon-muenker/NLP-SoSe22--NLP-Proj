> Loaded logger: ./experiments/_debug/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0036 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.3459 sec
> f(__ngram) took: 0.2867 sec
> f(__ngram) took: 0.2832 sec
> f(__load) took: 0.0043 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.3260 sec
> f(__ngram) took: 0.2950 sec
> f(__ngram) took: 0.2799 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.3641 sec

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 1024), ('2', 128)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0165 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 7.0595 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 6.9654 sec
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1694
  Input Dimension: 846
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6969 	loss(eval)=0.8063 	f1(train)=0.4500 	f1(eval)=0.6750 	duration(epoch)=0:00:01.109106
@002: 	loss(train)=0.8283 	loss(eval)=0.5809 	f1(train)=0.7250 	f1(eval)=0.6750 	duration(epoch)=0:00:01.065848
@003: 	loss(train)=0.6126 	loss(eval)=0.6619 	f1(train)=0.7000 	f1(eval)=0.6750 	duration(epoch)=0:00:01.094662
@004: 	loss(train)=0.6425 	loss(eval)=0.5851 	f1(train)=0.7500 	f1(eval)=0.6750 	duration(epoch)=0:00:01.087748
@005: 	loss(train)=0.6807 	loss(eval)=0.7278 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:01.085655
@006: 	loss(train)=0.8112 	loss(eval)=0.6322 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:01.103575
@007: 	loss(train)=0.5205 	loss(eval)=0.5371 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:01.072104
@008: 	loss(train)=0.5146 	loss(eval)=0.5169 	f1(train)=0.7250 	f1(eval)=0.6750 	duration(epoch)=0:00:01.084886
@009: 	loss(train)=0.4732 	loss(eval)=0.4913 	f1(train)=0.7000 	f1(eval)=0.6750 	duration(epoch)=0:00:01.070450
@010: 	loss(train)=0.4422 	loss(eval)=0.4847 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:01.065398
@011: 	loss(train)=0.5609 	loss(eval)=0.5707 	f1(train)=0.7250 	f1(eval)=0.6750 	duration(epoch)=0:00:01.064616
@012: 	loss(train)=0.7825 	loss(eval)=0.3386 	f1(train)=0.7000 	f1(eval)=0.7000 	duration(epoch)=0:00:01.098846
@013: 	loss(train)=0.4638 	loss(eval)=0.3271 	f1(train)=0.7500 	f1(eval)=0.7000 	duration(epoch)=0:00:01.084745
@014: 	loss(train)=0.3307 	loss(eval)=0.3702 	f1(train)=0.7250 	f1(eval)=0.7250 	duration(epoch)=0:00:01.088641
@015: 	loss(train)=0.3755 	loss(eval)=0.3077 	f1(train)=0.7250 	f1(eval)=0.7250 	duration(epoch)=0:00:01.094293
@016: 	loss(train)=0.3967 	loss(eval)=0.2993 	f1(train)=0.6500 	f1(eval)=0.7250 	duration(epoch)=0:00:01.080439
@017: 	loss(train)=0.3780 	loss(eval)=0.4004 	f1(train)=0.7500 	f1(eval)=0.7250 	duration(epoch)=0:00:01.081155
@018: 	loss(train)=0.4086 	loss(eval)=0.2387 	f1(train)=0.7500 	f1(eval)=0.7250 	duration(epoch)=0:00:01.076702
@019: 	loss(train)=0.3897 	loss(eval)=0.4411 	f1(train)=0.7500 	f1(eval)=0.8000 	duration(epoch)=0:00:01.077229
@020: 	loss(train)=0.3689 	loss(eval)=0.4435 	f1(train)=0.7750 	f1(eval)=0.8250 	duration(epoch)=0:00:01.102082
@021: 	loss(train)=0.4426 	loss(eval)=0.3812 	f1(train)=0.8250 	f1(eval)=0.8250 	duration(epoch)=0:00:01.081929
@022: 	loss(train)=0.4516 	loss(eval)=0.4076 	f1(train)=0.8250 	f1(eval)=0.7750 	duration(epoch)=0:00:01.097331
@023: 	loss(train)=0.3476 	loss(eval)=0.3544 	f1(train)=0.7500 	f1(eval)=0.7750 	duration(epoch)=0:00:01.099369
@024: 	loss(train)=0.2161 	loss(eval)=0.3611 	f1(train)=0.8250 	f1(eval)=0.7250 	duration(epoch)=0:00:01.088186
@025: 	loss(train)=0.4028 	loss(eval)=1.5697 	f1(train)=0.7500 	f1(eval)=0.7250 	duration(epoch)=0:00:01.076671
> Load best model based on evaluation loss.
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1694
  Input Dimension: 846
  Output Dimension: 2
@021: 	loss(train)=0.4426 	loss(eval)=0.3812 	f1(train)=0.8250 	f1(eval)=0.8250 	duration(epoch)=0:00:01.081929

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       33	 fp:        7 	 tn:       33	 fn:        7	 pre=0.8250	 rec=0.8250	 f1=0.8250	 acc=0.8250
negative      	 tp:       26	 fp:        6 	 tn:        7	 fn:        1	 pre=0.8125	 rec=0.9630	 f1=0.8814	 acc=0.8250
positive      	 tp:        7	 fp:        1 	 tn:       26	 fn:        6	 pre=0.8750	 rec=0.5385	 f1=0.6667	 acc=0.8250
