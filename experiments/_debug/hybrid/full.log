> Loaded logger: ./experiments/_debug/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0043 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2748 sec
> f(__ngram) took: 0.2131 sec
> f(__ngram) took: 0.2118 sec
> f(__load) took: 0.0033 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2317 sec
> f(__ngram) took: 0.2281 sec
> f(__ngram) took: 0.3127 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
  Memory Usage: 417.6494 MB
> f(__init__) took: 9.3526 sec
> f(df_encode) took: 0.5443 sec
> Memory Usage (w/ Embeds): 0.2730 MB
> f(df_encode) took: 0.5102 sec
> Memory Usage (w/ Embeds): 0.2730 MB

[--- FEATURE PIPELINE ---]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0000 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.0589 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.0163 sec
> f(df_encode) took: 0.9758 sec
> f(match) took: 0.0294 sec
> f(match) took: 0.0285 sec
> Init Base+Features+MetaMatcher Concatenation (Hybrid)
  Memory Usage: 2.2647 MB
  Trainable parameters: 593670
  Input Dimension: 769
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=0.5174 	loss(eval)=0.4744 	f1(train)=0.6750 	f1(eval)=0.8000 	duration(epoch)=0:00:00.046354
@010: 	loss(train)=0.2522 	loss(eval)=0.1880 	f1(train)=0.9000 	f1(eval)=0.9500 	duration(epoch)=0:00:00.043491
@015: 	loss(train)=0.1417 	loss(eval)=0.0990 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042243
@020: 	loss(train)=0.0463 	loss(eval)=0.0444 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042192
@025: 	loss(train)=0.0349 	loss(eval)=0.0138 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.043107
@030: 	loss(train)=0.0139 	loss(eval)=0.0122 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042337
@035: 	loss(train)=0.0114 	loss(eval)=0.0067 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042459
@040: 	loss(train)=0.0063 	loss(eval)=0.0044 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042378
@045: 	loss(train)=0.0041 	loss(eval)=0.0041 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042317
@050: 	loss(train)=0.0040 	loss(eval)=0.0036 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042252
@055: 	loss(train)=0.0038 	loss(eval)=0.0021 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.043014
@060: 	loss(train)=0.0029 	loss(eval)=0.0021 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042643
@065: 	loss(train)=0.0026 	loss(eval)=0.0019 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042402
@070: 	loss(train)=0.0017 	loss(eval)=0.0018 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.043465
@075: 	loss(train)=0.0025 	loss(eval)=0.0014 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042207
@080: 	loss(train)=0.0013 	loss(eval)=0.0012 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042597
@085: 	loss(train)=0.0012 	loss(eval)=0.0011 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.043014
@090: 	loss(train)=0.0011 	loss(eval)=0.0009 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042109
@095: 	loss(train)=0.0016 	loss(eval)=0.0007 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.042366
@100: 	loss(train)=0.0016 	loss(eval)=0.0008 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.045260
> Load best model based on evaluation loss.
> Init Base+Features+MetaMatcher Concatenation (Hybrid)
  Memory Usage: 2.2647 MB
  Trainable parameters: 593670
  Input Dimension: 769
  Output Dimension: 2
@100: 	loss(train)=0.0016 	loss(eval)=0.0008 	f1(train)=1.0000 	f1(eval)=1.0000 	duration(epoch)=0:00:00.045260

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       27	 fp:        0 	 tn:       13	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       13	 fp:        0 	 tn:       27	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
