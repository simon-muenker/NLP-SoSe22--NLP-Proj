> Loaded logger: ./experiments/_debug/hybrid/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0043 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2634 sec
> f(__ngram) took: 0.2027 sec
> f(__ngram) took: 0.1984 sec
> f(__load) took: 0.0038 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2184 sec
> f(__ngram) took: 0.2015 sec
> f(__ngram) took: 0.2887 sec

[--- LOAD COMPONENTS ---]
> Init Encoder: 'bert-base-uncased'
> f(__init__) took: 9.4003 sec
> Encode ./data/imdb._debug.csv
> f(df_encode) took: 0.5308 sec
> Encode ./data/imdb._debug.csv
> f(df_encode) took: 0.5143 sec

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 1024), ('2', 128)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0147 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 4.4201 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 4.2141 sec
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1694
  Input Dimension: 846
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=0.6880 	loss(eval)=0.6558 	f1(train)=0.4500 	f1(eval)=0.7250 	duration(epoch)=0:00:00.074693
@002: 	loss(train)=0.7153 	loss(eval)=0.6452 	f1(train)=0.7250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066685
@003: 	loss(train)=0.6651 	loss(eval)=0.6694 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.067553
@004: 	loss(train)=0.6397 	loss(eval)=0.6050 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.067177
@005: 	loss(train)=0.5824 	loss(eval)=0.5783 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066860
@006: 	loss(train)=0.5746 	loss(eval)=0.5860 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066752
@007: 	loss(train)=0.5389 	loss(eval)=0.7532 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.155619
@008: 	loss(train)=0.5069 	loss(eval)=0.4514 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.067918
@009: 	loss(train)=0.5655 	loss(eval)=0.5338 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065049
@010: 	loss(train)=0.5016 	loss(eval)=0.6309 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066741
@011: 	loss(train)=0.7578 	loss(eval)=0.5366 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065418
@012: 	loss(train)=0.5120 	loss(eval)=0.4785 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065321
@013: 	loss(train)=0.5298 	loss(eval)=0.5411 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.070198
@014: 	loss(train)=0.5378 	loss(eval)=0.5629 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065613
@015: 	loss(train)=0.5220 	loss(eval)=0.5730 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065520
@016: 	loss(train)=0.5120 	loss(eval)=0.5226 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064759
@017: 	loss(train)=0.5251 	loss(eval)=0.4983 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.068306
@018: 	loss(train)=0.4960 	loss(eval)=0.4707 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064096
@019: 	loss(train)=0.4962 	loss(eval)=0.4805 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065775
@020: 	loss(train)=0.6450 	loss(eval)=0.4764 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064819
@021: 	loss(train)=0.4541 	loss(eval)=0.4941 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065464
@022: 	loss(train)=0.5075 	loss(eval)=0.5612 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065007
@023: 	loss(train)=0.5566 	loss(eval)=0.6082 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065389
@024: 	loss(train)=0.4280 	loss(eval)=0.4027 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064576
@025: 	loss(train)=0.4737 	loss(eval)=0.5387 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064866
> Load best model based on evaluation loss.
> Init Base+Features Concatenation (Hybrid)
  Trainable parameters: 1694
  Input Dimension: 846
  Output Dimension: 2
@001: 	loss(train)=0.6880 	loss(eval)=0.6558 	f1(train)=0.4500 	f1(eval)=0.7250 	duration(epoch)=0:00:00.074693

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       29	 fp:       11 	 tn:       29	 fn:       11	 pre=0.7250	 rec=0.7250	 f1=0.7250	 acc=0.7250
negative      	 tp:       26	 fp:       10 	 tn:        3	 fn:        1	 pre=0.7222	 rec=0.9630	 f1=0.8254	 acc=0.7250
positive      	 tp:        3	 fp:        1 	 tn:       26	 fn:       10	 pre=0.7500	 rec=0.2308	 f1=0.3529	 acc=0.7250
