> Loaded logger: ./experiments/_debug/linguistic/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0082 sec
> f(__tokenize) took: 0.3734 sec
> f(__ngram) took: 0.2438 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0036 sec
> f(__tokenize) took: 0.2446 sec
> f(__ngram) took: 0.2419 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0039 sec
> f(__tokenize) took: 0.2658 sec
> f(__ngram) took: 0.2421 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0670 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 0.5948 sec
AVG           	 tp:      361	 fp:       39 	 tn:      361	 fn:       39	 pre=0.9025	 rec=0.9025	 f1=0.9025	 acc=0.9025
negative      	 tp:      176	 fp:       22 	 tn:      185	 fn:       17	 pre=0.8889	 rec=0.9119	 f1=0.9003	 acc=0.9025
positive      	 tp:      185	 fp:       17 	 tn:      176	 fn:       22	 pre=0.9158	 rec=0.8937	 f1=0.9046	 acc=0.9025
> Predict with Freq. Classifier on ./data/imdb._debug.csv
> f(predict) took: 0.0628 sec
AVG           	 tp:       28	 fp:       12 	 tn:       28	 fn:       12	 pre=0.7000	 rec=0.7000	 f1=0.7000	 acc=0.7000
negative      	 tp:       21	 fp:        9 	 tn:        7	 fn:        3	 pre=0.7000	 rec=0.8750	 f1=0.7778	 acc=0.7000
positive      	 tp:        7	 fp:        3 	 tn:       21	 fn:        9	 pre=0.7000	 rec=0.4375	 f1=0.5385	 acc=0.7000
