> Loaded logger: ./experiments/_debug/linguistic/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0024 sec
> f(__tokenize) took: 0.2943 sec
> f(__ngram) took: 0.1720 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0026 sec
> f(__tokenize) took: 0.2052 sec
> f(__ngram) took: 0.2060 sec
> Load/Init from ./data/imdb._debug.csv
> f(__load) took: 0.0041 sec
> f(__tokenize) took: 0.2367 sec
> f(__ngram) took: 0.2132 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb._debug.csv
> f(fit) took: 0.0215 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb._debug.csv
> f(predict) took: 2.0123 sec
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       24	 fp:        0 	 tn:       16	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       16	 fp:        0 	 tn:       24	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
> Predict with Freq. Classifier on ./data/imdb._debug.csv
> f(predict) took: 1.7130 sec
AVG           	 tp:       40	 fp:        0 	 tn:       40	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
negative      	 tp:       24	 fp:        0 	 tn:       16	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
positive      	 tp:       16	 fp:        0 	 tn:       24	 fn:        0	 pre=1.0000	 rec=1.0000	 f1=1.0000	 acc=1.0000
