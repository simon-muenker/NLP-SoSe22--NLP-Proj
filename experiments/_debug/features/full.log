> Loaded logger: ./experiments/_debug/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0037 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2832 sec
> f(__ngram) took: 0.2041 sec
> f(__ngram) took: 0.2006 sec
> f(__load) took: 0.0044 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2032 sec
> f(__ngram) took: 0.2007 sec
> f(__ngram) took: 0.2921 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 256), ('2', 2048)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0220 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.8448 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.8378 sec
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0006 MB
  Trainable parameters: 158
  Input Dimension: 78
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=7.5622 	loss(eval)=7.6272 	f1(train)=0.4000 	f1(eval)=0.3250 	duration(epoch)=0:00:00.061906
@010: 	loss(train)=2.9900 	loss(eval)=5.3826 	f1(train)=0.4750 	f1(eval)=0.3250 	duration(epoch)=0:00:00.060923
@015: 	loss(train)=4.0771 	loss(eval)=2.4154 	f1(train)=0.5000 	f1(eval)=0.3250 	duration(epoch)=0:00:00.060846
@020: 	loss(train)=0.5721 	loss(eval)=0.8753 	f1(train)=0.5750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.060882
@025: 	loss(train)=2.4752 	loss(eval)=1.3822 	f1(train)=0.5250 	f1(eval)=0.7000 	duration(epoch)=0:00:00.060405
@030: 	loss(train)=0.7104 	loss(eval)=0.5278 	f1(train)=0.4750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.060162
@035: 	loss(train)=0.7133 	loss(eval)=0.6134 	f1(train)=0.5250 	f1(eval)=0.8000 	duration(epoch)=0:00:00.059687
@040: 	loss(train)=0.7639 	loss(eval)=0.8718 	f1(train)=0.5750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059733
@045: 	loss(train)=0.6524 	loss(eval)=0.5091 	f1(train)=0.5500 	f1(eval)=0.7000 	duration(epoch)=0:00:00.060080
@050: 	loss(train)=0.5135 	loss(eval)=0.6293 	f1(train)=0.7500 	f1(eval)=0.6500 	duration(epoch)=0:00:00.059716
@055: 	loss(train)=0.5168 	loss(eval)=0.5899 	f1(train)=0.6250 	f1(eval)=0.7000 	duration(epoch)=0:00:00.060213
@060: 	loss(train)=0.6444 	loss(eval)=0.6501 	f1(train)=0.6250 	f1(eval)=0.6500 	duration(epoch)=0:00:00.060552
@065: 	loss(train)=0.7524 	loss(eval)=0.5247 	f1(train)=0.6750 	f1(eval)=0.7500 	duration(epoch)=0:00:00.059886
@070: 	loss(train)=0.6984 	loss(eval)=0.6315 	f1(train)=0.5250 	f1(eval)=0.6500 	duration(epoch)=0:00:00.059988
@075: 	loss(train)=0.6441 	loss(eval)=0.4848 	f1(train)=0.5750 	f1(eval)=0.7250 	duration(epoch)=0:00:00.060467
@080: 	loss(train)=0.6993 	loss(eval)=0.5562 	f1(train)=0.5500 	f1(eval)=0.7250 	duration(epoch)=0:00:00.060027
@085: 	loss(train)=0.6164 	loss(eval)=0.5149 	f1(train)=0.5750 	f1(eval)=0.7750 	duration(epoch)=0:00:00.059848
@090: 	loss(train)=0.6343 	loss(eval)=0.6213 	f1(train)=0.5000 	f1(eval)=0.7000 	duration(epoch)=0:00:00.059766
@095: 	loss(train)=0.6955 	loss(eval)=0.6435 	f1(train)=0.5500 	f1(eval)=0.7750 	duration(epoch)=0:00:00.060128
@100: 	loss(train)=0.5812 	loss(eval)=0.7260 	f1(train)=0.8000 	f1(eval)=0.6500 	duration(epoch)=0:00:00.060811
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0006 MB
  Trainable parameters: 158
  Input Dimension: 78
  Output Dimension: 2
@078: 	loss(train)=0.6211 	loss(eval)=0.4825 	f1(train)=0.6000 	f1(eval)=0.8250 	duration(epoch)=0:00:00.058960

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       33	 fp:        7 	 tn:       33	 fn:        7	 pre=0.8250	 rec=0.8250	 f1=0.8250	 acc=0.8250
negative      	 tp:       25	 fp:        5 	 tn:        8	 fn:        2	 pre=0.8333	 rec=0.9259	 f1=0.8772	 acc=0.8250
positive      	 tp:        8	 fp:        2 	 tn:       25	 fn:        5	 pre=0.8000	 rec=0.6154	 f1=0.6957	 acc=0.8250
