> Loaded logger: ./experiments/_debug/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0037 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2475 sec
> f(__ngram) took: 0.1905 sec
> f(__ngram) took: 0.1875 sec
> f(__load) took: 0.0044 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2098 sec
> f(__ngram) took: 0.2029 sec
> f(__ngram) took: 0.2886 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 256), ('2', 2048)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0218 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.8520 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 1.8645 sec
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0241 MB
  Trainable parameters: 6320
  Input Dimension: 78
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@005: 	loss(train)=2.2794 	loss(eval)=0.7694 	f1(train)=0.6250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.061999
@010: 	loss(train)=2.2409 	loss(eval)=0.6847 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.060092
@015: 	loss(train)=0.8783 	loss(eval)=0.6910 	f1(train)=0.5250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.060071
@020: 	loss(train)=0.8951 	loss(eval)=0.6929 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.063307
@025: 	loss(train)=0.6814 	loss(eval)=0.6929 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059399
@030: 	loss(train)=0.6854 	loss(eval)=0.6928 	f1(train)=0.7250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059807
@035: 	loss(train)=0.6894 	loss(eval)=0.6915 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059698
@040: 	loss(train)=0.6903 	loss(eval)=0.6926 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059636
@045: 	loss(train)=0.6714 	loss(eval)=0.6924 	f1(train)=0.5750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059750
@050: 	loss(train)=0.6819 	loss(eval)=0.6856 	f1(train)=0.6250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059437
@055: 	loss(train)=0.7175 	loss(eval)=0.6817 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.060015
@060: 	loss(train)=1.3525 	loss(eval)=0.6859 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059728
@065: 	loss(train)=0.6609 	loss(eval)=0.6872 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059603
@070: 	loss(train)=0.7238 	loss(eval)=0.6894 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059546
@075: 	loss(train)=0.9494 	loss(eval)=0.6865 	f1(train)=0.6000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059405
@080: 	loss(train)=0.5761 	loss(eval)=0.6732 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059478
@085: 	loss(train)=1.3756 	loss(eval)=0.7422 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059760
@090: 	loss(train)=0.7571 	loss(eval)=0.6823 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059712
@095: 	loss(train)=0.6960 	loss(eval)=0.6874 	f1(train)=0.6250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059425
@100: 	loss(train)=0.6989 	loss(eval)=0.6842 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059731
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Memory Usage: 0.0241 MB
  Trainable parameters: 6320
  Input Dimension: 78
  Output Dimension: 2
@100: 	loss(train)=0.6989 	loss(eval)=0.6842 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.059731

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       27	 fp:       13 	 tn:       27	 fn:       13	 pre=0.6750	 rec=0.6750	 f1=0.6750	 acc=0.6750
negative      	 tp:       27	 fp:       13 	 tn:        0	 fn:        0	 pre=0.6750	 rec=1.0000	 f1=0.8060	 acc=0.6750
positive      	 tp:        0	 fp:        0 	 tn:       27	 fn:       13	 pre=0.0000	 rec=0.0000	 f1=0.0000	 acc=0.6750
