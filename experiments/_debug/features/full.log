> Loaded logger: ./experiments/_debug/features/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval']) ---]
> f(__load) took: 0.0038 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.3279 sec
> f(__ngram) took: 0.2910 sec
> f(__ngram) took: 0.2827 sec
> f(__load) took: 0.0049 sec
> Load/Init from ./data/imdb._debug.csv
  Number of Samples: 40 
  Memory Usage: 0.0729 MB
> f(__tokenize) took: 0.2918 sec
> f(__ngram) took: 0.2911 sec
> f(__ngram) took: 0.2813 sec

[--- LOAD COMPONENTS ---]

[--- FEATURE PIPELINE ---]
> Init N-Gram Group Counter, with: [('1', 256), ('2', 2048)]
> Init NELA Pipeline
> Fit Pipeline ./data/imdb._debug.csv on (only N-Gram Group Counter)
> f(fit) took: 0.0221 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 2.2594 sec
> Apply Feature Pipeline on ./data/imdb._debug.csv
> f(apply) took: 2.1572 sec
> Init nGrams+NeLa Weighting (Features)
  Trainable parameters: 158
  Input Dimension: 78
  Output Dimension: 2

[--- TRAIN -> ./data/imdb._debug.csv ---]
@001: 	loss(train)=9.8141 	loss(eval)=4.8724 	f1(train)=0.4500 	f1(eval)=0.3500 	duration(epoch)=0:00:00.078359
@002: 	loss(train)=3.6397 	loss(eval)=2.1814 	f1(train)=0.4250 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066845
@003: 	loss(train)=4.8145 	loss(eval)=5.0083 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.067380
@004: 	loss(train)=2.1083 	loss(eval)=3.7945 	f1(train)=0.7000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.066027
@005: 	loss(train)=2.6939 	loss(eval)=3.1344 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065752
@006: 	loss(train)=2.1842 	loss(eval)=1.0784 	f1(train)=0.6750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.065087
@007: 	loss(train)=0.6838 	loss(eval)=1.1439 	f1(train)=0.8000 	f1(eval)=0.6250 	duration(epoch)=0:00:00.064967
@008: 	loss(train)=1.9186 	loss(eval)=1.2608 	f1(train)=0.6500 	f1(eval)=0.6000 	duration(epoch)=0:00:00.064477
@009: 	loss(train)=1.3455 	loss(eval)=0.7489 	f1(train)=0.5750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.064822
@010: 	loss(train)=1.1742 	loss(eval)=1.5507 	f1(train)=0.5500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.065172
@011: 	loss(train)=1.1100 	loss(eval)=1.4407 	f1(train)=0.6750 	f1(eval)=0.6750 	duration(epoch)=0:00:00.063644
@012: 	loss(train)=1.0378 	loss(eval)=0.6033 	f1(train)=0.7000 	f1(eval)=0.7250 	duration(epoch)=0:00:00.064674
@013: 	loss(train)=0.7611 	loss(eval)=0.7116 	f1(train)=0.7250 	f1(eval)=0.7750 	duration(epoch)=0:00:00.068451
@014: 	loss(train)=0.7126 	loss(eval)=0.4814 	f1(train)=0.6750 	f1(eval)=0.7500 	duration(epoch)=0:00:00.064362
@015: 	loss(train)=1.0509 	loss(eval)=1.5972 	f1(train)=0.7500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.063940
@016: 	loss(train)=1.5109 	loss(eval)=1.3525 	f1(train)=0.7000 	f1(eval)=0.6750 	duration(epoch)=0:00:00.063684
@017: 	loss(train)=0.8801 	loss(eval)=0.5216 	f1(train)=0.7250 	f1(eval)=0.8000 	duration(epoch)=0:00:00.064338
@018: 	loss(train)=0.8132 	loss(eval)=0.5692 	f1(train)=0.6500 	f1(eval)=0.7500 	duration(epoch)=0:00:00.064703
@019: 	loss(train)=0.7839 	loss(eval)=0.7127 	f1(train)=0.7500 	f1(eval)=0.8000 	duration(epoch)=0:00:00.064486
@020: 	loss(train)=0.5917 	loss(eval)=0.6263 	f1(train)=0.6750 	f1(eval)=0.7250 	duration(epoch)=0:00:00.064807
@021: 	loss(train)=0.5738 	loss(eval)=0.5237 	f1(train)=0.7000 	f1(eval)=0.8250 	duration(epoch)=0:00:00.064443
@022: 	loss(train)=0.5605 	loss(eval)=0.5348 	f1(train)=0.6500 	f1(eval)=0.7750 	duration(epoch)=0:00:00.064000
@023: 	loss(train)=0.6647 	loss(eval)=0.4562 	f1(train)=0.5750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.063758
@024: 	loss(train)=0.8338 	loss(eval)=0.6702 	f1(train)=0.7750 	f1(eval)=0.7000 	duration(epoch)=0:00:00.064324
@025: 	loss(train)=1.1731 	loss(eval)=0.5676 	f1(train)=0.6500 	f1(eval)=0.6750 	duration(epoch)=0:00:00.064822
> Load best model based on evaluation loss.
> Init nGrams+NeLa Weighting (Features)
  Trainable parameters: 158
  Input Dimension: 78
  Output Dimension: 2
@023: 	loss(train)=0.6647 	loss(eval)=0.4562 	f1(train)=0.5750 	f1(eval)=0.8500 	duration(epoch)=0:00:00.063758

[--- EVAL -> ./data/imdb._debug.csv ---]
AVG           	 tp:       34	 fp:        6 	 tn:       34	 fn:        6	 pre=0.8500	 rec=0.8500	 f1=0.8500	 acc=0.8500
negative      	 tp:       26	 fp:        5 	 tn:        8	 fn:        1	 pre=0.8387	 rec=0.9630	 f1=0.8966	 acc=0.8500
positive      	 tp:        8	 fp:        1 	 tn:       26	 fn:        5	 pre=0.8889	 rec=0.6154	 f1=0.7273	 acc=0.8500
