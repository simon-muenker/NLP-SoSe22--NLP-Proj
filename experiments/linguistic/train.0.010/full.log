> Loaded logger: ./experiments/linguistic/train.0.010/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.010.csv
> f(__load) took: 0.0097 sec
> f(__tokenize) took: 0.3290 sec
> f(__ngram) took: 0.2214 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0831 sec
> f(__tokenize) took: 1.2034 sec
> f(__ngram) took: 0.4715 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0627 sec
> f(__tokenize) took: 1.1684 sec
> f(__ngram) took: 0.6549 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.010.csv
> f(fit) took: 0.0617 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.0.010.csv
> f(predict) took: 1.6305 sec
AVG           	 tp:      379	 fp:       21 	 tn:      379	 fn:       21	 pre=0.9475	 rec=0.9475	 f1=0.9475	 acc=0.9475
negative      	 tp:      204	 fp:       16 	 tn:      175	 fn:        5	 pre=0.9273	 rec=0.9761	 f1=0.9510	 acc=0.9475
positive      	 tp:      175	 fp:        5 	 tn:      204	 fn:       16	 pre=0.9722	 rec=0.9162	 f1=0.9434	 acc=0.9475
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 2.9534 sec
AVG           	 tp:     3551	 fp:     1449 	 tn:     3551	 fn:     1449	 pre=0.7102	 rec=0.7102	 f1=0.7102	 acc=0.7102
negative      	 tp:     1986	 fp:      972 	 tn:     1565	 fn:      477	 pre=0.6714	 rec=0.8063	 f1=0.7327	 acc=0.7102
positive      	 tp:     1565	 fp:      477 	 tn:     1986	 fn:      972	 pre=0.7664	 rec=0.6169	 f1=0.6836	 acc=0.7102
