> Loaded logger: ./experiments/linguistic/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4738 sec
> f(__tokenize) took: 6.2945 sec
> f(__ngram) took: 3.3367 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0654 sec
> f(__tokenize) took: 1.5077 sec
> f(__ngram) took: 3.8727 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0598 sec
> f(__tokenize) took: 1.5826 sec
> f(__ngram) took: 0.9595 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 6.0375 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 12.8894 sec
AVG           	 tp:    31662	 fp:     8338 	 tn:    31662	 fn:     8338	 pre=0.7915	 rec=0.7915	 f1=0.7915	 acc=0.7915
negative      	 tp:    16189	 fp:     4520 	 tn:    15473	 fn:     3818	 pre=0.7817	 rec=0.8092	 f1=0.7952	 acc=0.7915
positive      	 tp:    15473	 fp:     3818 	 tn:    16189	 fn:     4520	 pre=0.8021	 rec=0.7739	 f1=0.7878	 acc=0.7915
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 4.2943 sec
AVG           	 tp:     3955	 fp:     1045 	 tn:     3955	 fn:     1045	 pre=0.7910	 rec=0.7910	 f1=0.7910	 acc=0.7910
negative      	 tp:     1983	 fp:      565 	 tn:     1972	 fn:      480	 pre=0.7783	 rec=0.8051	 f1=0.7915	 acc=0.7910
positive      	 tp:     1972	 fp:      480 	 tn:     1983	 fn:      565	 pre=0.8042	 rec=0.7773	 f1=0.7905	 acc=0.7910
