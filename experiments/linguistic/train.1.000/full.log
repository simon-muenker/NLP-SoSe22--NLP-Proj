> Loaded logger: ./experiments/linguistic/train.1.000/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.1.000.csv
> f(__load) took: 0.4563 sec
> f(__tokenize) took: 6.4129 sec
> f(__ngram) took: 3.8061 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0631 sec
> f(__tokenize) took: 1.6360 sec
> f(__ngram) took: 3.4749 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0700 sec
> f(__tokenize) took: 1.9044 sec
> f(__ngram) took: 1.3064 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.1.000.csv
> f(fit) took: 7.1724 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.1.000.csv
> f(predict) took: 14.4514 sec
AVG           	 tp:    31712	 fp:     8288 	 tn:    31712	 fn:     8288	 pre=0.7928	 rec=0.7928	 f1=0.7928	 acc=0.7928
negative      	 tp:    16176	 fp:     4427 	 tn:    15536	 fn:     3861	 pre=0.7851	 rec=0.8073	 f1=0.7961	 acc=0.7928
positive      	 tp:    15536	 fp:     3861 	 tn:    16176	 fn:     4427	 pre=0.8009	 rec=0.7782	 f1=0.7894	 acc=0.7928
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 5.3483 sec
AVG           	 tp:     3936	 fp:     1064 	 tn:     3936	 fn:     1064	 pre=0.7872	 rec=0.7872	 f1=0.7872	 acc=0.7872
negative      	 tp:     1992	 fp:      579 	 tn:     1944	 fn:      485	 pre=0.7748	 rec=0.8042	 f1=0.7892	 acc=0.7872
positive      	 tp:     1944	 fp:      485 	 tn:     1992	 fn:      579	 pre=0.8003	 rec=0.7705	 f1=0.7851	 acc=0.7872
