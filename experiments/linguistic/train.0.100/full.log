> Loaded logger: ./experiments/linguistic/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0491 sec
> f(__tokenize) took: 0.9546 sec
> f(__ngram) took: 0.4980 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0857 sec
> f(__tokenize) took: 1.2163 sec
> f(__ngram) took: 0.5042 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0628 sec
> f(__tokenize) took: 1.2038 sec
> f(__ngram) took: 0.8398 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.4286 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 2.5220 sec
AVG           	 tp:     3258	 fp:      742 	 tn:     3258	 fn:      742	 pre=0.8145	 rec=0.8145	 f1=0.8145	 acc=0.8145
negative      	 tp:     1628	 fp:      408 	 tn:     1630	 fn:      334	 pre=0.7996	 rec=0.8298	 f1=0.8144	 acc=0.8145
positive      	 tp:     1630	 fp:      334 	 tn:     1628	 fn:      408	 pre=0.8299	 rec=0.7998	 f1=0.8146	 acc=0.8145
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 2.8558 sec
AVG           	 tp:     3882	 fp:     1118 	 tn:     3882	 fn:     1118	 pre=0.7764	 rec=0.7764	 f1=0.7764	 acc=0.7764
negative      	 tp:     1950	 fp:      605 	 tn:     1932	 fn:      513	 pre=0.7632	 rec=0.7917	 f1=0.7772	 acc=0.7764
positive      	 tp:     1932	 fp:      513 	 tn:     1950	 fn:      605	 pre=0.7902	 rec=0.7615	 f1=0.7756	 acc=0.7764
