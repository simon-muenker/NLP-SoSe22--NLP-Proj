> Loaded logger: ./experiments/linguistic/train.0.100/full.log
> Setup PyTorch: seed(1), cuda(5)

[--- PREPARE DATA -> (['train', 'eval', 'test']) ---]
> Load/Init from ./data/imdb.train.0.100.csv
> f(__load) took: 0.0526 sec
> f(__tokenize) took: 0.9885 sec
> f(__ngram) took: 0.4982 sec
> Load/Init from ./data/imdb.eval.csv
> f(__load) took: 0.0831 sec
> f(__tokenize) took: 1.1839 sec
> f(__ngram) took: 0.5192 sec
> Load/Init from ./data/imdb.test.csv
> f(__load) took: 0.0641 sec
> f(__tokenize) took: 1.2228 sec
> f(__ngram) took: 0.8280 sec

[--- LOAD COMPONENTS ---]
> Init Freq. Classifier, n-grams: ['1', '2']

[--- RUN ---]
> Fit Freq. Classifier on ./data/imdb.train.0.100.csv
> f(fit) took: 0.4357 sec

[--- EVAL -> ['train', 'eval'] ---]
> Predict with Freq. Classifier on ./data/imdb.train.0.100.csv
> f(predict) took: 2.5278 sec
AVG           	 tp:     3258	 fp:      742 	 tn:     3258	 fn:      742	 pre=0.8145	 rec=0.8145	 f1=0.8145	 acc=0.8145
negative      	 tp:     1628	 fp:      408 	 tn:     1630	 fn:      334	 pre=0.7996	 rec=0.8298	 f1=0.8144	 acc=0.8145
positive      	 tp:     1630	 fp:      334 	 tn:     1628	 fn:      408	 pre=0.8299	 rec=0.7998	 f1=0.8146	 acc=0.8145
> Predict with Freq. Classifier on ./data/imdb.eval.csv
> f(predict) took: 2.8186 sec
AVG           	 tp:     3882	 fp:     1118 	 tn:     3882	 fn:     1118	 pre=0.7764	 rec=0.7764	 f1=0.7764	 acc=0.7764
negative      	 tp:     1950	 fp:      605 	 tn:     1932	 fn:      513	 pre=0.7632	 rec=0.7917	 f1=0.7772	 acc=0.7764
positive      	 tp:     1932	 fp:      513 	 tn:     1950	 fn:      605	 pre=0.7902	 rec=0.7615	 f1=0.7756	 acc=0.7764
